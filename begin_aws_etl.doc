#-----------------------------------------------------------------------------------------------------------------------------------------------
#	Chapter 01 : ETL 
#-----------------------------------------------------------------------------------------------------------------------------------------------
# 
# * What is ETL 
# ETL stands for Extract, Transform and Load , which are the three common steps of action which we need to perform when dealing with data.
#
# - Extract   : This is the proces of getting the data from the source, source might contain structured data such as csv or semi structured such as json
# - Transform : This is the process of transform the data in an useful format which contains operations such as clearning, splitting, joining, sorting etc 
# - Loading   : This is the process of storing data in a destination, this include storing data in s3, data warehoues, data lakes etc 
#
# * What are common challenges while dealing with data 
# When we deal with the data there are few common challenges we might face such as 
#
# . Extracting the data from various sources 
# . TRansforming them 
# . Catching up with ongoing data changes such as format changes, some fields removed or added etc 
# . Managing compute with scalable data requirements 
# . Handling more complicated data structures 
#
#-----------------------------------------------------------------------------------------------------------------------------------------------
#	Chapter 02 : Glue
#-----------------------------------------------------------------------------------------------------------------------------------------------
#
# * What is AWS Glue 
# Glue is a ETL product which introduced by AWS on 2017 and it offers wide vriety of integration with various AWS Services 
# This also helps you keep up with few of the data tranformation challenges such as 
#
# . Glue is serverless so you do not need to pay when it is idle 
# . It scales automatically according to the data requirements 
# . It helps to perform tranformation on onging changes in data structures by auto discovery and catalogs 
# . It also deals with complicated operations with fully managed spark clusters 
# 
# Glue has mainly few components 
# - Glue Data Catalog 	: This is metadata store which defines tables and schemas, This can automatically created or suggested by glue according to data source in crawler
# - Crawler 		: This helps to get the data into Glue from its source for further processing, data source defined in the crawler can be used by ETL jobs
# - Glue Job		: This is the process of triggering tranformation job and sending output to a datastore, Job reads data from source usinng crawler.
#
# NOTE : To know more about Glue concepts please visit https://docs.aws.amazon.com/glue/latest/dg/components-key-concepts.html
#
#-----------------------------------------------------------------------------------------------------------------------------------------------
#	Chapter 03 : Hadoop Ecosystem
#-----------------------------------------------------------------------------------------------------------------------------------------------
#
# * What is Hadoop ecosystem
# Hadoop is a big data processing framework which has various components which helps to get this job done. 
# There are various components within hadoop framework and some details about it given below
#
# - HDFS : This is called hadoop distributed filesystem which will be spanned across various different commodity hardware system and it provides fault tolerence upon hardware failure as well.
#        : To know more about hdfs, please check https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html
#
# - YARN : This is called yet another resourse negotiator, which helps to allocate resources within the hadoop cluster.
#        : To know more about YARN, please check https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html
#
# - Map Reduce : This is a software framework which used by hadoop to process large set of data and it can be ran in cheaper commodity hardware as well. 
#              : Map reduce uses a technique of creating map tasks and split data across multiple streams and process them faster and then uses reduce tasks to combine them. 
#              : To know more about mapreduce, please check : https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html
#
# - Spark : Spark is high performing data processing framework which does most of the processing in memory itself 
#         : This is more ideal for streaming data processing since it provides greater performance 
#
# Note : Main difference between Mapreduce and Spark are below 
#
# - Spark uses in memory operation to process data and because of that it provide greater performance where map reduce uses HDFS which is a disk based solution which offer lower performance 
# - Spark is expernsive where mapreduce is cheaper.
# - Spark is good for algorithmatic operations where mapreduce is more ideal for batch processing   
#
#
# * Spark ML offering 
# Spark offers few additional products for ML such as 'MLlib' and 'MAAbout' which helps to perform classification, recomendation and clustering 
# It also offers couple of deep learning offering such as Mxnet and Tensor Flow
#
#
#
#
#
