{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e12bc7",
   "metadata": {},
   "source": [
    "### Spark\n",
    "Apache spark is a distributed data processing framework, before you start, please spend some time in reading below documentation which will help you to understand the concepts.\n",
    "https://en.wikipedia.org/wiki/Apache_Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee46918",
   "metadata": {},
   "source": [
    "### Spark - How it works \n",
    "Spark mainly has three components as part of its execution \n",
    "\n",
    "##### Spark Context  \n",
    "This is driver program which sets the memory etc for the spark \n",
    "When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker node.\n",
    "##### Cluster manager \n",
    "Which manages resources using YARN \n",
    "##### Executor \n",
    "Which runs the task which sent by Spark conect \n",
    " \n",
    "Flow :  \"Spark Context\" --> \"Cluster manager\" --> \"Executor\"\n",
    "\n",
    "Note : To know more about pyspark refer - https://www.tutorialspoint.com/pyspark/pyspark_quick_guide.htm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3261dbf",
   "metadata": {},
   "source": [
    "#### Components of Spark are below \n",
    "\n",
    "##### RDDS (Resilient Distributed Dataset) \n",
    "RDDS is the fundamental data structure of Apache Spark which are an immutable collection of objects which computes on the different node of the cluster. Each and every dataset in Spark RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster.\n",
    "\n",
    "##### Spark Streaming \n",
    "This is used for analyzing data in streams, normally data will be send in mini batches for analyzing\n",
    "With streaming data frame which initialized will be keep gowring as the new mini batch of streams gets added \n",
    "You can Integrate Kenisis streaming with spark streaming\n",
    "\n",
    "##### Spark SQL\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine\n",
    "\n",
    "##### MLLib           \n",
    "This is machine learning library with the spark\n",
    "\n",
    "#### GraphX\n",
    "This produces graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b01f17",
   "metadata": {},
   "source": [
    "#### Spark MLLib \n",
    "MLlib is a machine learning library used by pyspark and its intended to provide the practical machine learning scalable and possible. At a high level MLLib privides tools such as. \n",
    "- ML Algotithms : Common learning algorithms such as classification, regression, clustering and collaborative filtering \n",
    "- featurization : feature extraction, transformation, dimensionality reduction and selection \n",
    "- Pipeline      : Tools for constructing, evaluating and tuning ML pipelines \n",
    "- Persistance   : Saving and load algorithms, models and pipelines \n",
    "- Utilities     : Linear algebra, statistics, data handling etc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
