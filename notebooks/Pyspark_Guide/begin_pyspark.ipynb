{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e12bc7",
   "metadata": {},
   "source": [
    "### Spark\n",
    "Apache spark is a distributed data processing framework, before you start, please spend some time in reading below documentation which will help you to understand the concepts.\n",
    "https://en.wikipedia.org/wiki/Apache_Spark\n",
    "\n",
    "- Source : https://github.com/ajay291491/Mastering-Big-Data-Analytics-with-PySpark\n",
    "- Course : https://learning.oreilly.com/videos/mastering-big-data/9781838640583/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee46918",
   "metadata": {},
   "source": [
    "### Spark - How it works \n",
    "Spark mainly has three components as part of its execution \n",
    "\n",
    "##### Spark Context  \n",
    "This is driver program which sets the memory etc for the spark \n",
    "When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker node.\n",
    "##### Cluster manager \n",
    "Which manages resources using YARN \n",
    "##### Executor \n",
    "Which runs the task which sent by Spark conect \n",
    " \n",
    "Flow :  \"Spark Context\" --> \"Cluster manager\" --> \"Executor\"\n",
    "\n",
    "Note : To know more about pyspark refer - https://www.tutorialspoint.com/pyspark/pyspark_quick_guide.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3261dbf",
   "metadata": {},
   "source": [
    "#### Components of Spark are below \n",
    "\n",
    "##### RDDS (Resilient Distributed Dataset) \n",
    "RDDS is the fundamental data structure of Apache Spark which are an immutable collection of objects which computes on the different node of the cluster. Each and every dataset in Spark RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster.\n",
    "\n",
    "##### Spark Streaming \n",
    "This is used for analyzing data in streams, normally data will be send in mini batches for analyzing\n",
    "With streaming data frame which initialized will be keep gowring as the new mini batch of streams gets added \n",
    "You can Integrate Kenisis streaming with spark streaming\n",
    "\n",
    "##### Spark SQL\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine\n",
    "\n",
    "##### MLLib           \n",
    "This is machine learning library with the spark\n",
    "\n",
    "#### GraphX\n",
    "This produces graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b01f17",
   "metadata": {},
   "source": [
    "#### Spark MLLib \n",
    "MLlib is a machine learning library used by pyspark and its intended to provide the practical machine learning scalable and possible. At a high level MLLib privides tools such as. \n",
    "- ML Algotithms : Common learning algorithms such as classification, regression, clustering and collaborative filtering \n",
    "- featurization : feature extraction, transformation, dimensionality reduction and selection \n",
    "- Pipeline      : Tools for constructing, evaluating and tuning ML pipelines \n",
    "- Persistance   : Saving and load algorithms, models and pipelines \n",
    "- Utilities     : Linear algebra, statistics, data handling etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854d59c",
   "metadata": {},
   "source": [
    "#### Spark DataFrame\n",
    "DataFrame is a distributed collection of rows(dataset) orginized in named columns. \n",
    "- This can be used for relational transform \n",
    "- As part of the pyspark.sql package, allows you to run queries over data \n",
    "- Faster than RDD (legacy) due to their query plan optimization \n",
    "- To know more : https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb93952",
   "metadata": {},
   "source": [
    "##### Spark DataFrame and RDD\n",
    "- Spark Dataframe is built on top of RDD\n",
    "- RDDs are immutable in nature, which means it cann't be altered once it is created \n",
    "- Since its immutable its easy and safe to share acorss multiple process\n",
    "- It can be created any time and can live both in memeory and disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96eafed",
   "metadata": {},
   "source": [
    "#### Spark SQL\n",
    "Spark is library which helps you to deal with data frames. \n",
    "- This helps to easily load and evaluate the data \n",
    "- Execute SQL queries in spark\n",
    "- DataFrame API with a rich Library functions\n",
    "- It has integration with hadoop and hive \n",
    "- Data source API will have lot of built in integration with various data sources\n",
    "- JDBC/ODBC connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d1341",
   "metadata": {},
   "source": [
    "#### Reading CSV Dataset\n",
    "Below i sthe step by step procedure to read CSV file in spark. \n",
    "It also shows various different methods in reading CSV\n",
    "\n",
    "- Reading without any parameters \n",
    "- Reading with standard parameter \n",
    "- Reading with custom scheme while loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124ce95",
   "metadata": {},
   "source": [
    "##### Reading with out any special parameter \n",
    "In this way data gets read, but this is not always the preferred way of opening a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a436973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a spark sql session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyFirstCSVLoad\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7e9a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ajayaghoshvl/.conda/envs/aws_exam_prep/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/03 15:59:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data-sets/ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3842b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|   _c0|    _c1|   _c2|      _c3|\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading First 5 rows in the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6b68e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the schema of the dataframe \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabbe71",
   "metadata": {},
   "source": [
    "##### Reading with standard Parameter Sets (More standard way of creating dataframe)\n",
    "When we initialize a dataframe then we additionally provide few paramater while initializing \n",
    "- path : Path where the file is stored to read \n",
    "- sep  : Sets a single character as a separator for each field and value. If None set, uses the default value, ,.\n",
    "- header : Uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "- quote :  sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, \". If you would like to turn off quotations, you need to set an empty string.\n",
    "- inferSchema : Infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47a693c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More standard way of Creating a dataframe \n",
    "df = spark.read.csv(\n",
    "    path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b1fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2f14bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb9f98",
   "metadata": {},
   "source": [
    "##### How to change the schema while loading dataframe\n",
    "We can change the header name and also the Type of the schema by manually setting those schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1c60cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are manually setting the schema and its type \n",
    "df = spark.read.csv(\n",
    "    path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    schema=\"userID INT, movieID INT, score DOUBLE, timestamp INT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44068370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+---------+\n",
      "|userID|movieID|score|timestamp|\n",
      "+------+-------+-----+---------+\n",
      "|     1|      1|  4.0|964982703|\n",
      "|     1|      3|  4.0|964981247|\n",
      "|     1|      6|  4.0|964982224|\n",
      "|     1|     47|  5.0|964983815|\n",
      "|     1|     50|  5.0|964982931|\n",
      "+------+-------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 16:26:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, movieID, score, timestamp\n",
      "Expected: score but found: rating\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86540461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ce591",
   "metadata": {},
   "source": [
    "#### Fixing issues in the data\n",
    "- In the following topic we will understand how to explore the data and fix them as needed. \n",
    "- Here we will be using the module \"pyspark.sql.functions\" for this purpose\n",
    "- Detail Doc: https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "##### Task : Covert the Unix timestamp to Human readable format and remove the original timestamp column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4aa379f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the spark sql function \n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7278aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell 51 to initize the dataframe before this \n",
    "# Renaming an existing column and Adding a new column\n",
    "df = df.withColumnRenamed(\"timestamp\", \"timestamp_unix\")           # Renaming timestamp to timestamp_unix       \n",
    "df = df.withColumn(\"timestamp\", f.from_unixtime(\"timestamp_unix\")) # Creating new column timestamp after converting existing timestamp_unix column to human readable format using 'f.from_unixtime'\n",
    "df = df.withColumn(\"timestamp\", f.to_timestamp(\"timestamp\"))       # Change schema of timestmap column as timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcf8e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+--------------+-------------------+\n",
      "|userID|movieID|score|timestamp_unix|          timestamp|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "|     1|      1|  4.0|     964982703|2000-07-30 19:45:03|\n",
      "|     1|      3|  4.0|     964981247|2000-07-30 19:20:47|\n",
      "|     1|      6|  4.0|     964982224|2000-07-30 19:37:04|\n",
      "|     1|     47|  5.0|     964983815|2000-07-30 20:03:35|\n",
      "|     1|     50|  5.0|     964982931|2000-07-30 19:48:51|\n",
      "|     1|     70|  3.0|     964982400|2000-07-30 19:40:00|\n",
      "|     1|    101|  5.0|     964980868|2000-07-30 19:14:28|\n",
      "|     1|    110|  4.0|     964982176|2000-07-30 19:36:16|\n",
      "|     1|    151|  5.0|     964984041|2000-07-30 20:07:21|\n",
      "|     1|    157|  5.0|     964984100|2000-07-30 20:08:20|\n",
      "|     1|    163|  5.0|     964983650|2000-07-30 20:00:50|\n",
      "|     1|    216|  5.0|     964981208|2000-07-30 19:20:08|\n",
      "|     1|    223|  3.0|     964980985|2000-07-30 19:16:25|\n",
      "|     1|    231|  5.0|     964981179|2000-07-30 19:19:39|\n",
      "|     1|    235|  4.0|     964980908|2000-07-30 19:15:08|\n",
      "|     1|    260|  5.0|     964981680|2000-07-30 19:28:00|\n",
      "|     1|    296|  3.0|     964982967|2000-07-30 19:49:27|\n",
      "|     1|    316|  3.0|     964982310|2000-07-30 19:38:30|\n",
      "|     1|    333|  5.0|     964981179|2000-07-30 19:19:39|\n",
      "|     1|    349|  4.0|     964982563|2000-07-30 19:42:43|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 17:15:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, movieID, score, timestamp\n",
      "Expected: score but found: rating\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923285b6",
   "metadata": {},
   "source": [
    "##### Now lets do the same steps above in single line operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8ff0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell 51 to initize the dataframe before this \n",
    "df = (\n",
    "    df\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")           # Renaming timestamp to timestamp_unix       \n",
    "    .withColumn(\"timestamp\", f.from_unixtime(\"timestamp_unix\")) # Creating new column timestamp after converting existing timestamp_unix column to human readable format using 'f.from_unixtime'\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(\"timestamp\"))       # Change schema of timestmap column as timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcbcb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+--------------+-------------------+\n",
      "|userID|movieID|score|timestamp_unix|          timestamp|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "|     1|      1|  4.0|     964982703|2000-07-30 19:45:03|\n",
      "|     1|      3|  4.0|     964981247|2000-07-30 19:20:47|\n",
      "|     1|      6|  4.0|     964982224|2000-07-30 19:37:04|\n",
      "|     1|     47|  5.0|     964983815|2000-07-30 20:03:35|\n",
      "|     1|     50|  5.0|     964982931|2000-07-30 19:48:51|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 17:16:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, movieID, score, timestamp\n",
      "Expected: score but found: rating\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa379",
   "metadata": {},
   "source": [
    "#### Lets do all above steps into single step\n",
    "- Create dataframe \n",
    "- set schems\n",
    "- Set original timestamp to a column called timestamp_unix\n",
    "- Generate a new column named timestamp which convert unix timestamp to human readable \n",
    "- Set the schema of the new column to timestamp from INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4744b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a spark sql session \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f \n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyFirstCSVLoad\").getOrCreate()\n",
    "df = (\n",
    "    spark.read.csv(\n",
    "        path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        quote='\"',\n",
    "        schema=\"userID INT, moveID INT, rating DOUBLE, timestamp INT\"\n",
    "    )\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41b232fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+--------------+-------------------+\n",
      "|userID|moveID|rating|timestamp_unix|          timestamp|\n",
      "+------+------+------+--------------+-------------------+\n",
      "|     1|     1|   4.0|     964982703|2000-07-30 19:45:03|\n",
      "|     1|     3|   4.0|     964981247|2000-07-30 19:20:47|\n",
      "|     1|     6|   4.0|     964982224|2000-07-30 19:37:04|\n",
      "|     1|    47|   5.0|     964983815|2000-07-30 20:03:35|\n",
      "|     1|    50|   5.0|     964982931|2000-07-30 19:48:51|\n",
      "+------+------+------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- moveID: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 17:32:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, moveID, rating, timestamp\n",
      "Expected: moveID but found: movieId\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a3ef8",
   "metadata": {},
   "source": [
    "#### Fixing issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
