{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e12bc7",
   "metadata": {},
   "source": [
    "### Spark\n",
    "Apache spark is a distributed data processing framework, before you start, please spend some time in reading below documentation which will help you to understand the concepts.\n",
    "https://en.wikipedia.org/wiki/Apache_Spark\n",
    "\n",
    "- Source : https://github.com/ajay291491/Mastering-Big-Data-Analytics-with-PySpark\n",
    "- Course : https://learning.oreilly.com/videos/mastering-big-data/9781838640583/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff02b9",
   "metadata": {},
   "source": [
    "## Chapter 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee46918",
   "metadata": {},
   "source": [
    "### Spark - How it works \n",
    "Spark mainly has three components as part of its execution \n",
    "\n",
    "##### Spark Context  \n",
    "This is driver program which sets the memory etc for the spark \n",
    "When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker node.\n",
    "##### Cluster manager \n",
    "Which manages resources using YARN \n",
    "##### Executor \n",
    "Which runs the task which sent by Spark conect \n",
    " \n",
    "Flow :  \"Spark Context\" --> \"Cluster manager\" --> \"Executor\"\n",
    "\n",
    "Note : To know more about pyspark refer - https://www.tutorialspoint.com/pyspark/pyspark_quick_guide.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3261dbf",
   "metadata": {},
   "source": [
    "#### Components of Spark are below \n",
    "\n",
    "##### RDDS (Resilient Distributed Dataset) \n",
    "RDDS is the fundamental data structure of Apache Spark which are an immutable collection of objects which computes on the different node of the cluster. Each and every dataset in Spark RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster.\n",
    "\n",
    "##### Spark Streaming \n",
    "This is used for analyzing data in streams, normally data will be send in mini batches for analyzing\n",
    "With streaming data frame which initialized will be keep gowring as the new mini batch of streams gets added \n",
    "You can Integrate Kenisis streaming with spark streaming\n",
    "\n",
    "##### Spark SQL\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine\n",
    "\n",
    "##### MLLib           \n",
    "This is machine learning library with the spark\n",
    "\n",
    "#### GraphX\n",
    "This produces graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b01f17",
   "metadata": {},
   "source": [
    "#### Spark MLLib \n",
    "MLlib is a machine learning library used by pyspark and its intended to provide the practical machine learning scalable and possible. At a high level MLLib privides tools such as. \n",
    "- ML Algotithms : Common learning algorithms such as classification, regression, clustering and collaborative filtering \n",
    "- featurization : feature extraction, transformation, dimensionality reduction and selection \n",
    "- Pipeline      : Tools for constructing, evaluating and tuning ML pipelines \n",
    "- Persistance   : Saving and load algorithms, models and pipelines \n",
    "- Utilities     : Linear algebra, statistics, data handling etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854d59c",
   "metadata": {},
   "source": [
    "#### Spark DataFrame\n",
    "DataFrame is a distributed collection of rows(dataset) orginized in named columns. \n",
    "- This can be used for relational transform \n",
    "- As part of the pyspark.sql package, allows you to run queries over data \n",
    "- Faster than RDD (legacy) due to their query plan optimization \n",
    "- To know more : https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb93952",
   "metadata": {},
   "source": [
    "##### Spark DataFrame and RDD\n",
    "- Spark Dataframe is built on top of RDD\n",
    "- RDDs are immutable in nature, which means it cann't be altered once it is created \n",
    "- Since its immutable its easy and safe to share acorss multiple process\n",
    "- It can be created any time and can live both in memeory and disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96eafed",
   "metadata": {},
   "source": [
    "#### Spark SQL\n",
    "Spark is library which helps you to deal with data frames. \n",
    "- This helps to easily load and evaluate the data \n",
    "- Execute SQL queries in spark\n",
    "- DataFrame API with a rich Library functions\n",
    "- It has integration with hadoop and hive \n",
    "- Data source API will have lot of built in integration with various data sources\n",
    "- JDBC/ODBC connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d1341",
   "metadata": {},
   "source": [
    "#### Reading CSV Dataset\n",
    "Below i sthe step by step procedure to read CSV file in spark. \n",
    "It also shows various different methods in reading CSV\n",
    "\n",
    "- Reading without any parameters \n",
    "- Reading with standard parameter \n",
    "- Reading with custom scheme while loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c71df8",
   "metadata": {},
   "source": [
    "## Chapter 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124ce95",
   "metadata": {},
   "source": [
    "##### Reading with out any special parameter \n",
    "In this way data gets read, but this is not always the preferred way of opening a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a436973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a spark sql session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyFirstCSVLoad\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7e9a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ajayaghoshvl/.conda/envs/aws_exam_prep/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/03 15:59:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data-sets/ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3842b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|   _c0|    _c1|   _c2|      _c3|\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading First 5 rows in the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f6b68e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the schema of the dataframe \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabbe71",
   "metadata": {},
   "source": [
    "##### Reading with standard Parameter Sets (More standard way of creating dataframe)\n",
    "When we initialize a dataframe then we additionally provide few paramater while initializing \n",
    "- path : Path where the file is stored to read \n",
    "- sep  : Sets a single character as a separator for each field and value. If None set, uses the default value, ,.\n",
    "- header : Uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "- quote :  sets a single character used for escaping quoted values where the separator can be part of the value. If None is set, it uses the default value, \". If you would like to turn off quotations, you need to set an empty string.\n",
    "- inferSchema : Infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47a693c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More standard way of Creating a dataframe \n",
    "df = spark.read.csv(\n",
    "    path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b1fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2f14bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb9f98",
   "metadata": {},
   "source": [
    "##### How to change the schema while loading dataframe\n",
    "We can change the header name and also the Type of the schema by manually setting those schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1c60cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are manually setting the schema and its type \n",
    "df = spark.read.csv(\n",
    "    path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "    sep=\",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    schema=\"userID INT, movieID INT, score DOUBLE, timestamp INT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44068370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+---------+\n",
      "|userID|movieID|score|timestamp|\n",
      "+------+-------+-----+---------+\n",
      "|     1|      1|  4.0|964982703|\n",
      "|     1|      3|  4.0|964981247|\n",
      "|     1|      6|  4.0|964982224|\n",
      "|     1|     47|  5.0|964983815|\n",
      "|     1|     50|  5.0|964982931|\n",
      "+------+-------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 16:26:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, movieID, score, timestamp\n",
      "Expected: score but found: rating\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86540461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f80b69",
   "metadata": {},
   "source": [
    "## Chapter 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ce591",
   "metadata": {},
   "source": [
    "#### Fixing issues in the data - Part 01\n",
    "- In the following topic we will understand how to explore the data and fix them as needed. \n",
    "- Here we will be using the module \"pyspark.sql.functions\" for this purpose\n",
    "- Detail Doc: https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "##### Task : Covert the Unix timestamp to Human readable format and remove the original timestamp column \n",
    "- Initialize the spark function \n",
    "- Rename and existing columnnamed timestamp to timestamp_unix \n",
    "- Append a new column which will human readable form timestamp_unix column\n",
    "- Complete all above steps in single line of code\n",
    "- Complete all above steps along with initiazing dataframe \n",
    "- Drop the timestamp_unix column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4aa379f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the spark sql function \n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7278aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell 51 to initize the dataframe before this \n",
    "# Renaming an existing column and Adding a new column\n",
    "df = df.withColumnRenamed(\"timestamp\", \"timestamp_unix\")           # Renaming timestamp to timestamp_unix       \n",
    "df = df.withColumn(\"timestamp\", f.from_unixtime(\"timestamp_unix\")) # Creating new column timestamp after converting existing timestamp_unix column to human readable format using 'f.from_unixtime'\n",
    "df = df.withColumn(\"timestamp\", f.to_timestamp(\"timestamp\"))       # Change schema of timestmap column as timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcf8e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+--------------+-------------------+\n",
      "|userID|movieID|score|timestamp_unix|          timestamp|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "|     1|      1|  4.0|     964982703|2000-07-30 19:45:03|\n",
      "|     1|      3|  4.0|     964981247|2000-07-30 19:20:47|\n",
      "|     1|      6|  4.0|     964982224|2000-07-30 19:37:04|\n",
      "|     1|     47|  5.0|     964983815|2000-07-30 20:03:35|\n",
      "|     1|     50|  5.0|     964982931|2000-07-30 19:48:51|\n",
      "|     1|     70|  3.0|     964982400|2000-07-30 19:40:00|\n",
      "|     1|    101|  5.0|     964980868|2000-07-30 19:14:28|\n",
      "|     1|    110|  4.0|     964982176|2000-07-30 19:36:16|\n",
      "|     1|    151|  5.0|     964984041|2000-07-30 20:07:21|\n",
      "|     1|    157|  5.0|     964984100|2000-07-30 20:08:20|\n",
      "|     1|    163|  5.0|     964983650|2000-07-30 20:00:50|\n",
      "|     1|    216|  5.0|     964981208|2000-07-30 19:20:08|\n",
      "|     1|    223|  3.0|     964980985|2000-07-30 19:16:25|\n",
      "|     1|    231|  5.0|     964981179|2000-07-30 19:19:39|\n",
      "|     1|    235|  4.0|     964980908|2000-07-30 19:15:08|\n",
      "|     1|    260|  5.0|     964981680|2000-07-30 19:28:00|\n",
      "|     1|    296|  3.0|     964982967|2000-07-30 19:49:27|\n",
      "|     1|    316|  3.0|     964982310|2000-07-30 19:38:30|\n",
      "|     1|    333|  5.0|     964981179|2000-07-30 19:19:39|\n",
      "|     1|    349|  4.0|     964982563|2000-07-30 19:42:43|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 17:15:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, movieID, score, timestamp\n",
      "Expected: score but found: rating\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923285b6",
   "metadata": {},
   "source": [
    "##### Now lets do the same steps above in single line operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8ff0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell 51 to initize the dataframe before this \n",
    "df = (\n",
    "    df\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")           # Renaming timestamp to timestamp_unix       \n",
    "    .withColumn(\"timestamp\", f.from_unixtime(\"timestamp_unix\")) # Creating new column timestamp after converting existing timestamp_unix column to human readable format using 'f.from_unixtime'\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(\"timestamp\"))       # Change schema of timestmap column as timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcbcb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+--------------+-------------------+\n",
      "|userID|movieID|score|timestamp_unix|          timestamp|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "|     1|      1|  4.0|     964982703|2000-07-30 19:45:03|\n",
      "|     1|      3|  4.0|     964981247|2000-07-30 19:20:47|\n",
      "|     1|      6|  4.0|     964982224|2000-07-30 19:37:04|\n",
      "|     1|     47|  5.0|     964983815|2000-07-30 20:03:35|\n",
      "|     1|     50|  5.0|     964982931|2000-07-30 19:48:51|\n",
      "+------+-------+-----+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- movieID: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/03 17:16:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: userId, movieId, rating, timestamp\n",
      " Schema: userID, movieID, score, timestamp\n",
      "Expected: score but found: rating\n",
      "CSV file: file:///study_docs/study_documents/notebooks/Pyspark_Guide/data-sets/ml-latest-small/ratings.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdaa379",
   "metadata": {},
   "source": [
    "#### Lets do all above steps into single step to make \"ratings\" dataframe\n",
    "- Create dataframe \n",
    "- set schemas\n",
    "- Set original timestamp to a column called timestamp_unix\n",
    "- Generate a new column named timestamp which convert unix timestamp to human readable \n",
    "- Set the schema of the new column to timestamp from INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4744b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a spark sql session \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f \n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyFirstCSVLoad\").getOrCreate()\n",
    "ratings_df = (\n",
    "    spark.read.csv(\n",
    "        path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\"\n",
    "    )\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b232fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+--------------+-------------------+\n",
      "|userId|movieId|rating|timestamp_unix|          timestamp|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "|     1|      1|   4.0|     964982703|2000-07-30 19:45:03|\n",
      "|     1|      3|   4.0|     964981247|2000-07-30 19:20:47|\n",
      "|     1|      6|   4.0|     964982224|2000-07-30 19:37:04|\n",
      "|     1|     47|   5.0|     964983815|2000-07-30 20:03:35|\n",
      "|     1|     50|   5.0|     964982931|2000-07-30 19:48:51|\n",
      "+------+-------+------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp_unix: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(5)\n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a3ef8",
   "metadata": {},
   "source": [
    "##### How to drop a column from dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb94be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      1|   4.0|2000-07-30 19:45:03|\n",
      "|     1|      3|   4.0|2000-07-30 19:20:47|\n",
      "|     1|      6|   4.0|2000-07-30 19:37:04|\n",
      "|     1|     47|   5.0|2000-07-30 20:03:35|\n",
      "|     1|     50|   5.0|2000-07-30 19:48:51|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below function will drop the data and will also show the result\n",
    "ratings_df.drop(\"timestamp_unix\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97117e",
   "metadata": {},
   "source": [
    "## Chapter 04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7641b48",
   "metadata": {},
   "source": [
    "#### Fixing issues in the data - Part 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda24543",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- Create movie dataframe from moves.csv\n",
    "- Filter the data with a specific keyword \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d48b34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dataframe from movies.csv\n",
    "\n",
    "movies_df = spark.read.csv(\n",
    "    path=\"data-sets/ml-latest-small/movies.csv\",\n",
    "    sep=\",\",\n",
    "    quote='\"',\n",
    "    header=True,\n",
    "    schema=\"movieId INT, title STRING, genres STRING \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ec53644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.show(15)\n",
    "movies_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132c91d",
   "metadata": {},
   "source": [
    "###### where() & f.col() - Filter the data with a specific keyword \n",
    "- movies_df.where() - used for filtering specific value\n",
    "- f.col(\"genres\")   - Used to access the column with the name \"genres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d348ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "848cac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+\n",
      "|movieId|               title|genres|\n",
      "+-------+--------------------+------+\n",
      "|      9| Sudden Death (1995)|Action|\n",
      "|     71|    Fair Game (1995)|Action|\n",
      "|    204|Under Siege 2: Da...|Action|\n",
      "|    251|  Hunted, The (1995)|Action|\n",
      "|    667|Bloodsport 2 (a.k...|Action|\n",
      "|   1170|Best of the Best ...|Action|\n",
      "|   1497|  Double Team (1997)|Action|\n",
      "|   1599|        Steel (1997)|Action|\n",
      "|   2196|    Knock Off (1998)|Action|\n",
      "|   2534|    Avalanche (1978)|Action|\n",
      "|   2817|Aces: Iron Eagle ...|Action|\n",
      "|   2965|Omega Code, The (...|Action|\n",
      "|   3283|Minnie and Moskow...|Action|\n",
      "|   3444|   Bloodsport (1988)|Action|\n",
      "|   3769|Thunderbolt and L...|Action|\n",
      "|   4200|Double Impact (1991)|Action|\n",
      "|   4387|Kiss of the Drago...|Action|\n",
      "|   4441|Game of Death (1978)|Action|\n",
      "|   4531|     Red Heat (1988)|Action|\n",
      "|   4568|Best of the Best ...|Action|\n",
      "+-------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.where(f.col(\"genres\") == 'Action').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334c4da",
   "metadata": {},
   "source": [
    "##### f.split() - Using split to create an Array \n",
    "- If you look at the above example it is only showing the exact match for the Genres \"Action\"\n",
    "- This is because the movie format is seperated with pipe (|) example \"Action|Crime|Thriller\"\n",
    "- So we need to split the delimiter pipe to covert this as an array "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739fef3",
   "metadata": {},
   "source": [
    "Lets create a new dataframe with the genere as a list from the main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b8386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc94409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating a new dataframe called movie_genre_df with an additional column 'generes_array' \n",
    "which contain the split values of genres with pipe\n",
    "Note : Split takes delimeter as regex, so we need to use escape sequance\n",
    "\"\"\"\n",
    "movie_genre_df = (\n",
    "    movies_df.withColumn(\"generes_array\", f.split(\"genres\", \"\\|\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c170e50",
   "metadata": {},
   "source": [
    "If you look at below output a new column created with type Array and has elements inside that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51607b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- generes_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_genre_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2e3db4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|       generes_array|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|[Adventure, Anima...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|[Adventure, Child...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|   [Comedy, Romance]|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|[Comedy, Drama, R...|\n",
      "|      5|Father of the Bri...|              Comedy|            [Comedy]|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|[Action, Crime, T...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|   [Comedy, Romance]|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|[Adventure, Child...|\n",
      "|      9| Sudden Death (1995)|              Action|            [Action]|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|[Action, Adventur...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_genre_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06a46d",
   "metadata": {},
   "source": [
    "##### f.explode - Lets extract each elemnts in the array and create new rows using that\n",
    "- Now lets Take out each element of that array and create that as a single row items per elements \n",
    "- f.explode() : Can help to create new rows from element in a specific array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2aac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab85902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below e are using f.explode on \"generes_array\" to create a new column \"genre\"\n",
    "movie_genre_df = movie_genre_df.withColumn(\"genre\", f.explode(\"generes_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47a5ada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+\n",
      "|movieId|           title|    genre|\n",
      "+-------+----------------+---------+\n",
      "|      1|Toy Story (1995)|Adventure|\n",
      "|      1|Toy Story (1995)|Animation|\n",
      "|      1|Toy Story (1995)| Children|\n",
      "|      1|Toy Story (1995)|   Comedy|\n",
      "|      1|Toy Story (1995)|  Fantasy|\n",
      "|      1|Toy Story (1995)|Adventure|\n",
      "|      1|Toy Story (1995)|Animation|\n",
      "|      1|Toy Story (1995)| Children|\n",
      "|      1|Toy Story (1995)|   Comedy|\n",
      "|      1|Toy Story (1995)|  Fantasy|\n",
      "|      1|Toy Story (1995)|Adventure|\n",
      "|      1|Toy Story (1995)|Animation|\n",
      "|      1|Toy Story (1995)| Children|\n",
      "|      1|Toy Story (1995)|   Comedy|\n",
      "|      1|Toy Story (1995)|  Fantasy|\n",
      "|      1|Toy Story (1995)|Adventure|\n",
      "|      1|Toy Story (1995)|Animation|\n",
      "|      1|Toy Story (1995)| Children|\n",
      "|      1|Toy Story (1995)|   Comedy|\n",
      "|      1|Toy Story (1995)|  Fantasy|\n",
      "+-------+----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- generes_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- genre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing only the columns what we needed\n",
    "movie_genre_df.select(\"movieId\", \"title\", \"genre\").show()\n",
    "movie_genre_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993c95b",
   "metadata": {},
   "source": [
    "###### Lets understand what are the unique (distinct) types of data available in a columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c55da",
   "metadata": {},
   "source": [
    "- Using this you can understand what are different types of data available in a column\n",
    "- Any invalid type data in that column\n",
    "- distinct(), is the function which we use to print the distict values in a columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee90b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_genres_df = movie_genre_df.select(\"genre\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "406a1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             genre|\n",
      "+------------------+\n",
      "|             Crime|\n",
      "|           Romance|\n",
      "|          Thriller|\n",
      "|         Adventure|\n",
      "|             Drama|\n",
      "|               War|\n",
      "|       Documentary|\n",
      "|           Fantasy|\n",
      "|           Mystery|\n",
      "|           Musical|\n",
      "|         Animation|\n",
      "|         Film-Noir|\n",
      "|(no genres listed)|\n",
      "|              IMAX|\n",
      "|            Horror|\n",
      "|           Western|\n",
      "|            Comedy|\n",
      "|          Children|\n",
      "|            Action|\n",
      "|            Sci-Fi|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "available_genres_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e83502",
   "metadata": {},
   "source": [
    "##### Now lets create a new dataframe which has only rows which do not have any genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "160a1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing column \"genre\" and filtering with keyword \"no genre listed\"\n",
    "\n",
    "movies_without_genre_df = movie_genre_df.where(f.col(\"genre\") == \"(no genres listed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d65705a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking the count of total rows in the dataframe\n",
    "\n",
    "movies_without_genre_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3d2a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+--------------------+------------------+\n",
      "|movieId|               title|            genres|       generes_array|             genre|\n",
      "+-------+--------------------+------------------+--------------------+------------------+\n",
      "| 114335|   La cravate (1957)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 122888|      Ben-hur (2016)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 122896|Pirates of the Ca...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 129250|   Superfast! (2015)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 132084| Let It Be Me (1995)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 134861|Trevor Noah: Afri...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 141131|    Guardians (2016)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 141866|   Green Room (2015)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 142456|The Brand New Tes...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 143410|          Hyena Road|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 147250|The Adventures of...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 149330|A Cosmic Christma...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 152037|  Grease Live (2016)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 155589|Noin 7 veljestä (...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 156605|            Paterson|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 159161|Ali Wong: Baby Co...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 159779|A Midsummer Night...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 161008|The Forbidden Dan...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 165489|Ethel & Ernest (2...|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "| 166024|     Whiplash (2013)|(no genres listed)|[(no genres listed)]|(no genres listed)|\n",
      "+-------+--------------------+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_without_genre_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421ab2c",
   "metadata": {},
   "source": [
    "## Chapter 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae8bf9",
   "metadata": {},
   "source": [
    "##### groupBy() - how to create goups based on specific columns values\n",
    "- groupBy() function will help you to group the data with a specific element\n",
    "- This can be combined with other function such as count, sum etc to produce valuble data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536afb8b",
   "metadata": {},
   "source": [
    "Lets group the movie_genre_df dataframe's genre with \"genre\" column and see the count of movies againt genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b881343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe that holds the count of total group of genres in the list\n",
    "movies_per_genre_df = movie_genre_df.groupBy(\"genre\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bec46cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             genre|count|\n",
      "+------------------+-----+\n",
      "|             Crime|12685|\n",
      "|           Romance|13313|\n",
      "|          Thriller|20039|\n",
      "|         Adventure|17185|\n",
      "|             Drama|29296|\n",
      "|               War| 3436|\n",
      "|       Documentary|  875|\n",
      "|           Fantasy|11133|\n",
      "|           Mystery| 7511|\n",
      "|           Musical| 3609|\n",
      "|         Animation| 8795|\n",
      "|         Film-Noir| 1054|\n",
      "|(no genres listed)|   34|\n",
      "|              IMAX| 2912|\n",
      "|            Horror| 7787|\n",
      "|           Western| 1481|\n",
      "|            Comedy|25150|\n",
      "|          Children| 8874|\n",
      "|            Action|21388|\n",
      "|            Sci-Fi|11443|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This shows the total count of each genre\n",
    "movies_per_genre_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e596cef",
   "metadata": {},
   "source": [
    "##### join() - Using join you can join two different dataframes together \n",
    "- This can be used for joining two different dataframes \n",
    "- There are various joins types available and default one is inner \n",
    "- Other sql joins such as outerjoin, left inner, left outer, right inner, right outer and cross joins available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef4477",
   "metadata": {},
   "source": [
    "To Know more about joins : https://learning.oreilly.com/videos/mastering-big-data/9781838640583/9781838640583-video3_4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e12c71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ratings.csv file (will be using this for joining with movies_df data frame)\n",
    "ratings_df = (\n",
    "    spark.read.csv(\n",
    "        path=\"data-sets/ml-latest-small/ratings.csv\",\n",
    "        sep=\",\",\n",
    "        quote='\"',\n",
    "        header=True,\n",
    "    )\n",
    "    .withColumnRenamed(\"timestamp\", \"timestamp_unix\")\n",
    "    .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp_unix\")))\n",
    "    .drop(\"timestamp_unix\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a629c70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      1|   4.0|2000-07-30 19:45:03|\n",
      "|     1|      3|   4.0|2000-07-30 19:20:47|\n",
      "|     1|      6|   4.0|2000-07-30 19:37:04|\n",
      "|     1|     47|   5.0|2000-07-30 20:03:35|\n",
      "|     1|     50|   5.0|2000-07-30 19:48:51|\n",
      "|     1|     70|   3.0|2000-07-30 19:40:00|\n",
      "|     1|    101|   5.0|2000-07-30 19:14:28|\n",
      "|     1|    110|   4.0|2000-07-30 19:36:16|\n",
      "|     1|    151|   5.0|2000-07-30 20:07:21|\n",
      "|     1|    157|   5.0|2000-07-30 20:08:20|\n",
      "|     1|    163|   5.0|2000-07-30 20:00:50|\n",
      "|     1|    216|   5.0|2000-07-30 19:20:08|\n",
      "|     1|    223|   3.0|2000-07-30 19:16:25|\n",
      "|     1|    231|   5.0|2000-07-30 19:19:39|\n",
      "|     1|    235|   4.0|2000-07-30 19:15:08|\n",
      "|     1|    260|   5.0|2000-07-30 19:28:00|\n",
      "|     1|    296|   3.0|2000-07-30 19:49:27|\n",
      "|     1|    316|   3.0|2000-07-30 19:38:30|\n",
      "|     1|    333|   5.0|2000-07-30 19:19:39|\n",
      "|     1|    349|   4.0|2000-07-30 19:42:43|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f73738",
   "metadata": {},
   "source": [
    "Now lets do a inner join between movies_df and ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d5f282fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe with the inner join from movies_df with ratings_df\n",
    "opinion_df = movies_df.join(ratings_df, [\"movieId\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bfa58443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+------+-------------------+\n",
      "|movieId|               title|              genres|userId|rating|          timestamp|\n",
      "+-------+--------------------+--------------------+------+------+-------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|     1|   4.0|2000-07-30 19:45:03|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|     1|   4.0|2000-07-30 19:20:47|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|     1|   4.0|2000-07-30 19:37:04|\n",
      "|     47|Seven (a.k.a. Se7...|    Mystery|Thriller|     1|   5.0|2000-07-30 20:03:35|\n",
      "|     50|Usual Suspects, T...|Crime|Mystery|Thr...|     1|   5.0|2000-07-30 19:48:51|\n",
      "|     70|From Dusk Till Da...|Action|Comedy|Hor...|     1|   3.0|2000-07-30 19:40:00|\n",
      "|    101|Bottle Rocket (1996)|Adventure|Comedy|...|     1|   5.0|2000-07-30 19:14:28|\n",
      "|    110|   Braveheart (1995)|    Action|Drama|War|     1|   4.0|2000-07-30 19:36:16|\n",
      "|    151|      Rob Roy (1995)|Action|Drama|Roma...|     1|   5.0|2000-07-30 20:07:21|\n",
      "|    157|Canadian Bacon (1...|          Comedy|War|     1|   5.0|2000-07-30 20:08:20|\n",
      "|    163|    Desperado (1995)|Action|Romance|We...|     1|   5.0|2000-07-30 20:00:50|\n",
      "|    216|Billy Madison (1995)|              Comedy|     1|   5.0|2000-07-30 19:20:08|\n",
      "|    223|       Clerks (1994)|              Comedy|     1|   3.0|2000-07-30 19:16:25|\n",
      "|    231|Dumb & Dumber (Du...|    Adventure|Comedy|     1|   5.0|2000-07-30 19:19:39|\n",
      "|    235|      Ed Wood (1994)|        Comedy|Drama|     1|   4.0|2000-07-30 19:15:08|\n",
      "|    260|Star Wars: Episod...|Action|Adventure|...|     1|   5.0|2000-07-30 19:28:00|\n",
      "|    296| Pulp Fiction (1994)|Comedy|Crime|Dram...|     1|   3.0|2000-07-30 19:49:27|\n",
      "|    316|     Stargate (1994)|Action|Adventure|...|     1|   3.0|2000-07-30 19:38:30|\n",
      "|    333|    Tommy Boy (1995)|              Comedy|     1|   5.0|2000-07-30 19:19:39|\n",
      "|    349|Clear and Present...|Action|Crime|Dram...|     1|   4.0|2000-07-30 19:42:43|\n",
      "+-------+--------------------+--------------------+------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_df.show()\n",
    "opinion_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee0a2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataframe with the Outer join from movies_df with ratings_df\n",
    "opinion_outer_df = movies_df.join(ratings_df, [\"movieId\"], \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7bd963c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+------+------+-------------------+\n",
      "|movieId|               title|genres|userId|rating|          timestamp|\n",
      "+-------+--------------------+------+------+------+-------------------+\n",
      "|    148|Awfully Big Adven...| Drama|   191|   5.0|1996-04-17 18:08:17|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|    32|   3.0|1997-02-23 22:32:45|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|    57|   3.0|2000-09-24 01:00:04|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|    91|   1.0|2005-04-05 16:10:17|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   104|   4.5|2009-03-26 23:45:29|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   133|   4.0|1996-09-23 16:16:33|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   136|   4.0|1996-05-18 21:07:38|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   171|   3.0|1997-06-21 16:08:03|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   176|   5.0|1996-08-15 12:37:55|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   182|   4.5|2003-06-05 03:20:44|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   216|   3.0|2000-11-26 04:24:01|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   217|   2.0|2000-04-17 04:55:27|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   218|   4.0|2005-03-24 00:41:14|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   260|   4.5|2005-02-26 09:17:35|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   273|   5.0|1996-06-27 08:42:28|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   287|   4.5|2005-03-07 21:38:56|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   312|   4.0|2003-01-21 18:59:24|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   357|   3.5|2012-09-26 03:38:02|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   372|   3.0|1997-09-16 14:05:26|\n",
      "|    471|Hudsucker Proxy, ...|Comedy|   373|   5.0|1996-11-01 06:39:48|\n",
      "+-------+--------------------+------+------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100854"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_outer_df.show()\n",
    "opinion_outer_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337190d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
