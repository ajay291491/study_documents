#---------------------------------------------------------------------------------------------
#  Chapter 01 - Introduction to Docker
#---------------------------------------------------------------------------------------------
# * Install Docker
# You can install Docker by install below two packages 
# | 
# | [root@dockerstation01 ~]# rpm -qa| grep -i docker
# | [root@dockerstation01 ~]# rpm -qa| grep -i docker
# | docker-ce-cli-18.09.6-3.el7.x86_64
# | docker-ce-18.09.1-3.el7.x86_64
# | [root@dockerstation01 ~]#
# | 
#
# * Verify the installation 
# You can verify the instllation of docker server and client by running below command 
# | 
# | [root@dockerstation01 ~]# docker version
# | Client:
# |  Version:           18.09.6
# |  API version:       1.39
# |  Go version:        go1.10.8
# |  Git commit:        481bc77156
# |  Built:             Sat May  4 02:34:58 2019
# |  OS/Arch:           linux/amd64
# |  Experimental:      false
# |
# | Server: Docker Engine - Community
# |  Engine:
# |   Version:          18.09.1
# |   API version:      1.39 (minimum version 1.12)
# |   Go version:       go1.10.6
# |   Git commit:       4c52b90
# |   Built:            Wed Jan  9 19:06:30 2019
# |   OS/Arch:          linux/amd64
# |   Experimental:     false
# | [root@dockerstation01 ~]#
# |
#
# * How to start a container 
# To start a container you will need to run the 'docker run' command. When you start a container there are few steps happens in the background 
#
# Syntax : docker run <image_name>
#
# Below are few steps which is outlined in the chronological order while running a docker run command 
#
# 1. When you run the command docker client contacts the docker server for image location
# 2. Then Docker server verifies the image in its cache, if its available it helps to start the image from the cache
# 3. Incase there is no cache available then it forwards the requst to container image repository "Docker Hub'
# 4. It checks for the image in the docker hub and then pulls that image int0 local repository or cache 
# 5. Then docker client will start the container from the image which has downloaded.
#
# Example : Below is an example to start a 'hello-world' program. 
#
# | [root@dockerstation01 ~]# docker run hello-world
# |
# | Hello from Docker!
# | This message shows that your installation appears to be working correctly.
# |
# | To generate this message, Docker took the following steps:
# |  1. The Docker client contacted the Docker daemon.
# |  2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
# |     (amd64)
# |  3. The Docker daemon created a new container from that image which runs the
# |     executable that produces the output you are currently reading.
# |  4. The Docker daemon streamed that output to the Docker client, which sent it
# |     to your terminal.
# |
# | To try something more ambitious, you can run an Ubuntu container with:
# |  $ docker run -it ubuntu bash
# |
# | Share images, automate workflows, and more with a free Docker ID:
# |  https://hub.docker.com/
# |
# | For more examples and ideas, visit:
# |  https://docs.docker.com/get-started/
# |
# | [root@dockerstation01 ~]#
# |
# NOTE : Below tutorial will explain about how docker run works
# https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436626?start=79#bookmarks
#
#
# * What happens when you install docker container
# When you install docker container there will be a Linux virtual machine gets installed in the background which acts as the Docker Server
# You can see docker server operating system details in the 'docker version' command to verify this. 
# Any container process which runs will be from this container, this will be true regardless the underlying OS is Linux, Windows or Mac.
#
# Docker installation layering is done like below. 
# - Underlying Operating system (Windows / Linux / Mac)
# - Docker Server Linux VM gets created on top of the OS 
# - Docker contaner process running inside the Docker Server Linux VM
#
# NOTE : Udemy Detailed Tutorial on the Docker layers and how its works 
# https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436636?start=1#bookmarks
#
#
# * What is a Docker Image and container
# Docker image is a collection of software and filesystem snapshot along with a set of steps which can be used to startup a container.
# Container is a set of process which spins up with the instruction given from the docker images
# 
# NOTE : Below udemy tutorial gives a complete details about container image. 
# https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436632?start=422#bookmarks 
#
# 
# * What is namespacing and cgroups
# namespacing - namespacing a Linux feature which allows to isolate resourse per processs or group of process 
# cgroups     - cgroups also a Linux feature which limit the amount of resources used per process 
#
# NOTE : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436636?start=63#bookmarks
#
#---------------------------------------------------------------------------------------------
#  Chapter 02 - Manipulating a docker Container
#---------------------------------------------------------------------------------------------
#
# * Executing commands in Container 
# When you are running a command insider a docker container that will work only if that contaner image contains that commands embedded inside. 
# If the container image is not embedded with the commands then the command won't work 
#
# Examples : Below are two instances where the same command behave in different while executing in two different containers
#
# | [root@dockerstation01 ~]# docker run hello-world uptime
# | docker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "exec: \"uptime\": executable file not found in $PATH": unknown.
# | [root@dockerstation01 ~]# docker run busybox uptime
# |  23:26:30 up  1:53,  0 users,  load average: 0.07, 0.02, 0.01
# | [root@dockerstation01 ~]# 
#
# Note : Here uptime command was not embedded in the 'hello-world' image where its available in 'busybox' 
#
# To run a command in a docker container you will need to follow below syntax 
#
# syntax : docker run <image> <command>
#
# * ps command
# This command can be used to find the list of running continers. 
# When you add '--all' parameter to the command then its going to give the details about all the commands ran in the past
#
# Syntax : docker ps 
#        : docker ps --all
#
# Example : Running the command to show currently running containers
#
# | [root@dockerstation01 ~]# docker ps 
# | CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
# | 3d488796f6ab        busybox             "top"               6 seconds ago       Up 4 seconds                            optimistic_mahavira
# | [root@dockerstation01 ~]# 
#
# Example : Running the command to show the history of all containers 
#
# | [root@dockerstation01 ~]# docker ps --all
# | CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                        PORTS               NAMES
# | 3d488796f6ab        busybox             "top"                    About a minute ago   Up About a minute                                 optimistic_mahavira
# | ce70ccd4eaba        busybox             "uptime"                 7 minutes ago        Exited (0) 7 minutes ago                          blissful_fermat
# | 9e3a5166cd32        hello-world         "uptime"                 7 minutes ago        Created                                           sleepy_babbage
# | df515bd2bc4a        busybox             "ls etc"                 19 minutes ago       Exited (0) 19 minutes ago                         romantic_goldberg
# | 7b6d14cdc48c        busybox             "ls"                     19 minutes ago       Exited (0) 19 minutes ago                         focused_kapitsa
# | 1af30959d59f        busybox             "systemctl -a"           19 minutes ago       Created                                           epic_beaver
# | 4356fa5da624        busybox             "uname -a"               19 minutes ago       Exited (0) 19 minutes ago                         reverent_wu
# | feec6f069754        busybox             "cat /etc/redhat-rel…"   20 minutes ago       Exited (1) 20 minutes ago                         festive_edison
# | bbfc0aaa39d9        busybox             "uname -r"               20 minutes ago       Exited (0) 20 minutes ago                         eloquent_noyce
# | cbbe3a745190        busybox             "top"                    20 minutes ago       Exited (130) 20 minutes ago                       elegant_banzai
# | dd4a37b07df2        busybox             "ping yahoo.com"         20 minutes ago       Exited (1) 20 minutes ago                         xenodochial_cori
# | 188cf85a7b3f        busybox             "ping google.com"        21 minutes ago       Exited (1) 20 minutes ago                         sad_shockley
# | e875caa538be        busybox             "ping"                   21 minutes ago       Exited (1) 21 minutes ago                         peaceful_snyder
# | 5f88bfa642d2        busybox             "uptime"                 22 minutes ago       Exited (0) 22 minutes ago                         hopeful_payne
# | 486ffb75e744        busybox             "sh"                     22 minutes ago       Exited (0) 22 minutes ago                         hopeful_heyrovsky
# | dd0a1203e18c        hello-world         "uptime"                 31 minutes ago       Created                                           pedantic_jackson
# | f7dfeb40ecde        hello-world         "ls"                     31 minutes ago       Created                                           compassionate_fermi
# | 9b7a8dc92eff        hello-world         "/hello"                 2 hours ago          Exited (0) 2 hours ago                            festive_colden
# | 64392280e628        hello-world         "/hello"                 2 weeks ago          Exited (0) 2 weeks ago                            sleepy_sinoussi
# | 779c5c963c0c        hello-world         "/hello"                 2 weeks ago          Exited (0) 2 weeks ago                            clever_heyrovsky
# | [root@dockerstation01 ~]#
# |
#
# * Understanding the docker run 
# When you are invoking the docker run command it actually a combination of docker create and docker start commands. 
#
# docker run = docker create <image_name> + docker run <container ID>
#
# * Docker create 
# Docker create will help you to create a writable container from a container image. 
# This is useful when you want to create a container with some specific command which need to be in a ready to create state later. 
# Once you create the container the container state will be in the "Created" status. 
#
# Syntax : docker create <image_name>
#
# Example : Below example is to create a fedora container
#
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker create ubuntu
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | Trying to pull registry.redhat.io/ubuntu:latest...Failed
# | Trying to pull quay.io/ubuntu:latest...Failed
# | Trying to pull docker.io/ubuntu:latest...Getting image source signatures
# | Copying blob 6abc03819f3e: 27.52 MiB / 27.52 MiB [==========================] 1s
# | Copying blob 05731e63f211: 844 B / 844 B [==================================] 1s
# | Copying blob 0bd67c50d6be: 164 B / 164 B [==================================] 1s
# | Copying config 7698f282e524: 3.27 KiB / 3.27 KiB [==========================] 0s
# | Writing manifest to image destination
# | Storing signatures
# | 348902c0561cc725828c4f79a7206cd19d6a3789567f158043aedb0610d6aa50
# | [ec2-user@ip-172-31-21-124 ~]$
# |
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND    CREATED        STATUS                 PORTS  NAMES
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash  3 seconds ago  Created                       gifted_booth  --> You can see the container in the Created State
# | 779800ec7966  docker.io/library/busybox:latest      sh         2 days ago     Exited (0) 2 days ago         admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello     2 days ago     Exited (0) 2 days ago         kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# |
#
# Detail Tutoroial : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436650?start=79#bookmarks
#
# * Docker start
# Docker start will help you to start the container which is already  created. 
# This will be useful when you want to start a conatiner again and again which is already created 
#
# Syntax : docker start [-a] <container_ID>
#
# If you are using "-a" string then it will attach the startingh output to the terminal, i.e STDOUT, STDERR and forward will be printed on the console 
#
# Example : Below is an example for starting a container which is already created 
#
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker create ubuntu uptime
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | fc4f9980fe0addc25a7584f360a6dc82a0fefa9433ff8b0ea9e970da935ca33d
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND    CREATED         STATUS                         PORTS  NAMES
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime     3 seconds ago   Created                               admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash  10 minutes ago  Exited (0) About a minute ago         gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh         2 days ago      Exited (0) 2 days ago                 admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello     2 days ago      Exited (0) 2 days ago                 kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker start -a fc4f9980fe0a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# |  00:36:39 up 2 days, 37 min,  0 users,  load average: 0.05, 0.06, 0.02
# | [ec2-user@ip-172-31-21-124 ~]$
#
# * Restarting a container 
# When a container is exited then it doesnot mean that it is dead or it cannot be started again 
# You can always restart the same container using the docker start method which will help you to re-issue the same default command. 
# Kindly note when you are starting an exited container you won't be able to attach or trigger new commands to it, it will only pick the default command
#
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND    CREATED         STATUS                     PORTS  NAMES
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime     19 minutes ago  Exited (0) 19 minutes ago         admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash  30 minutes ago  Exited (0) 21 minutes ago         gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh         2 days ago      Exited (0) 2 days ago             admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello     2 days ago      Exited (0) 2 days ago             kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker start -a fc4f9980fe0a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# |  00:56:26 up 2 days, 57 min,  0 users,  load average: 0.00, 0.00, 0.00
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker start -a fc4f9980fe0a ls			-> Note that same container doesnot allow to override the default command
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | you cannot start and attach multiple containers at once
# | [ec2-user@ip-172-31-21-124 ~]$
# |
#
# Detail Tutorial : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436652?start=139#bookmarks
#
# * How to delete a container
# If you no longer need the container instance then you can delete them by using 'docker rm' or 'docker system prune' command 
#
# Syntax : docker rm [conatiner_ID | all]
# Syntax : docker system prune           --> This will delete all containers 
#
# | [ec2-user@ip-172-31-21-124 ~]$ docker ps --all
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                             COMMAND  CREATED            STATUS                        PORTS  NAMES
# | f1beb2b1f2db  docker.io/library/busybox:latest  uptime   About an hour ago  Exited (0) About an hour ago         suspicious_wiles
# | d782303fb193  docker.io/library/busybox:latest  uptime   About an hour ago  Exited (0) About an hour ago         priceless_darwin
# | 70cf55e7554e  docker.io/library/busybox:latest  sh       About an hour ago  Exited (0) About an hour ago         gracious_hamilton
# | 839d4ec68f32  docker.io/library/busybox:latest  sh       About an hour ago  Created                              optimistic_khorana
# | ac96670a3ff1  docker.io/library/busybox:latest  uptime   2 hours ago        Exited (0) 2 hours ago               pedantic_shirley
# | db8671b4bc4f  docker.io/library/busybox:latest  ls       2 hours ago        Exited (0) 2 hours ago               cocky_clarke
# | a7bce2f35192  docker.io/library/busybox:latest  ls       2 hours ago        Exited (0) 2 hours ago               vigorous_bassi
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ docker rm f1beb2b1f2db                           --> Deleteing only one Container
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | f1beb2b1f2db4040ea21e964999f706c932de4b76baef80b86f149e4866ccf63
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ docker rm --all				    --> Deleting all containers 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 70cf55e7554edaa85ba8c6eb2ffa05c545b83491355ee48701d47f4bcb56ca9f
# | ac96670a3ff19ac728ee0739333198de315ab3feed187d43f1f8da1cc2848e10
# | 839d4ec68f32dfe7d18e75e5ffa6fec6f176e8357770c4c938ae1531d357f31b
# | a7bce2f3519201c38bf07211a8248d131e96d475641ca4d4523dff532b915989
# | d782303fb1936afd32f9216c0255c702dca3cd5b4c10131558dd22c0db9e9c37
# | db8671b4bc4f9eaf5f119cca11823fa898c08a096e683c4b091e22f284710f07
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ docker ps --all
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE  COMMAND  CREATED  STATUS  PORTS  NAMES
# | [ec2-user@ip-172-31-21-124 ~]$
# |
#
# * Logs in docker 
# You can checks the logs from a container using the docker logs command
#
# Syntax : docker logs <conatiner_ID>
#
# Example: Below example show how to get the logs for a specific container 
#
# |
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps --all
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND    CREATED            STATUS                        PORTS  NAMES
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime     About an hour ago  Exited (0) 25 minutes ago            admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash  About an hour ago  Exited (0) About an hour ago         gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh         2 days ago         Exited (0) 2 days ago                admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello     2 days ago         Exited (0) 2 days ago                kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker logs fc4f9980fe0a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# |  00:36:39 up 2 days, 37 min,  0 users,  load average: 0.05, 0.06, 0.02
# |  00:56:26 up 2 days, 57 min,  0 users,  load average: 0.00, 0.00, 0.00
# | [ec2-user@ip-172-31-21-124 ~]$
# |
# 
# * Stopping a container 
# To stop a running container you have two methods either you can use the 'stop' method you can use the 'kill' method.
# Stop - This is a graceful method where issuing the kill method it wait for some seconds so that continer has bring down the process
# Kill - This method will immedietely issue the sigterm signal to kill the container 
#
# Syntax : docker stop <Container_ID>
#        : docker kill <Container_ID>
#
# Example : Below example will show us about killing the docker container 
# 
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker create busybox ping google.com			--> Creating a long running container
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 0c1192c600ae61e1a825cb0fb03f1ec7f5939d6d5e3881c2697cf20af9bcae25
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND          CREATED        STATUS                   PORTS  NAMES
# | 0c1192c600ae  docker.io/library/busybox:latest      ping google.com  3 seconds ago  Created                         friendly_lamport	--> Created 
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime           24 hours ago   Exited (0) 23 hours ago         admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash        24 hours ago   Exited (0) 24 hours ago         gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh               3 days ago     Exited (0) 3 days ago           admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello           3 days ago     Exited (0) 3 days ago           kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker start 0c1192c600ae										--> Starting 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 0c1192c600ae
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND          CREATED         STATUS                   PORTS  NAMES
# | 0c1192c600ae  docker.io/library/busybox:latest      ping google.com  40 seconds ago  Up 26 seconds ago               friendly_lamport	--> up and running 
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime           24 hours ago    Exited (0) 23 hours ago         admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash        24 hours ago    Exited (0) 24 hours ago         gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh               3 days ago      Exited (0) 3 days ago           admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello           3 days ago      Exited (0) 3 days ago           kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker stop 0c1192c600ae										--> graceful shutdown
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 0c1192c600ae61e1a825cb0fb03f1ec7f5939d6d5e3881c2697cf20af9bcae25
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker start 0c1192c600ae
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 0c1192c600ae
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker kill 0c1192c600ae										--> Killed immedietely 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 0c1192c600ae61e1a825cb0fb03f1ec7f5939d6d5e3881c2697cf20af9bcae25
# | [ec2-user@ip-172-31-21-124 ~]$
# |
#
# Udemy Detailed : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436664?start=96#bookmarks
#
# * How to run a command inside a container 
# While a container is running there will be requirement for you to run the commands and fetch output from them. 
# While you ran the command it will interact with the container and then it will provide the output from the container. 
# When you try to ran an output from the command, then you will need to provide container id to fatch the output 
#
# Syntax : docker exec -it <Container_ID> <command>
#
# Example : Below command will help you to fetch uptime from a running container 
#
# | ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND          CREATED         STATUS                         PORTS  NAMES
# | 0c1192c600ae  docker.io/library/busybox:latest      ping google.com  21 minutes ago  Exited (137) 19 minutes ago           friendly_lamport
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime           24 hours ago    Exited (0) 24 hours ago               admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash        24 hours ago    Exited (0) About a minute ago         gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh               3 days ago      Exited (0) 3 days ago                 admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello           3 days ago      Exited (0) 3 days ago                 kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker start 0c1192c600ae
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | 0c1192c600ae
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker exec -it 0c1192c600ae uptime
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# |  00:28:51 up 3 days, 29 min,  0 users,  load average: 0.00, 0.00, 0.00
# | [ec2-user@ip-172-31-21-124 ~]$
#
# NOTE : Here "-it' flag which we have used is important, reason is it helps  us to interact with the contaciner in a ternimal
# -i  : This means attch my terminal to container while running the comamnds 
# -t  : this means output should be printed in a nice terminal format
#
# * How to launch a shell insider a container
# Instead of running command all the time using 'exec' method you can always launch a shell in container. 
# You can launch the shell with 'sh' or 'zsh' along with the exec input for command. 
#
# Syntax : docker exec -it <container_id> [sh | zsh]
#
# Example : Below command will launch a shell 
#
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                             COMMAND          CREATED       STATUS           PORTS  NAMES
# | 0c1192c600ae  docker.io/library/busybox:latest  ping google.com  23 hours ago  Up 23 hours ago         friendly_lamport
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker exec -it 0c1192c600ae sh
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | / # uname -a
# | Linux 0c1192c600ae 4.18.0-80.el8.x86_64 #1 SMP Wed Mar 13 12:02:46 UTC 2019 x86_64 GNU/Linux
# | / # more /etc/
# | group        hostname     hosts        localtime    network/     passwd       resolv.conf  shadow
# | / # ls
# | bin   dev   etc   home  proc  root  run   sys   tmp   usr   var
# | / # exit
# | [ec2-user@ip-172-31-21-124 ~]$
# |
#
# * How to start a container with a shell 
# You can always launch a container with a shell by attaching the 'sh' command with '-it' flag while running the container
# The downside for this approach will be you will be only running that shell with the container and there won't be any apps running 
# As soon as you exit the shell it will terminate the session. 
#
# Syntax : docker run -it <image_name> sh
#
# Example : below example will show you how to launch a shell 
#
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND               CREATED       STATUS                     PORTS  NAMES
# | 1016f0da1363  docker.io/library/mariadb:10.3        docker-entrypoint...  22 hours ago  Exited (1) 10 minutes ago         xenodochial_euler
# | 0c1192c600ae  docker.io/library/busybox:latest      ping google.com       23 hours ago  Up 23 hours ago                   friendly_lamport
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime                47 hours ago  Exited (0) 47 hours ago           admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash             47 hours ago  Exited (0) 23 hours ago           gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh                    3 days ago    Exited (0) 3 days ago             admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello                3 days ago    Exited (0) 3 days ago             kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker run -i -t busybox sh
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | / #
# | / #
# | / # pwd
# | /
# | / # ls
# | bin   dev   etc   home  proc  root  run   sys   tmp   usr   var
# | / # ls /var
# | spool  www
# | / # ls /etc/init.d
# | ls: /etc/init.d: No such file or directory
# | / # exit
# | [ec2-user@ip-172-31-21-124 ~]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                 COMMAND               CREATED         STATUS                     PORTS  NAMES
# | cc80521b1b45  docker.io/library/busybox:latest      sh                    54 seconds ago  Exited (1) 18 seconds ago         nervous_turing
# | 1016f0da1363  docker.io/library/mariadb:10.3        docker-entrypoint...  22 hours ago    Exited (1) 11 minutes ago         xenodochial_euler
# | 0c1192c600ae  docker.io/library/busybox:latest      ping google.com       23 hours ago    Up 23 hours ago                   friendly_lamport
# | fc4f9980fe0a  docker.io/library/ubuntu:latest       uptime                47 hours ago    Exited (0) 47 hours ago           admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest       /bin/bash             47 hours ago    Exited (0) 23 hours ago           gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest      sh                    3 days ago      Exited (0) 3 days ago             admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest  /hello                3 days ago      Exited (0) 3 days ago             kind_kalam
# | [ec2-user@ip-172-31-21-124 ~]$
# |
#
# Udemy Detail : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436674?start=67#bookmarks
#
# * Container Isolation 
# When you are running multiple containers from same image they are created and stored in the filesystem seperately. 
# Which means there is no direct relationship between the filesystem created between the containers. 
# If you are creating a container A and Container B from the same image then you create few files under A, then that is going remain only in A and cannot see in B. 
# 
#
# * How push a image to docker hub 
# To push an image to docker hub you will need to login to docker hub and then use the command docker push 
# To push an image you will need to first tag a name to that image ID 
#
# Syntax : docker tag image-id/name repo_name/image_name 
#        : docker login 
#        : docker push 
#
# | [root@dockerstation01 complex]# docker tag hello-world ajay291491/hello-world
# | [root@dockerstation01 complex]# docker login -u ajay291491
# | Password: 
# | WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
# | Configure a credential helper to remove this warning. See
# | https://docs.docker.com/engine/reference/commandline/login/#credentials-store
# | 
# | Login Succeeded
# | [root@dockerstation01 complex]# docker images
# | REPOSITORY               TAG                 IMAGE ID            CREATED             SIZE
# | complex_worker           latest              cb9e5822e314        4 hours ago         84.9MB
# | complex_client           latest              dc444f3f7a57        4 hours ago         253MB
# | complex_api              latest              19007a09a8b7        4 hours ago         94.5MB
# | <none>                   <none>              756a94798524        4 hours ago         94.5MB
# | <none>                   <none>              ddc5c178f21f        4 hours ago         79.9MB
# | <none>                   <none>              9a7bb4f8daff        4 hours ago         79.9MB
# | <none>                   <none>              9fcfb78b97ca        5 hours ago         79.9MB
# | complex_nginx            latest              0aa4b1b74433        5 hours ago         126MB
# | postgres                 latest              25252fcb432a        32 hours ago        312MB
# | node                     alpine              8fd54fb9eb5c        33 hours ago        79.9MB
# | redis                    latest              857c4ab5f029        8 days ago          98.2MB
# | nginx                    latest              e445ab08b2be        2 weeks ago         126MB
# | ajay291491/hello-world   latest              fce289e99eb9        7 months ago        1.84kB
# | hello-world              latest              fce289e99eb9        7 months ago        1.84kB
# | [root@dockerstation01 complex]# docker push ajay291491/hello-world
# | The push refers to repository [docker.io/ajay291491/hello-world]
# | af0b15c8625b: Mounted from library/hello-world 
# | latest: digest: sha256:92c7f9c92844bbbb5d0a101b22f7c2a7949e40f8ea90c8b3bc396879d95e899a size: 524
# | [root@dockerstation01 complex]#
#
# Udemy Detail : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436678?start=4#bookmarks
# 
#---------------------------------------------------------------------------------------------
#  Chapter 03 - Building custom images through Docker Server
#---------------------------------------------------------------------------------------------
#
# * Docker Build
# For doing docker build it uses 'Dockerfile' which is explained in detail in the next session. 
# Once you call the docker build it goes theough a set of process 
# 
# Process : Dockerfile => Docker Client (we interact with build command) => Docker Server (which process the request) => Process and Create a usable image 
#
# Syntax : vi Dockerfile
#          docker build . 
#
# Note : here '.' is denoted as the build context and Dockerfile should always start with 'D'
#
# * Docker file
# Docker build is normally done through a dockerfile and it has mainly three sessions 'Build', 'Run' and 'Command'
# At each stage there are certain process takes place and which will result in creating the image. 
#
# Build : At this stage you will need to specify the location from which you are going to download the base image. 
#       : Upon completion of the this stage base image will be downloaded and it will spin up a temporary container which can be said as a staging conatiner 
#
# Run   : Here will be mentioning the list of software we need to install etc
#       : At this stage the staging cotainer which created above will be used to install all packages within it. 
#       : Once its completes its phase it updates or save the image and destroy the staging container. 
#
# CMD   : At this stage it will spin up another staging container and then it will update the startup command etc to that container. 
#       : Once this stage is done, then it will update save the container to the image and then destroy the container. 
#
# Udemy Detail : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436706?start=228#bookmarks
#
# * Creating a Dockerfile 
# You can setup a directory to create a docker image, then setup a docker file similar to below 
#
# | [ec2-user@ip-172-31-21-124 docker_local_hub]$ cd /apps/docker_local_hub/apache/    --> make a directory specific to that build process
# | [ec2-user@ip-172-31-21-124 apache]$ ls
# | Dockerfile
# | [ec2-user@ip-172-31-21-124 apache]$ 
# | [ec2-user@ip-172-31-21-124 apache]$ more Dockerfile
# | # BUILD - What base image we are going to use here
# | FROM centos
# |
# | # RUN - Download and install dependancy for the new image
# | RUN yum install httpd -y -q 
# |
# | # CMD - Tell what command or action it need to perform during startup
# | CMD ["/usr/sbin/httpd”, “-D”, “FOREGROUND”]
# |
# | [ec2-user@ip-172-31-21-124 apache]$
# |
#
# * Building a Docker Image 
# Once you have created the Dockerfile the you can run the build process as you are seeing below. 
# Once the build process completes then you can run the container from the created image  
#
# | [ec2-user@ip-172-31-21-124 apache]$ more Dockerfile 
# | # BUILD - What base image we are going to use here 
# | FROM centos
# | 
# | # RUN - Download and install dependancy for thennnnn21 new image 
# | RUN yum install httpd -y -q
# | 
# | # CMD - Tell what command or action it need to perform during startup
# | CMD ["/usr/sbin/httpd"]
# | 
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker build . 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | STEP 1: FROM centos
# | STEP 2: RUN yum install httpd -y -q
# | warning: /var/cache/yum/x86_64/7/base/packages/apr-util-1.5.2-6.el7.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# | Public key for apr-util-1.5.2-6.el7.x86_64.rpm is not installed
# | Public key for httpd-tools-2.4.6-89.el7.centos.x86_64.rpm is not installed
# | Importing GPG key 0xF4A80EB5:
# |  Userid     : "CentOS-7 Key (CentOS 7 Official Signing Key) <security@centos.org>"
# |  Fingerprint: 6341 ab27 53d7 8a78 a7c2 7bb1 24c6 a8a7 f4a8 0eb5
# |  Package    : centos-release-7-6.1810.2.el7.centos.x86_64 (@CentOS)
# |  From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
# | --> 750a90fe719bd9dd6162fc8fce5092c762fb729b23ee9d178d6131e2e2788f1c
# | STEP 3: FROM 750a90fe719bd9dd6162fc8fce5092c762fb729b23ee9d178d6131e2e2788f1c
# | STEP 4: CMD ["/usr/sbin/httpd", "-DNO_DETACH"]
# | --> 2ed0af508b4fbe38557e7c9beccae772203d2dcd4747fb3498c0118eef161301
# | STEP 5: COMMIT 
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker image list 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | REPOSITORY                      TAG      IMAGE ID       CREATED              SIZE
# | <none>                          <none>   2ed0af508b4f   About a minute ago   347 MB    ==> New image created
# | docker.io/library/fedora        latest   289289d1a15b   2 weeks ago          326 MB
# | docker.io/library/busybox       latest   ef46e0caa533   2 weeks ago          1.43 MB
# | docker.io/library/mariadb       10.3     56089178535f   2 weeks ago          356 MB
# | docker.io/library/mariadb       latest   56089178535f   2 weeks ago          356 MB
# | docker.io/library/ubuntu        latest   7698f282e524   5 weeks ago          72.3 MB
# | docker.io/library/centos        latest   9f38484d220f   3 months ago         209 MB
# | docker.io/library/hello-world   latest   fce289e99eb9   5 months ago         5.62 kB
# | [ec2-user@ip-172-31-21-124 apache]$
# | [ec2-user@ip-172-31-21-124 apache]$
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker run 2ed0af508b4f    --> Starting a container from the created image
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.88.0.30. Set the 'ServerName' directive globally to suppress this message
# | [ec2-user@ip-172-31-21-124 apache]$
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker ps 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                                             COMMAND               CREATED         STATUS             PORTS  NAMES
# | 76fc5b8c1a22  2ed0af508b4fbe38557e7c9beccae772203d2dcd4747fb3498c0118eef161301  /usr/sbin/httpd -...  18 seconds ago  Up 18 seconds ago         wonderful_poincare  ==> New Container
# | 0c1192c600ae  docker.io/library/busybox:latest                                  ping google.com       10 days ago     Up 10 days ago            friendly_lamport
# | [ec2-user@ip-172-31-21-124 apache]$ 
# | 
#
# Udemy Detail : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436706?start=228#bookmarks
#
# * Modifying an existing image 
# In case you will need to create a modified image for an existing one then you can do that by following steps 
# 
# 1. Update the Dockerfile with the new requirement which you have created already 
# 2. Then run the docker build process 
# 3. Then docker build will create the new image from the cached previous image only with the new modifications added
# 4. Once the build process completes, it will create a new image with the latest requirement. 
# 5. This does not delete or overwrite the previous image
#
# Example : Below example we are going to create the new docker image with additional 'gcc' requirements
#
# | [ec2-user@ip-172-31-21-124 apache]$ more Dockerfile
# | # BUILD - What base image we are going to use here
# | FROM centos
# |
# | # RUN - Download and install dependancy for thennnnn21 new image
# | RUN yum install httpd -y -q
# | RUN yum install gcc -y -q
# |
# | # CMD - Tell what command or action it need to perform during startup
# | CMD ["/usr/sbin/httpd", "-DNO_DETACH"]
# |
# | [ec2-user@ip-172-31-21-124 apache]$
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker build .
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | STEP 1: FROM centos
# | STEP 2: RUN yum install httpd -y -q
# | --> Using cache 750a90fe719bd9dd6162fc8fce5092c762fb729b23ee9d178d6131e2e2788f1c
# | STEP 3: FROM 750a90fe719bd9dd6162fc8fce5092c762fb729b23ee9d178d6131e2e2788f1c
# | STEP 4: RUN yum install gcc -y -q
# | Delta RPMs disabled because /usr/bin/applydeltarpm not installed.
# | --> 2c086ce33cd684c8cf66dca6bf2fb91df49016c75130fe24a028898d5de2eb92
# | STEP 5: FROM 2c086ce33cd684c8cf66dca6bf2fb91df49016c75130fe24a028898d5de2eb92
# | STEP 6: CMD ["/usr/sbin/httpd", "-DNO_DETACH"]
# | --> 9f27a8c302be8e3efc0e1cc6c6a2c0bca778cbea0652188a7044f2f582e6d0be
# | STEP 7: COMMIT
# | [ec2-user@ip-172-31-21-124 apache]$
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker image list | head -3
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | REPOSITORY                      TAG      IMAGE ID       CREATED         SIZE
# | <none>                          <none>   9f27a8c302be   3 minutes ago   526 MB
# | <none>                          <none>   2ed0af508b4f   4 days ago      347 MB
# | [ec2-user@ip-172-31-21-124 apache]$
#
#
# * Tagging or Naming an image while building 
# While creatng an image on the previous instances we must have seen that it had only an image ID and there is no name associated with it. 
# You can create the image with a name associated with it, for that only requirement is you should have a docker hud id, if its your personal image. 
# There is a naming convention it follows normally. 
#
# Syntax  : <docker_hub_id>/<image_name>:<versiob>
# Example : ajay291491/apache:latest 
#
# Note : Normally for the version number we denote that with the latest. 
#
# To build a image with the 'name' tag you will need to provide '-t' followed by image name 
#
# Syntax  : docker build -t <docker_hub_id>/<image_name>:<version> . 
# Example : docker build -t ajay291491/apache:latest . 
#
# Example : In below example we are going to build a docker image with the name 
#
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker build -t ajay291491/apache-gcc:latest . 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | STEP 1: FROM centos
# | STEP 2: RUN yum install httpd -y -q
# | --> Using cache 750a90fe719bd9dd6162fc8fce5092c762fb729b23ee9d178d6131e2e2788f1c
# | STEP 3: FROM 750a90fe719bd9dd6162fc8fce5092c762fb729b23ee9d178d6131e2e2788f1c
# | STEP 4: RUN yum install gcc -y -q
# | --> Using cache 2c086ce33cd684c8cf66dca6bf2fb91df49016c75130fe24a028898d5de2eb92
# | STEP 5: FROM 2c086ce33cd684c8cf66dca6bf2fb91df49016c75130fe24a028898d5de2eb92
# | STEP 6: RUN yum install tree -y -q
# | --> 4b9e61b6888d4427f783bf2abd0bd1bda80c601a8e5ac4e75d6d25a59abd5dbc
# | STEP 7: FROM 4b9e61b6888d4427f783bf2abd0bd1bda80c601a8e5ac4e75d6d25a59abd5dbc
# | STEP 8: CMD ["/usr/sbin/httpd", "-DNO_DETACH"]
# | --> 0c7753e7e80409e82e84e9be1fa396965d7c63200f274a7299a90010a077f6a5
# | STEP 9: COMMIT ajay291491/apache-gcc:latest
# | [ec2-user@ip-172-31-21-124 apache]$
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker image list 
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | REPOSITORY                        TAG      IMAGE ID       CREATED          SIZE
# | localhost/ajay291491/apache-gcc   latest   0c7753e7e804   12 seconds ago   631 MB
# | <none>                            <none>   9f27a8c302be   34 minutes ago   526 MB
# | <none>                            <none>   2ed0af508b4f   4 days ago       347 MB
# | docker.io/library/fedora          latest   289289d1a15b   2 weeks ago      326 MB
# | docker.io/library/busybox         latest   ef46e0caa533   2 weeks ago      1.43 MB
# | docker.io/library/mariadb         latest   56089178535f   3 weeks ago      356 MB
# | docker.io/library/mariadb         10.3     56089178535f   3 weeks ago      356 MB
# | docker.io/library/ubuntu          latest   7698f282e524   6 weeks ago      72.3 MB
# | docker.io/library/centos          latest   9f38484d220f   3 months ago     209 MB
# | docker.io/library/hello-world     latest   fce289e99eb9   5 months ago     5.62 kB
# | [ec2-user@ip-172-31-21-124 apache]$ 
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker run localhost/ajay291491/apache-gcc
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.88.0.31. Set the 'ServerName' directive globally to suppress this message
# | 
# | 
# | [ec2-user@ip-172-31-21-124 apache]$ sudo docker ps -a
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | CONTAINER ID  IMAGE                                                             COMMAND               CREATED         STATUS                     PORTS  NAMES
# | 32e425129b1e  localhost/ajay291491/apache-gcc:latest                            /usr/sbin/httpd -...  26 seconds ago  Exited (0) 12 seconds ago         gallant_wright
# | 76fc5b8c1a22  2ed0af508b4fbe38557e7c9beccae772203d2dcd4747fb3498c0118eef161301  /usr/sbin/httpd -...  4 days ago      Up 4 days ago                     wonderful_poincare
# | 6ec232aa749a  docker.io/library/busybox:latest                                  sh                    2 weeks ago     Exited (0) 12 days ago            agitated_visvesvaraya
# | a8ed65ecd30c  docker.io/library/busybox:latest                                  sh                    2 weeks ago     Exited (0) 11 days ago            agitated_rosalind
# | cc80521b1b45  docker.io/library/busybox:latest                                  sh                    2 weeks ago     Exited (1) 2 weeks ago            nervous_turing
# | 1016f0da1363  docker.io/library/mariadb:10.3                                    docker-entrypoint...  2 weeks ago     Exited (1) 2 weeks ago            xenodochial_euler
# | 0c1192c600ae  docker.io/library/busybox:latest                                  ping google.com       2 weeks ago     Up 2 weeks ago                    friendly_lamport
# | fc4f9980fe0a  docker.io/library/ubuntu:latest                                   uptime                2 weeks ago     Exited (0) 2 weeks ago            admiring_dijkstra
# | 348902c0561c  docker.io/library/ubuntu:latest                                   /bin/bash             2 weeks ago     Exited (0) 2 weeks ago            gifted_booth
# | 779800ec7966  docker.io/library/busybox:latest                                  sh                    2 weeks ago     Exited (0) 2 weeks ago            admiring_nightingale
# | 9a20dd1593c4  docker.io/library/hello-world:latest                              /hello                2 weeks ago     Exited (0) 2 weeks ago            kind_kalam
# | [ec2-user@ip-172-31-21-124 apache]$ 
# | [ec2-user@ip-172-31-21-124 apache]$ 
# | 
#
# * Saving a running conatiner to new image 
# Additional to above mentioned method, there is one more way using you can create a modified docker image. 
# To do that you will need to follow below process 
#
# 1. Run the container with 'sh' command to get the shell access
# 2. Once the shell is launched make all modification needed, such as installing a package or tuning a component etc. 
# 3. Then open another terminal and run a docker commit on the container id 
# 4. This will help you to create a new modified docker image. 
#
# Syntax : docker commit -c 'CMD["arguments"] container_ID new_image_name
#
#---------------------------------------------------------------------------------------------
#  Chapter 04 - Making real projects with Container 
#
#  UDEMY : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436978#overview
#---------------------------------------------------------------------------------------------
# 
# * Glossary 
# Below are the few configuration files parameter we are going to use as part of creating our application image via 'dockerfile' 
#
# FROM    : This will be the rgistery from we are taking the base image 
# WORKDIR : This will be the default working directory which will be used with in the container, wehn you copy files this will be the location where its getting created. 
#         : In case this directory is not available it will be created as part of the docker build process 
# COPY    : This command will help you to copy the files from the local files into the docker 
# RUN     : These are list of steps which need to run as part of building images, such as installing package, building something etc 
# CMD     : This is the startup command which you are going to use while starting the container 
#
# * Port Binding 
# When you work with a container you will need to bind the undelying host port with the container port. 
# reason for this is container will be accesses from the same host IP with a port number. 
# When traffic reaches the host port, host should know to which port it should route the traffic to. 
# 
# Note : Port binding is always mentioned in the container run time rather than while building the image. 
# Syntax : docker run -p 8080:80 <docker_image>
#
# Here -p represents the port binding 
# 8080 is the host port 
# 80 is the conatiner port to which it is getting mapped to 
#
# * Tagging an image 
# When you normally create an image it gets created with the UUID number 
# if you also want to associate name along with the image, you can tag that. 
#
# Syntax : docker build -t <docker_hub_id>/image_name:<optional_version_number>
#
# Note : if you do not supply a version number it assume it as the latest image 
#
# * Real Project : Creating a node js web server using docker 
# Task : Create a node JS web server up and running via docker and then it should accessible via the network using port 8080
#
# Resources vailable : 
# - Base Image : node:alpine    [ This means we are geting the alpine image which is fron node docker hub repo which comes with JS utilities such as npm]
# - JS files   : index.js       [ This file will hold the configuration about the web server ]
# - JS files   : package.js     [ This file contains the dependancy package list which needed for the index.js]
# - WORKDIR    : /opt/apps      [ This will be the working directory with in the container to copy the files ] 
# - RUN        : npm install    [ This will install all the required packages for the JS application which mentioned in package.js]
# - RUN        : npm start      [ This will start the JS development server ]
#
# Example : Please see below ste by step details to create this container 
#
# . STEP 1: Create the index.js and package.json file for the webserver 
#
# | [root@ip-172-31-21-124 node_js]# cd cd /apps/docker_local_hub/node_js 
# | [root@ip-172-31-21-124 node_js]# more index.js
# | const express = require('express');
# |
# | const app = express();
# |
# | app.get('/', (req, res) => {
# |     res.send('Welcome to Sathsang, how are you doing');
# | });
# |
# | app.listen(8080, () => {
# |     console.log('Listening on port 8080');
# | });
# |     
# | [root@ip-172-31-21-124 node_js]# vim package.json
# | [root@ip-172-31-21-124 node_js]# more index.js
# | const express = require('express');
# |
# | const app = express();
# |
# | app.get('/', (req, res) => {
# |     res.send('Welcome to Sathsang, how are you doing');
# | });
# |
# | app.listen(8080, () => {
# |     console.log('Listening on port 8080');
# | });
# |     
# | [root@ip-172-31-21-124 node_js]#
# 
#
# . STEP 2:  Create the Docker file for the image build 
#
# | [root@ip-172-31-21-124 node_js]# more Dockerfile
# | # Base image used from node, which contains alpine base image with npm utilities
# |
# | FROM node:alpine
# |
# | # Working directory for the container
# |
# | WORKDIR /opt/apps
# |
# | # COPY these file in pwd to  working directory of container
# |
# | COPY ./ ./
# |
# | # Steps to run as part of building the container
# |
# | RUN npm install
# |
# | # Default command for container to startup
# |
# | CMD ["npm", "start"]
# | [root@ip-172-31-21-124 node_js]#
#
# . STEP 3: Build the my_nodejs_webapp using the images given above 
#
# | [root@ip-172-31-21-124 node_js]# docker build -t ajay291491:my_nodejs_webapp .
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | STEP 1: FROM node:alpine
# | Getting image source signatures
# | Copying blob e7c96db7181b: 2.63 MiB / 2.63 MiB [============================] 1s
# | Copying blob a2b3c20ed471: 21.73 MiB / 21.73 MiB [==========================] 1s
# | Copying blob 240c8d07e1ef: 1.27 MiB / 1.27 MiB [============================] 1s
# | Copying blob 21738480b7ba: 280 B / 280 B [==================================] 1s
# | Copying config a9a8b83644f7: 5.52 KiB / 5.52 KiB [==========================] 0s
# | Writing manifest to image destination
# | Storing signatures
# | STEP 2: WORKDIR /opt/apps
# | --> ffcdce5b2301c839f403f66e6334e4b5e6b2298db133c2c6ff466faf955502f6
# | STEP 3: FROM ffcdce5b2301c839f403f66e6334e4b5e6b2298db133c2c6ff466faf955502f6
# | STEP 4: COPY ./ ./
# | --> 545e51456b63686e4d13dadadf86ff8577961441d6b97e79c728dcd896b32efd
# | STEP 5: FROM 545e51456b63686e4d13dadadf86ff8577961441d6b97e79c728dcd896b32efd
# | STEP 6: RUN npm install
# | ERRO[0008] read container terminal output: input/output error: input/output error
# | npm notice created a lockfile as package-lock.json. You should commit this file.
# | npm WARN apps No description
# | npm WARN apps No repository field.
# | npm WARN apps No license field.
# |
# | added 50 packages from 37 contributors and audited 126 packages in 4.781s
# | found 0 vulnerabilities
# |
# | --> d9939a78ba16438de25cd0562953bc76a4593992b663212e2c2c601ad9580a5f
# | STEP 7: FROM d9939a78ba16438de25cd0562953bc76a4593992b663212e2c2c601ad9580a5f
# | STEP 8: CMD ["npm", "start"]
# | --> d85d0dcc24a40f270fac81e2c4968068a76c2a5fca00e3f9ea6d72dfba43ccbb
# | STEP 9: COMMIT ajay291491:my_nodejs_webapp
# | [root@ip-172-31-21-124 node_js]#
#
# . STEP 4: List the newly created image and that is available with the tag 
#
# | [root@ip-172-31-21-124 node_js]# docker image list
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | REPOSITORY                        TAG                IMAGE ID       CREATED          SIZE
# | localhost/ajay291491              my_nodejs_webapp   d85d0dcc24a4   19 seconds ago   87 MB
# | docker.io/library/node            alpine             a9a8b83644f7   2 weeks ago      83 MB
# | localhost/ajay291491/apache-gcc   latest             0c7753e7e804   3 weeks ago      631 MB
# | <none>                            <none>             9f27a8c302be   3 weeks ago      526 MB
# | <none>                            <none>             2ed0af508b4f   3 weeks ago      347 MB
# | docker.io/library/fedora          latest             289289d1a15b   5 weeks ago      326 MB
# | docker.io/library/busybox         latest             ef46e0caa533   5 weeks ago      1.43 MB
# | docker.io/library/mariadb         10.3               56089178535f   6 weeks ago      356 MB
# | docker.io/library/mariadb         latest             56089178535f   6 weeks ago      356 MB
# | docker.io/library/ubuntu          latest             7698f282e524   2 months ago     72.3 MB
# | docker.io/library/centos          latest             9f38484d220f   4 months ago     209 MB
# | docker.io/library/hello-world     latest             fce289e99eb9   6 months ago     5.62 kB
# | [root@ip-172-31-21-124 node_js]#
#
# . STEP 5: Start the container with the port binding 
#
# | [root@ip-172-31-21-124 node_js]# docker  run -p 8080:80 -it d85d0dcc24a4
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# |
# | > @ start /opt/apps
# | > node index.js
# |
# | Listening on port 8080
# | ^C[root@ip-172-31-21-124 node_js]# docker  run -p 80:80 -it d85d0dcc24a4
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# |
# | > @ start /opt/apps
# | > node index.js
# |
# | Listening on port 8080
# |
#
# . STEP 6: Access the web service and make sure all working fine.
#
# | [root@sathsang kubernets]# curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:8080
# | Welcome to Sathsang infra, An instance of joy with AWS and Docker
# | [root@sathsang kubernets]#
#
# * Rebuilding a container 
# As an when you have changes to the application running on the container below is going to the proccedure you are going to follow. 
# Note : there is no method available to make hot changes on the existing container image as they are just a snapshot of the config when they have made.
#
# 1. Make the relevant changes to the code in the source code directory 
# 2. Once the changes to source code has been made then rebuild the same image 
# 3. When you rebuild the image, from the point where the changes are effective, docker will flush those changes and then rebuild the image. 
#
# For example, if you have multiple files on the same conatiner, when you make changes to one file then the place from files is defined will get rebuild. 
#
# NOTE : if you want to make use of the maximum existing cache and then you need to only use the minimal changes to existing cache, then you will need plan your docker file accordingly. 
#
# Example : We are going to rebuild the existing container with some changes to index.js file
#
# . STEP 1 - Make the changes to the index.js file in the source code directory
#
# | root@ip-172-31-21-124 node_js]# more index.js
# | const express = require('express');
# |
# | const app = express();
# |
# | app.get('/', (req, res) => {
# |     res.send('Welcome to Sathsang infra, instance of joy with AWS and Docker \n');    --> This is the change we have made to the conatiner 
# | });
# |
# | app.listen(8080, () => {
# |     console.log('Listening on port 8080');
# | });
# |
# | [root@ip-172-31-21-124 node_js]#
#
# . STEP 2 : Rearrange the Dockerfile to make sure only minimum cache will get bursted 
#
# | [root@ip-172-31-21-124 node_js]# more Dockerfile
# | # Base image used from node, which contains alpine base image with npm utilities
# |
# | FROM node:alpine
# |
# | # Working directory for the container
# |
# | WORKDIR /opt/apps
# |
# | # COPY the static file in pwd to  working directory of container
# |
# | COPY ./package.json ./						--> Moved package.json above since it doesn't require to be rebuilt
# |
# | # Steps to run as part of building the container
# |
# | RUN npm install
# |
# | # COPY the dynamic files
# |
# | COPY ./ ./								--> This will be the place where actual changes will occur
# |
# | # Default command for container to startup
# |
# | CMD ["npm", "start"]
# | [root@ip-172-31-21-124 node_js]#
#
# . STEP 3 : Rebuild the conatiner 
#
# | [root@ip-172-31-21-124 node_js]# docker rebuild -t ajay291491/node_web_app .
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | Command "rebuild" not found.
# | See `podman --help`.
# | [root@ip-172-31-21-124 node_js]# docker build -t ajay291491/node_web_app .
# | Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# | STEP 1: FROM node:alpine
# | STEP 2: WORKDIR /opt/apps
# | --> Using cache 87eea9b1a98c628c2fc5dec2931533e65863e52f116ac49dc281feb31cf9f704
# | STEP 3: FROM 87eea9b1a98c628c2fc5dec2931533e65863e52f116ac49dc281feb31cf9f704
# | STEP 4: COPY ./package.json ./
# | --> Using cache c8bdfc26fd92e12b1e35f4a09e97d521b8719eba4f38b8a37198d5265ba163ed
# | STEP 5: FROM c8bdfc26fd92e12b1e35f4a09e97d521b8719eba4f38b8a37198d5265ba163ed
# | STEP 6: RUN npm install
# | --> Using cache 6d2d190066366c0d7da559da0b2762ed426ee4721c0c11944f6f677adc17948d
# | STEP 7: FROM 6d2d190066366c0d7da559da0b2762ed426ee4721c0c11944f6f677adc17948d
# | STEP 8: COPY ./ ./										--> From here remaining steps will be executed again
# | --> Using cache dc3681c55fad568def23434be0ae13779742a245428099df978bd92c5d606bfc
# | STEP 9: FROM dc3681c55fad568def23434be0ae13779742a245428099df978bd92c5d606bfc
# | STEP 10: CMD ["npm", "start"]
# | --> Using cache 1ecc5a7cb29e743f02340bc14f4f02f353dfb14af72460d5e93048ed1dd9c45c
# | STEP 11: COMMIT ajay291491/node_web_app
# | [root@ip-172-31-21-124 node_js]#
#
# . STEP 4 : Now run the conatiner and test it again
#
# | [root@sathsang kubernets]# curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:8080
# | Welcome to Sathsang infra, instance of joy with AWS and Docker 
# | [root@sathsang kubernets]##
# 
# * Why we need to be careful in modifying or planning the Dockerfile 
# As the applications becomes bigger and bigger it will increase the time for building that image. 
# So it will be always good we keep minimal changes to the existing cache so that during build time only minimal changes will be done. 
# That will save of lot of time when you rebuild applications and it will have lot of importance when comes to cloud. 
#
#
#---------------------------------------------------------------------------------------------
#  Chapter 05 - Making multi container application
#
#  UDEMY : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11436978#overview
#---------------------------------------------------------------------------------------------
#
# * Docker Compose
# Docker Compose is a tool for defining a multi container appliaction. 
# One of the prime responsibility of docker compose is to create a networking between different containers in a multi container app infra. 
# This brings up an isolated environment for multiple containers to work together. 
# In most of the case there will be multiple containers working in various different ports in an isolated environment. 
# They will be exposed to the external world through a port which bind to their hypervisor. 
# You will be defining the configuration about your multi container application  and its servics in 'docker-compose.yml' file. 
# This yaml file will bring up all the services and appliaction as defined in the yaml file. 
# 
# Using docker compose mainly consist of three steps. 
#
# 1. Defining all your app environment in a Dockerfile so that it can be reproduced anywhere
# 2. Defining all your services which makes your application under the services, so that it can create multiple container under an isolated environment 
# 3. Run 'docker-compose up' to bring all your application up. 
#
# Key Commands 
#
# $ docker compose up              ==> This is a combination of 'docker run <few images>'
# $ docker compose up --build      ==> This is comination of 'docker build . ' + 'docker run <list of images>'
# $ docker compose down            ==> This will terminate the containers
#
# udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437002?start=75#bookmarks
#
# * File structure - docker-compose.yml
# When you are using docker-compose method, then you can keep below files as needed. 
#
#  1. If you are already using an image which is built then you might require only the 'docker-compose.yml' file. 
#  2. if you are not build any image so far, you arrange files like 
#     - application codes or files as needed
#     - 'Dockerfile' to build the application image using the files and config
#     - 'docker-compose.yml' to build the isolated environment for various containers. 
#
# Syntax : Below is the syntax of the docker-compose.yml
#
# version:  {{ version_number }}          => version of docker compose you are using
# services:                       => List docker containers you are going to spin up, these containers will be in a same isolated environment and can communicate using port number internally
#   {{ service_name_one }} :		 => This is name of the container and under the isolated env this container will be accessible via this name 
#     images: {{ name of the image }}    => This should be a valid image name in the repo
#     ports : 
#        - {{ port_number }}          => list of port number for the service to open
#
#   {{ service_name_two }} : 
#     image: {{ name of the image }}
#     ports: 
#       - {{ port_number_one }}
#       - {{ external_port:port_number_two }}		==> This kind of config are used to map between local and hypervisor ports 
#
#   {{ service_name_three }}:
#     build: {{ . }}					==> You can also build the image, if needed via docker compose
#     ports:
#       - {{ port_number }} 				
#
#
#  Example: Below example shows a sample docker-compose.yml file for the visitor application 
#
# | [root@ip-172-31-21-124 visitor_count]# more docker-compose.yml 
# | version: '3'
# | services: 
# |   node_app:
# |     build: .			-> This is a image getting build locally
# |     ports:
# |       - "4000:8081"			-> portmapping for external access
# |   redis-server:
# |     image: 'redis'A			-> creating a container named 'redis-server' using docker hub image 'redis'
# | [root@ip-172-31-21-124 visitor_count]# 
# | 
#
#
# * Creating Multi container apps for - "Visitor Count APP"
# In this chapter we will start working on creating application with multiple containers. 
# Here we will create an application with two containers one is node JS application and it will have a redis backend. 
# Node application will act as the front end and redis will act as the backend.
# Whom ever access the url, this app will return the value of how many times it got accessed. 
#
# . Tasks Involved for apps 
#   - Create a visitor app which consist of package.json and index.js 
#   - A backednd redis database which stores the vistor count
#   - Node web application expected to communicate with a backend redis-server 
#    
# . Tasks involved from redis
#   - Create docker image for node app which brings up a web app which stores vistor count
#   - Pull redis docker image from docker hub
#   - create a network infra for both node web app container and redis servers to communicate via ports in an isolated environment 
#
# STEP 1 : Create an application directory and application code base inside. 
# 
# | [root@ip-172-31-21-124 visitor_count]# pwd
# | /apps/docker_local_hub/visitor_count
# | [root@ip-172-31-21-124 visitor_count]# ls -l package.json index.js 
# | -rw-r--r--. 1 root root 443 Aug  5 05:33 index.js
# | -rw-r--r--. 1 root root 118 Aug  5 04:47 package.json
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# more package.json 
# | {
# |   "dependencies": {
# |     "express": "*",
# |     "redis": "2.8.0"		--> This is just the node module name for redis, just like python module
# |   },
# |   "scripts": {
# |     "start": "node index.js"
# |   }
# | }
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# more index.js 
# | const express = require('express');
# | const redis = require('redis');		--> Importing redis module to communicate with redis DB
# | const app = express();
# | 
# | const client = redis.createClient({		--> Defining where to look for redis database
# |   host: 'redis-server',			--> redis container name will be 'redis-server', this will be defined in 'docker-compose.yml' 
# |   port: 6379				--> port in which node container @ app level will communicate with redis container
# | });
# | 
# | client.set('visits', 0);
# | 
# | app.get('/', (req, res) => {			--> Logic access the vistor counter and then increase the counter
# |   client.get('visits', (err, visits) => {
# |     res.send('Number of visits is ' + visits);
# |     client.set('visits', parseInt(visits) + 1);
# |   });
# | });
# | 
# | app.listen(8081, () => {
# |   console.log('Listening on port 8081');
# | });
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# 
# | 
#
# STEP 2 : Create Dockerfile to create the application
#
# | [root@ip-172-31-21-124 visitor_count]# more Dockerfile 
# | # Getting the base Image from alpine:linux
# | FROM node:alpine
# | 
# | # Setting up the working directory in container
# | WORKDIR /app
# | 
# | # Copying the package.json file to /app directory
# | COPY package.json .
# | 
# | # Running npm install command to install dependancies 
# | RUN npm install
# | 
# | # Copy rest of the files in code base(as of now index.js) to /app to redice cache burst
# | COPY . .
# | 
# | # Command to start the service in container
# | CMD ["npm", "start"]
# | [root@ip-172-31-21-124 visitor_count]# 
# | 
#
# STEP 3 : Create 'docker-compose.yml' to bring both node app and redis container on same isolated envioenment 
#
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# more docker-compose.yml 
# | version: '3'
# | services: 
# |   node_app:					--> This service will get built locally, docker compose has to run from '/apps/docker_local_hub/visitor_count'
# |     build: .
# |     ports:
# |       - "4000:8081"				--> Port binding with hypervisor and local container 
# |   redis-server:
# |     image: 'redis'
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# 
# | 
#
#
# STEP 4 : Start the container environment using the 'docker compose up' command
#
# | [root@ip-172-31-21-124 visitor_count]# docker-compose up
# | Building node_app
# | Step 1/6 : FROM node:alpine
# | alpine: Pulling from library/node
# | e7c96db7181b: Pull complete
# | 72484f09da35: Pull complete
# | 86bee4bed5f2: Pull complete
# | f9e983f0fe2c: Pull complete
# | Digest: sha256:300e3d2c19067c1aec9d9b2bd3acbd43d53797a5836d70a23e437a5634bcd33a
# | Status: Downloaded newer image for node:alpine
# |  ---> d97a436daee9
# | Step 2/6 : WORKDIR /app
# |  ---> Running in 2e3d8ecdd192
# | Removing intermediate container 2e3d8ecdd192
# |  ---> efb442f50567
# | Step 3/6 : COPY package.json .
# |  ---> 97b2310ddc9f
# | Step 4/6 : RUN npm install
# |  ---> Running in d0a5e431a2c8
# | npm notice created a lockfile as package-lock.json. You should commit this file.
# | npm WARN app No description
# | npm WARN app No repository field.
# | npm WARN app No license field.
# | 
# | added 54 packages from 41 contributors and audited 130 packages in 2.633s
# | found 0 vulnerabilities
# | 
# | Removing intermediate container d0a5e431a2c8
# |  ---> b9b8c8581e14
# | Step 5/6 : COPY . .
# |  ---> 4b1f9010e35e
# | Step 6/6 : CMD ["npm", "start"]
# |  ---> Running in 002916d81331
# | Removing intermediate container 002916d81331
# |  ---> 664d764d9678
# | Successfully built 664d764d9678
# | Successfully tagged visitor_count_node_app:latest
# | WARNING: Image for service node_app was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.
# | Pulling redis-server (redis:)...
# | latest: Pulling from library/redis
# | f5d23c7fed46: Pull complete
# | 831c20fd50cb: Pull complete
# | bc2a0f25caa5: Pull complete
# | 745ac314a007: Pull complete
# | 6deeca231441: Pull complete
# | 6291e84f5373: Pull complete
# | Digest: sha256:854715f5cd1b64d2f62ec219a7b7baceae149453e4d29a8f72cecbb5ac51c4ad
# | Status: Downloaded newer image for redis:latest
# | Creating visitor_count_redis-server_1 ... done
# | Creating visitor_count_node_app_1     ... done
# | Attaching to visitor_count_node_app_1, visitor_count_redis-server_1
# | redis-server_1  | 1:C 05 Aug 2019 07:28:31.155 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
# | redis-server_1  | 1:C 05 Aug 2019 07:28:31.155 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started
# | redis-server_1  | 1:C 05 Aug 2019 07:28:31.155 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
# | redis-server_1  | 1:M 05 Aug 2019 07:28:31.157 * Running mode=standalone, port=6379.
# | redis-server_1  | 1:M 05 Aug 2019 07:28:31.157 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
# | redis-server_1  | 1:M 05 Aug 2019 07:28:31.157 # Server initialized
# | redis-server_1  | 1:M 05 Aug 2019 07:28:31.157 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
# | redis-server_1  | 1:M 05 Aug 2019 07:28:31.157 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
# | redis-server_1  | 1:M 05 Aug 2019 07:28:31.157 * Ready to accept connections
# | node_app_1      | 
# | node_app_1      | > @ start /app
# | node_app_1      | > node index.js
# | node_app_1      | 
# | node_app_1      | Listening on port 8081
# | 
# | 
#
# STEP 5 : Access the url and make sure it is working as expected 
#
# | [sathsang@sathsang ~]$ curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:4000/;echo
# | Number of visits is 38
# | [sathsang@sathsang ~]$ 
# | [sathsang@sathsang ~]$ curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:4000/;echo
# | Number of visits is 39
# | [sathsang@sathsang ~]$ curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:4000/;echo
# | Number of visits is 40
# | [sathsang@sathsang ~]$ 
#
#
# NOTE : The way this application works is like below 
#
# Client --> GET {{ https://hypervisor_url:4000i }} --> Hypervisor redirects -->  {{ via port 8081 }} --> node_app container --> {{ via port 6379 }} --> redis-server container
#
# * How to start the docker-compose in backgroup 
# You can start the docker compose in background by using a "-d" string similar to what you have used for docker run . 
#
# Syntax : docker-compose up -d 
#
# Example : Below is the docker compose started in background 
#
# | [root@ip-172-31-21-124 visitor_count]# docker-compose up -d
# | Starting visitor_count_redis-server_1 ... done
# | Starting visitor_count_node_app_1     ... done
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# 
# | [root@ip-172-31-21-124 visitor_count]# docker container ls
# | CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                    NAMES
# | 813a0df19f4b        visitor_count_node_app   "docker-entrypoint.s…"   3 hours ago         Up 16 seconds       0.0.0.0:4000->8081/tcp   visitor_count_node_app_1
# | e01afa785820        redis                    "docker-entrypoint.s…"   3 hours ago         Up 16 seconds       6379/tcp                 visitor_count_redis-server_1
# | [root@ip-172-31-21-124 visitor_count]# 
# | [sathsang@sathsang ~]$
# | [sathsang@sathsang ~]$ curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:4000/;echo
# | Number of visits is 0
# | [sathsang@sathsang ~]$ curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:4000/;echo
# | Number of visits is 1
# | [sathsang@sathsang ~]$ curl http://ec2-13-58-45-122.us-east-2.compute.amazonaws.com:4000/;echo
# | Number of visits is 2
# | [sathsang@sathsang ~]$ 
#
#
# * How to stop a running containers started via docker compose
# You can stop the running containers using the command 'docker-compose down'
#
# syntax : docker compose down
#
# Example : Below is an example for bringing down the application 
# 
# | [root@ip-172-31-21-124 visitor_count]# docker-compose down
# | Stopping visitor_count_node_app_1     ... done
# | Stopping visitor_count_redis-server_1 ... done
# | Removing visitor_count_node_app_1     ... done
# | Removing visitor_count_redis-server_1 ... done
# | Removing network visitor_count_default
# | [root@ip-172-31-21-124 visitor_count]# docker container ls
# | CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
# | [root@ip-172-31-21-124 visitor_count]# 
# | 
#
# * Restart Policies : How to handle crash in a container 
# You can always handle the crash in a container based on the its gracefully or unexpected. 
# You can get this done via 'restart policies. 
# You can update the restart policies in the docker-compose.yml under the services to make sure crashes are handled properly. 
#
# Note : You will need to define the restart policies for every containers under the service definition 
#
# Below are the restart policies available. 
# 
# no             : never attempt fo restart the container even if it is crashes or restarts
# always         : if this container stops for any reason then restart always 
# on-failure     : Only restart if the container stop with a non-zero error code 
# unless-stopeed : Always restart the container unless it is forcefully stopped
#
# Example : below is an example of the visitor count application updated with restart policy on failure 
#
# | [root@ip-172-31-21-124 visitor_count]# more docker-compose.yml 
# | version: '3'
# | services: 
# |   node_app:
# |     build: .
# |     ports:
# |       - "4000:8081"
# |     restart: 'on-failure'			--> restart policy to restart if there a failure 
# |   redis-server:
# |     image: 'redis'
# |     restart: 'on-failure'			--> restart policy to restart if there a failure
# | [root@ip-172-31-21-124 visitor_count]# 
#
#
# Udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437008?start=413#bookmarks
#
#
#------------------------------------------------------------------------------------------------------------------
#  Chapter 06 - Creating Production Grade workflow 
#
#  UDEMY : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437040#overview
#------------------------------------------------------------------------------------------------------------------
#
# * Data Management in Docker 
# When you are using a conatiner managing data is a diffcult task as it brings up few challenges. 
#
# . Data which reside within the container doesn't persist as soon as container is brought down. 
# . In case another process needs the same data for reference its is not easy to get that if that is within the container. 
# . Writing into a container’s writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. 
#
# To deal with above challenges and make the data persistent even after the container is shutdown docker brings up two options 
# Regardless which option you choose docker access the data as plain files and directories within the container regardless of the method you have used.
#
# 1. Volume 
# 2. Bind Mounts
#
# Note : Go to below link and look for the image which represents the data management in docker
#
# Reference : https://docs.docker.com/storage/ 
#
# * Docker Volume
# Volumes are stored in a part of the host filesystem which is managed by Docker, for example, (/var/lib/docker/volumes/).
# Non-Docker process should not modify the data on these volumes, volumes are the best way to persist data in the docker. 
# Volumes are created and managed by Docker during container or service creation. 
#
# If you want, you can manaually create and destroy the volume using below command also 
#
# Syntax : docker volume create
# Syntax : docker volume prune 
#
# Example : Below is a manual creation of volume 
#
# | [root@ip-172-31-21-124 docker_local_hub]# docker volume create 
# | 27cd6286781c4d6f9241af8d8c29917b0270bbd9ecd1b25d5a6693510bdfed0e
# | [root@ip-172-31-21-124 docker_local_hub]# docker volume ls 
# | DRIVER              VOLUME NAME
# | local               27cd6286781c4d6f9241af8d8c29917b0270bbd9ecd1b25d5a6693510bdfed0e
# | local               81b0f6c31a3b63dab9c97c2cc7ee5eefcc4cf64dc8a9715a2c784703434d8444
# | local               62057d87f7c23e26a9d3c1c3c88fc823f2c2b9c9e0015d33526d68a89e57c226
# | local               a9e8d56ea83bd014f62a0de90312de32758b5b6da3750460dac71a7b1de28b77
# | [root@ip-172-31-21-124 docker_local_hub]#
#
# Note : Normally the manual creation methods are not used, they are used via container run time using '-v' flag. 
#
# * Docker Bind Mounts
# Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container.
# The file or directory is referenced by its full path on the host machine. The file or directory does not need to exist on the Docker host already.
# It is created on demand if it does not yet exist. Bind mounts are very performant, but they rely on the host machine’s filesystem having a specific directory structure available.
#
# * How to access volume during runtime 
# You will be using '-v' flag to map or bind a volume to a container, When you are mapping a volume or binding a mount it should follow 
#
# syntax : docker run -v  {{ host_local_dir:conatiner_dir }}  {{ docker_image }}
#
# If you want any of the directory to exclude as part of the mount volumes then you can still use '-v' flag this time without a mapping with ':'
#
# Syntax : docker run -v {{ exclude_dir }} {{ docker_image }}
#
# * Creating Production Grade workflow - Using docker Build 
#
# Level Set : In this chapter we will be going through a step by step approach to understand various different concept around 
#
# Note : We are using a React as the example for an application which can get deployed through the docker container. 
#
# . Creating a sample react application (This is just to showcase an example application)
# . Running test cases on that application
# . Building that application 
# . Create an image with docker build 
# . Creating volume bookmarks in docker 
# . Understading what to refer in volume bookmark and what not to
# . Building the same application using docker-compose 
# . use Travis CICD pipeline to deploy the application 
# . Deploy your application to AWS instance 
#
# * STEP 1 : Install node package on your localhost and then create a project and then create a production build
#
# $ npm install -g create-react-app
# $ cd /app/
# $ create-react-app frontend
# $ cd frontend
# $ npm start   --> This will bring up the development server instancce 
#
# * STEP 2 : If all above steps are completed, then we can create a production build 
#
# | [root@sathsang frontend]# npm build
# | npm WARN build `npm build` called with no arguments. Did you mean to `npm run-script build`?
# | [root@sathsang frontend]# npm run build
# | 
# | > frontend@0.1.0 build /app/frontend
# | > react-scripts build
# | 
# | Creating an optimized production build...
# | Compiled successfully.
# | 
# | File sizes after gzip:
# | 
# |   36.44 KB  build/static/js/2.b41502e9.chunk.js
# |   762 B     build/static/js/runtime~main.a8a9905a.js
# |   602 B     build/static/js/main.28647029.chunk.js
# |   517 B     build/static/css/main.2cce8147.chunk.css
# | 
# | The project was built assuming it is hosted at the server root.
# | You can control this with the homepage field in your package.json.
# | For example, add this to build it for GitHub Pages:
# | 
# |   "homepage" : "http://myname.github.io/myapp",
# | 
# | The build folder is ready to be deployed.
# | You may serve it with a static server:
# | 
# |   npm install -g serve
# |   serve -s build
# | 
# | Find out more about deployment here:
# | 
# |   https://bit.ly/CRA-deploy
# | 
# | [root@sathsang frontend]# ls
# | build  node_modules  package.json  package-lock.json  public  README.md  src
# | [root@sathsang frontend]#
#
#
# STEP 3 : Once you have the build directory then you can create a docker container 
#
# This time we are using 'Dockerfile.dev' file intead just 'Dockerfile'
#
# | [root@sathsang frontend]# more   Dockerfile.dev 
# | # Base image from docker hub
# | FROM node:alpine
# | 
# | # Working directory within the container
# | WORKDIR /app
# | 
# | # Copying package.json to avoid cache bursting
# | COPY package.json . 
# | 
# | # Install the npm packages needed
# | RUN npm install
# | 
# | # Copy the directories inside container
# | COPY . .
# | 
# | # Start command for the container
# | CMD ["npm", "run", "start"]
# | [root@sathsang frontend]# 
# | 
#
# STEP 4 : Build the docker image 
#
# | [root@sathsang frontend]# docker build -t ajay291491/react_app:latest -f Dockerfile.dev . 
# | Sending build context to Docker daemon  148.3MB
# | Step 1/6 : FROM node:alpine
# | alpine: Pulling from library/node
# | e7c96db7181b: Pull complete 
# | 72484f09da35: Pull complete 
# | 86bee4bed5f2: Pull complete 
# | f9e983f0fe2c: Pull complete 
# | Digest: sha256:300e3d2c19067c1aec9d9b2bd3acbd43d53797a5836d70a23e437a5634bcd33a
# | Status: Downloaded newer image for node:alpine
# |  ---> d97a436daee9
# | Step 2/6 : WORKDIR /app
# |  ---> Running in e49fe433696e
# | Removing intermediate container e49fe433696e
# |  ---> 3b9d654ff534
# | Step 3/6 : COPY package.json .
# |  ---> bc771db1e810
# | Step 4/6 : RUN npm install
# |  ---> Running in 68856e944f70
# | npm WARN deprecated fsevents@2.0.6: Please update: there are crash fixes
# | npm WARN deprecated flatten@1.0.2: I wrote this module a very long time ago; you should use something else.
# | npm WARN deprecated left-pad@1.3.0: use String.prototype.padStart()
# | 
# | > core-js@2.6.9 postinstall /app/node_modules/babel-runtime/node_modules/core-js
# | > node scripts/postinstall || echo "ignore"
# | 
# | Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!
# | 
# | The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
# | > https://opencollective.com/core-js 
# | > https://www.patreon.com/zloirock 
# | 
# | Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)
# | 
# | 
# | > core-js-pure@3.1.4 postinstall /app/node_modules/core-js-pure
# | > node scripts/postinstall || echo "ignore"
# | 
# | Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!
# | 
# | The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
# | > https://opencollective.com/core-js 
# | > https://www.patreon.com/zloirock 
# | 
# | Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)
# | 
# | npm notice created a lockfile as package-lock.json. You should commit this file.
# | npm WARN @typescript-eslint/eslint-plugin@1.6.0 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself.
# | npm WARN @typescript-eslint/parser@1.6.0 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself.
# | npm WARN ts-pnp@1.1.2 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself.
# | npm WARN @typescript-eslint/typescript-estree@1.6.0 requires a peer of typescript@* but none is installed. You must install peer dependencies yourself.
# | npm WARN tsutils@3.17.0 requires a peer of typescript@>=2.8.0 || >= 3.2.0-dev || >= 3.3.0-dev || >= 3.4.0-dev || >= 3.5.0-dev || >= 3.6.0-dev || >= 3.6.0-beta || >= 3.7.0-dev || >= 3.7.0-beta but none is installed. You must install peer dependencies yourself.
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/jest-haste-map/node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/chokidar/node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.0.6 (node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.0.6: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | 
# | added 1388 packages from 675 contributors and audited 902283 packages in 27.438s
# | found 0 vulnerabilities
# | 
# | Removing intermediate container 68856e944f70
# |  ---> f75c776463cc
# | Step 5/6 : COPY . .
# |  ---> 5b5656274331
# | Step 6/6 : CMD ['npm', 'run', 'start']
# |  ---> Running in dd78e20b8cf2
# | Removing intermediate container dd78e20b8cf2
# |  ---> 0c87b60e5d17
# | Successfully built 0c87b60e5d17
# | Successfully tagged ajay291491/react_app:latest
# | [root@sathsang frontend]# docker image ls 
# | REPOSITORY             TAG                 IMAGE ID            CREATED             SIZE
# | ajay291491/react_app   latest              0c87b60e5d17        18 seconds ago      380MB
# | node                   alpine              d97a436daee9        11 days ago         79.3MB
# | hello-world            latest              fce289e99eb9        7 months ago        1.84kB
# | [root@sathsang frontend]#
#
#
# STEP 5 : Start the conatiner
#
# | [root@sathsang frontend]# docker run -p 3000:3000 ajay291491/react_app
# | 
# | > frontend@0.1.0 start /app
# | > react-scripts start
# | 
# | Starting the development server...
# | 
# | Compiled successfully!
# | 
# | You can now view frontend in the browser.
# | 
# |   Local:            http://localhost:3000/
# |   On Your Network:  http://172.17.0.2:3000/
# | 
# | Note that the development build is not optimized.
# | To create a production build, use npm run build.
# | 
#
# STEP 6 : Access the url
#
# | [root@sathsang kubernets]# curl http://172.17.0.2:3000/ > /dev/null
# |   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
# |                                  Dload  Upload   Total   Spent    Left  Speed
# | 100  1653  100  1653    0     0  1148k      0 --:--:-- --:--:-- --:--:-- 1614k
# | [root@sathsang kubernets]# 
#
# STEP 7 : Use Docker Volume to map local /app/frontend/src/App.js file can be edited by 
#
# | [root@dockerstation01 frontend]# docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app ajay291491/react_app
# |
# | > frontend@0.1.0 start /app
# | > react-scripts start
# |
# | Starting the development server...
# |
# | Compiled successfully!
# |
# | You can now view frontend in the browser.
# |
# |   Local:            http://localhost:3000/
# |   On Your Network:  http://172.17.0.2:3000/
# |
# | Note that the development build is not optimized.
# | To create a production build, use npm run build.
# |
# | Compiling...
# | Compiled successfully!
#
# NOTE : On step 7 we have used couple of volume option, below is the details for the same
# 
# -v $(pwd):/app          => This means we are mapping local hosts PWD to /app of the container 
# -v /app/node_modules    => This means the /app/node_modules of container run time will still exist regardless of above definition
#
#
# * Docker-Compose - Create the Same react app which created above 
# Now we will create the same app with the 'docker-compose' utility, you can see below steps to see the procedure 
#
# STEP 1 : Create the docker-compose.yml file 
#
# | [root@dockerstation01 frontend]# more docker-compose.yml
# | version: '3'
# | services:
# |     react_app:						=> only one app we are creating and defined under services
# |         build:						=> As we using a different name for Dockerfile as Dockerfile.dev, we need to define the build context
# |             context: .					=> This means PWD
# |             dockerfile: Dockerfile.dev			=> Name of the docker file located in the PWD
# |         ports:
# |             - '3000:3000'					=> Binding local and external port as 3000
# |         volumes:
# |             - /app/node_modules				=> This directory should be excluded from hosts volume mapping 
# |             - .:/app					=> This directory should be includes to map from local hosts PWD to container's /app
# | [root@dockerstation01 frontend]#
#
# 
# STEP 2 : Run compose and bring up the appliaction as defined 
#
# | [root@dockerstation01 frontend]# docker-compose up 
# | Creating network "frontend_default" with the default driver
# | Building react_app
# | Step 1/7 : FROM node:alpine
# |  ---> d97a436daee9
# | Step 2/7 : WORKDIR /app
# |  ---> Using cache
# |  ---> 1098e641e2d1
# | Step 3/7 : COPY package.json .
# |  ---> 3dd0ff5017e3
# | Step 4/7 : COPY . .
# |  ---> 1066c6fb8cd2
# | Step 5/7 : RUN npm install
# |  ---> Running in 6169e8d6bbb5
# | 
# | > core-js@2.6.9 postinstall /app/node_modules/babel-runtime/node_modules/core-js
# | > node scripts/postinstall || echo "ignore"
# | 
# | Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!
# | 
# | The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
# | > https://opencollective.com/core-js 
# | > https://www.patreon.com/zloirock 
# | 
# | Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)
# | 
# | 
# | > core-js-pure@3.1.4 postinstall /app/node_modules/core-js-pure
# | > node scripts/postinstall || echo "ignore"
# | 
# | Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!
# | 
# | The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
# | > https://opencollective.com/core-js 
# | > https://www.patreon.com/zloirock 
# | 
# | Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)
# | 
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/chokidar/node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/jest-haste-map/node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.0.6 (node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.0.6: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | 
# | added 1388 packages from 675 contributors and audited 902283 packages in 72.076s
# | found 0 vulnerabilities
# | 
# | Removing intermediate container 6169e8d6bbb5
# |  ---> 0e1adcddcc80
# | Step 6/7 : COPY . .
# |  ---> 0bcfc54c4924
# | Step 7/7 : CMD ["npm", "run", "start"]
# |  ---> Running in f77fb39e9e2f
# | Removing intermediate container f77fb39e9e2f
# |  ---> 35d41cc0e672
# | Successfully built 35d41cc0e672
# | Successfully tagged frontend_react_app:latest
# | WARNING: Image for service react_app was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.
# | Creating frontend_react_app_1 ... done
# | Attaching to frontend_react_app_1
# | react_app_1  | 
# | react_app_1  | > frontend@0.1.0 start /app
# | react_app_1  | > react-scripts start
# | react_app_1  | 
# | react_app_1  | Starting the development server...
# | react_app_1  | 
# | react_app_1  | Compiled successfully!
# | react_app_1  | 
# | react_app_1  | You can now view frontend in the browser.
# | react_app_1  | 
# | react_app_1  |   Local:            http://localhost:3000/
# | react_app_1  |   On Your Network:  http://172.18.0.2:3000/
# | react_app_1  | 
# | react_app_1  | Note that the development build is not optimized.
# | react_app_1  | To create a production build, use npm run build.
# | react_app_1  | 
# | 
# |
#
# STEP 3 : Access the URL and confirm its working
#
# | [sathsang@sathsang ~]$ curl http://dockerstation01.svr.apac.sathsang.net:3000/ |echo
# | 
# |   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
# |                                  Dload  Upload   Total   Spent    Left  Speed
# | 100  1653  100  1653    0     0   240k      0 --:--:-- --:--:-- --:--:--  269k
# | (23) Failed writing body
# | [sathsang@sathsang ~]$
#
# 
# * Running a test case inside a container 
# You can run the test case insider the conatiner using couple of approaches
# 
# 1. You can pass the test command along with the conatiner while you are running it, if its supported 
# 2. You can also define 'tests' under 'docker-compose.yml' file to run tests while during the startup of a container 
#
# Example 1 : Running test case while starting a container 
#
# | [root@dockerstation01 frontend]# docker run frontend_react_app npm run test
# |
# | > frontend@0.1.0 test /app
# | > react-scripts test
# |
# |
# | PASS src/App.test.js
# |   ✓ renders without crashing (21ms)
# |
# | Test Suites: 1 passed, 1 total
# | Tests:       1 passed, 1 total
# | Snapshots:   0 total
# | Time:        2.448s
# | Ran all test suites.
# |
# | q
# | [root@dockerstation01 frontend]#
#
#
# Example 2 : Running test case using a 'docker-compose.yml' tests 
#
# | [root@dockerstation01 frontend]# more docker-compose.yml 
# | version: '3'
# | services:
# |     react_app:
# |         build:
# |             context: . 
# |             dockerfile: Dockerfile.dev
# |         ports: 
# |             - '3000:3000'
# |         volumes: 
# |             - /app/node_modules
# |             - .:/app
# |     tests:
# |         build:
# |             context: .
# |             dockerfile: Dockerfile.dev
# |         volumes:
# |             - /app/node_modules
# |             - .:/app
# |         command: ["npm", "run", "test"]
# | [root@dockerstation01 frontend]# 
# | [root@dockerstation01 frontend]# 
# | [root@dockerstation01 frontend]# docker-compose up --build
# | Building react_app
# | Step 1/7 : FROM node:alpine
# |  ---> d97a436daee9
# | Step 2/7 : WORKDIR /app
# |  ---> Using cache
# |  ---> 1098e641e2d1
# | Step 3/7 : COPY package.json .
# |  ---> Using cache
# |  ---> 3dd0ff5017e3
# | Step 4/7 : COPY . .
# |  ---> b61bf31d47df
# | Step 5/7 : RUN npm install
# |  ---> Running in 5da9e915c7ce
# | 
# | > core-js@2.6.9 postinstall /app/node_modules/babel-runtime/node_modules/core-js
# | > node scripts/postinstall || echo "ignore"
# | 
# | Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!
# | 
# | The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
# | > https://opencollective.com/core-js 
# | > https://www.patreon.com/zloirock 
# | 
# | Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)
# | 
# | 
# | > core-js-pure@3.1.4 postinstall /app/node_modules/core-js-pure
# | > node scripts/postinstall || echo "ignore"
# | 
# | Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!
# | 
# | The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: 
# | > https://opencollective.com/core-js 
# | > https://www.patreon.com/zloirock 
# | 
# | Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)
# | 
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/chokidar/node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.9 (node_modules/jest-haste-map/node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.9: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@2.0.6 (node_modules/fsevents):
# | npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@2.0.6: wanted {"os":"darwin","arch":"any"} (current: {"os":"linux","arch":"x64"})
# | 
# | added 1388 packages from 675 contributors and audited 902283 packages in 77.955s
# | found 0 vulnerabilities
# | 
# | Removing intermediate container 5da9e915c7ce
# |  ---> 18a692c2d75e
# | Step 6/7 : COPY . .
# |  ---> 0271d5e55aed
# | Step 7/7 : CMD ["npm", "run", "start"]
# |  ---> Running in f3d7d524ff64
# | Removing intermediate container f3d7d524ff64
# |  ---> 911985e97266
# | Successfully built 911985e97266
# | Successfully tagged frontend_react_app:latest
# | Building tests
# | Step 1/7 : FROM node:alpine
# |  ---> d97a436daee9
# | Step 2/7 : WORKDIR /app
# |  ---> Using cache
# |  ---> 1098e641e2d1
# | Step 3/7 : COPY package.json .
# |  ---> Using cache
# |  ---> 3dd0ff5017e3
# | Step 4/7 : COPY . .
# |  ---> Using cache
# |  ---> b61bf31d47df
# | Step 5/7 : RUN npm install
# |  ---> Using cache
# |  ---> 18a692c2d75e
# | Step 6/7 : COPY . .
# |  ---> Using cache
# |  ---> 0271d5e55aed
# | Step 7/7 : CMD ["npm", "run", "start"]
# |  ---> Using cache
# |  ---> 911985e97266
# | Successfully built 911985e97266
# | Successfully tagged frontend_tests:latest
# | Recreating frontend_react_app_1 ... done
# | Creating frontend_tests_1       ... done
# | Attaching to frontend_react_app_1, frontend_tests_1
# | react_app_1  | 
# | react_app_1  | > frontend@0.1.0 start /app
# | react_app_1  | > react-scripts start
# | react_app_1  | 
# | react_app_1  | Starting the development server...
# | react_app_1  | 
# | react_app_1  | Compiled successfully!
# | react_app_1  | 
# | react_app_1  | You can now view frontend in the browser.
# | react_app_1  | 
# | react_app_1  |   Local:            http://localhost:3000/
# | react_app_1  |   On Your Network:  http://172.18.0.2:3000/
# | react_app_1  | 
# | react_app_1  | Note that the development build is not optimized.
# | react_app_1  | To create a production build, use npm run build.
# | react_app_1  | 
# | tests_1      | 
# | tests_1      | > frontend@0.1.0 test /app
# | tests_1      | > react-scripts test
# | tests_1      | 
# | tests_1      | PASS src/App.test.js
# | tests_1      |   ✓ renders without crashing (27ms)
# | tests_1      | 
# | tests_1      | Test Suites: 1 passed, 1 total
# | tests_1      | Tests:       1 passed, 1 total
# | tests_1      | Snapshots:   0 total
# | tests_1      | Time:        3.592s
# | tests_1      | Ran all test suites.
# | tests_1      | 
# | 
# |
# | [root@dockerstation01 frontend]# docker ps 
# | CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS              PORTS                    NAMES
# | bb0c177c861d        frontend_react_app   "docker-entrypoint.s…"   2 minutes ago       Up 2 minutes        0.0.0.0:3000->3000/tcp   frontend_react_app_1
# | edcc5f0c765c        frontend_tests       "docker-entrypoint.s…"   2 minutes ago       Up About a minute                            frontend_tests_1
# | [root@dockerstation01 frontend]#
#
#
# * Block in Dockerfile 
# In Dockerfile every declaration you make with 'FROM' statement is recognised as a block 
# You can have multiple blocks in the Same Dockerfile and these blocks can refer each other by tagging them. 
# When you declare a Block you can create a alias for that block and then if you want to access something from the container in that block you can access. 
#
# Example : Below Dockerfile with multiple blocks 
#
# | [root@dockerstation01 frontend]# more Dockerfile
# | #
# | # {{  Block for bulding the apps }}
# | #
# | # Pulling node:alpine image with a block alias 'build_instance'
# | FROM node:alpine as build_instance						=> Block which is created with alias 'build_instance'
# |
# | # Setting up working directory
# | WORKDIR /app
# |
# | # Copy pakcage.json for npm install
# | COPY package.json .
# |
# | # Perform NPM install
# | RUN npm install
# |
# | # Performing React build
# | CMD ["npm", "run", "build"]
# |
# |
# | #
# | # {{ Block for Starting the Production version of application via nginx }}
# | #
# | # Pulling the nginx image							=> Instance which launches 'nginx' web server 
# | FROM nginx
# |
# | # Copy the build image '/app/build' from Conatiner with block alias 'build_instance' to nginix  document root
# | COPY --from=build_instance /app/build /usr/share/nginx/html			=> Copying from container / block with alias 'uild_instance' to nginx container's document root
# | [root@dockerstation01 frontend]#
#
# * Performing Multi stage Docker builds (Current React app deployed to prod react)
# At times you might need to perform multi stage build using the same Dockerfile. 
# For example we have created a react app and setup that in the development server. 
# Now lets take that into the next level by deploying that to the production, to do that you will require couple of things 
#
# . Build the existing development app 
# . Create a nginix container and copy only the build directories to that  
#
# NOTE : Once the build process is over you do not require other directories and files apart from the build directory 
#
# In this excersize we will be using couple of Docker 
#
# Udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437096#overview
#
# STEP 1 : Create Dockerfile to create a interim container for build and then copy over the build image to nginx container 
#
# | [root@dockerstation01 frontend]# more Dockerfile
# | #
# | # {{  Block for bulding the apps }}
# | #
# | # Pulling node:alpine image with a block alias 'build_instance'
# | FROM node:alpine as build
# | # |
# | # Setting up working directory
# | WORKDIR /app
# | # |
# | # Copy pakcage.json for npm install
# | COPY package.json .
# | # |
# | # Perform NPM install
# | RUN npm install
# | COPY . .
# | # |
# | # Performing React build
# | RUN npm run build
# | # |
# | # |
# | #
# | # {{ Block for Starting the Production version of application via nginx }}
# | #
# | # Pulling the nginx image
# | FROM nginx
# | # |
# | # Copy the build image '/app/build' from Conatiner with block alias 'build_instance' to nginix  document root
# | RUN mkdir -p /usr/share/nginx/html
# | COPY --from=build /app/build /usr/share/nginx/html/
# | [root@dockerstation01 frontend]
#
# STEP 2 : Run Docker build 
#
# | [root@dockerstation01 frontend]# docker build -t ajay291491/react_prod . 
# | Sending build context to Docker daemon    535kB
# | Step 1/9 : FROM node:alpine as build
# |  ---> d97a436daee9
# | Step 2/9 : WORKDIR /app
# |  ---> Using cache
# |  ---> 12fc9e2c1e0e
# | Step 3/9 : COPY package.json .
# |  ---> Using cache
# |  ---> f48d4e4b2c96
# | Step 4/9 : RUN npm install
# |  ---> Using cache
# |  ---> b0ca70d777f5
# | Step 5/9 : COPY . .
# |  ---> 40820a973a3a
# | Step 6/9 : RUN npm run build
# |  ---> Running in 16d4e2764e4f
# | 
# | > frontend@0.1.0 build /app
# | > react-scripts build
# | 
# | Creating an optimized production build...
# | Compiled successfully.
# | 
# | File sizes after gzip:
# | 
# |   36.44 KB  build/static/js/2.b41502e9.chunk.js
# |   762 B     build/static/js/runtime~main.a8a9905a.js
# |   592 B     build/static/js/main.70c965f8.chunk.js
# |   517 B     build/static/css/main.2cce8147.chunk.css
# | 
# | The project was built assuming it is hosted at the server root.
# | You can control this with the homepage field in your package.json.
# | For example, add this to build it for GitHub Pages:
# | 
# |   "homepage" : "http://myname.github.io/myapp",
# | 
# | The build folder is ready to be deployed.
# | You may serve it with a static server:
# | 
# |   npm install -g serve
# |   serve -s build
# | 
# | Find out more about deployment here:
# | 
# |   https://bit.ly/CRA-deploy
# | 
# | Removing intermediate container 16d4e2764e4f
# |  ---> 9275ce80b041
# | Step 7/9 : FROM nginx
# |  ---> e445ab08b2be
# | Step 8/9 : RUN mkdir -p /usr/share/nginx/html
# |  ---> Using cache
# |  ---> d93bd2bfcc43
# | Step 9/9 : COPY --from=build /app/build /usr/share/nginx/html/
# |  ---> 11caf318459e
# | Successfully built 11caf318459e
# | Successfully tagged ajay291491/react_prod:latest
#
# STEP 3 : Validate the Docker image and start the conatiner 
#
# | [root@dockerstation01 frontend]#
# | [root@dockerstation01 frontend]# docker images 
# | REPOSITORY              TAG                 IMAGE ID            CREATED              SIZE
# | <none>                  <none>              9275ce80b041        16 seconds ago       257MB
# | ajay291491/react_prod   latest              11caf318459e        16 seconds ago       126MB
# | <none>                  <none>              c6d262751382        About a minute ago   256MB
# | <none>                  <none>              0b886574ecc9        5 minutes ago        247MB
# | <none>                  <none>              f5c1b6280302        6 minutes ago        256MB
# | <none>                  <none>              ef62bf21449f        13 minutes ago       247MB
# | <none>                  <none>              8c73f402a1c4        15 minutes ago       256MB
# | <none>                  <none>              9f7d8478c0f7        15 minutes ago       256MB
# | <none>                  <none>              82b535bf2f96        15 minutes ago       256MB
# | <none>                  <none>              94abc5aea952        16 minutes ago       79.8MB
# | <none>                  <none>              33577cb89b91        16 minutes ago       227MB
# | <none>                  <none>              7d86be4e3d85        22 minutes ago       227MB
# | <none>                  <none>              1c8346e31bda        28 minutes ago       256MB
# | <none>                  <none>              7a3175ef4eb5        28 minutes ago       256MB
# | <none>                  <none>              927be43c0826        33 minutes ago       256MB
# | node                    alpine              d97a436daee9        12 days ago          79.3MB
# | nginx                   latest              e445ab08b2be        2 weeks ago          126MB
# | hello-world             latest              fce289e99eb9        7 months ago         1.84kB
#
# STEP 4 : Access the website and understand the logs 
#
# | [root@dockerstation01 frontend]# docker run -p 8080:80 11caf318459e
# | 192.168.122.1 - - [07/Aug/2019:15:38:11 +0000] "GET / HTTP/1.1" 200 2045 "-" "Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0" "-"
# | 192.168.122.1 - - [07/Aug/2019:15:38:12 +0000] "GET /static/css/main.2cce8147.chunk.css HTTP/1.1" 200 994 "http://dockerstation01.svr.apac.sathsang.net:8080/" "Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0" "-"
# | 192.168.122.1 - - [07/Aug/2019:15:38:12 +0000] "GET /static/js/2.b41502e9.chunk.js HTTP/1.1" 200 118777 "http://dockerstation01.svr.apac.sathsang.net:8080/" "Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0" "-"
# | 192.168.122.1 - - [07/Aug/2019:15:38:12 +0000] "GET /static/js/main.70c965f8.chunk.js HTTP/1.1" 200 1078 "http://dockerstation01.svr.apac.sathsang.net:8080/" "Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0" "-"
# | 192.168.122.1 - - [07/Aug/2019:15:38:12 +0000] "GET /static/media/logo.5d5d9eef.svg HTTP/1.1" 200 2671 "http://dockerstation01.svr.apac.sathsang.net:8080/" "Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0" "-"
# | 172.17.0.1 - - [07/Aug/2019:15:38:38 +0000] "GET / HTTP/1.1" 200 2045 "-" "curl/7.61.1" "-"
# | 172.17.0.1 - - [07/Aug/2019:15:39:01 +0000] "GET / HTTP/1.1" 200 2045 "-" "curl/7.61.1" "-"
# | 172.17.0.1 - - [07/Aug/2019:15:39:16 +0000] "GET / HTTP/1.1" 200 2045 "-" "curl/7.61.1" "-"
# | 
#
#------------------------------------------------------------------------------------------------------------------
#  Chapter 07 - CICD and Deployment to AWS Elasticbeanstalk
#
#  Udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437116#overview
#------------------------------------------------------------------------------------------------------------------
#
# - In this lesson learned about pushing the existing Dockerfile and associated files to a git repo
# - Then integrated git repo with a Travis CI and CICD tool. 
# - Then upadte the .travis.yml to the repo and commit that to repo
# - Then push the code and made sure the code gets build in Travis CI. 
# - Once the Travis build is successful then we have setup a AWS Elasticbeanstalk 
# - Then upadte the .travis.yml to the repo with the Deployment details of the elasticbeanstalk
# - Then we commit the code base
# - Then the application gets build and accessible 
#
# Few key configurations used in this chapter given below. 
#
# . Git repo             : https://github.com/ajay291491/docker-react.git
# . Travis               : https://travis-ci.org/ajay291491/docker-react
# . AWS Elasticbeanstalk : https://us-east-2.console.aws.amazon.com/elasticbeanstalk/home?region=us-east-2#/applications
#
# . Dockerfile 
#
# | [root@dockerstation01 frontend]# more Dockerfile
# | #
# | # {{  Block for bulding the apps }}
# | #
# | # Pulling node:alpine image with a block alias 'build_instance'
# | FROM node:alpine as build
# | # |
# | # Setting up working directory
# | WORKDIR /app
# | # |
# | # Copy pakcage.json for npm install
# | COPY package.json .
# | # |
# | # Perform NPM install
# | RUN npm install
# | COPY . .
# | # |
# | # Performing React build
# | RUN npm run build
# | # |
# | # |
# | #
# | # {{ Block for Starting the Production version of application via nginx }}
# | #
# | # Pulling the nginx image
# | FROM nginx
# | EXPOSE 80				=> This config used by Travis and AWS Elasticbeanstalk during build
# | # |
# | # Copy the build image '/app/build' from Conatiner with block alias 'build_instance' to nginix  document root
# | RUN mkdir -p /usr/share/nginx/html
# | COPY --from=build /app/build /usr/share/nginx/html/
# | [root@dockerstation01 frontend]# 
#
#
# . Travis config file 
#
# | [root@dockerstation01 frontend]# more .travis.yml 
# | sudo: required
# | services:
# |     - docker		=> Type of service to be used for build
# | before_install:
# |     - docker build -t ajay291491/docker-react -f Dockerfile.dev .		=> Build procedure 
# | 
# | script:
# |     - docker run -e CI=true ajay291491/docker-react npm run test -- --coverage	=> test case 
# | 
# | deploy:
# |     provider: elasticbeanstalk							=> defining AWS elasticbeanstalk as provider 
# |     region: "us-east-2"								=> AWS region
# |     app: "docker-react"								=> app name created in AWS elasticbeanstalk
# |     env: "DockerReact-env"								=> Environment in elasticbeanstalk
# |     bucket_name: "elasticbeanstalk-us-east-2-298337959615"				=> S3 bucket used for this 
# |     bucket_path: "docker-react"							=> Bucket path is the app name 
# |     on:
# |         branch: master								=> which branch to be used 
# |     access_key_id: $AWS_ACCESS_KEY							=> Access key ID setup within Travis project setting / Collected from AWS IAM as programatic user
# |     secret_access_key:
# |         secure: "$AWS_SECRET_KEY"							=> Access key setup within Travis project setting / Collected from AWS IAM as programatic user
# | [root@dockerstation01 frontend]# 
#
#
# NOTE : Whole training about this can be found at https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437116#overview
#
#------------------------------------------------------------------------------------------------------------------
# Chapter 08 - Complex Application's Development workflow
#
# Udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437274#overview
#------------------------------------------------------------------------------------------------------------------
#
# In Chapter 08 , 09 and 10 we will be bringing together a complex application with various components such as 
# . redis
# . postgres
# . nginx router for client and API/Server
# . React based Client
# . Node based worker
# . Node based server 
#
# Detailed workflow : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437294?start=32#bookmarks
# Nginix routing    : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437294?start=340#bookmarks
#
# NOTE : All above application will be instantiating a container while running and all these appliaction will be communicating each other via Docker network. 
#
# * Key taks in this Topic 
#
# 1. Download source code for worker , server and client to /app
# 2. Create Dockerfile.yml for worker, server and client
# 3. Build worker, server and client 
# 4. Start worker, server and client 
# 5. Go to project root and create docker-compose 
# 6. Defines supported services 
#    - Postgres
#    - Redis 
# 7. Define server service
#    - define build context
#    - build from Dockerfile.dev in server directory
#    - exclude the /app/node_modules volume to get inherited 
#    - map volume ./server with /app
# 8. Set the environment variables for server
# 	- redis environment variables 
#	- postgres environment variables 
#	
# 9. Define client service 
#   - define build context 
#   - build from Dockerfile.dev in client directory
#   - exclude the /app/node_modules volume to get inherited 
#   - map volume ./client with /app
#  
# 10. Define worker service 
#   - define build context 
#   - build from Dockerfile.dev in worker directory
#   - exclude the /app/node_modules volume to get inherited 
#   - map volume ./worker with /app
#  
# 11. Create nginx service 
#   - Create nginx directoty under complex root dir 
#   - Create default.conf with port details 
#   - create Dockerfile.dev for nginx with custom config 
#   - Update the Service  under docker-compose.yml with nginx
#   
# 12. Run docker compose and make sure application up and running in 3050
#
# Expectation : All above mentioned services are expected to run only in the local development box in this chapter 
#
#
# * Key Files 
#
# | [root@dockerstation01 complex]# pwd
# | /app/complex
# | [root@dockerstation01 complex]# more worker/Dockerfile.dev 
# | FROM node:alpine
# | WORKDIR /app
# | COPY ./package.json ./
# | COPY . .
# | RUN npm install  
# | CMD ["npm", "run", "dev"]
# | 
# | [root@dockerstation01 complex]# more client/Dockerfile.dev 
# | FROM node:alpine
# | WORKDIR '/app'
# | COPY ./package.json ./
# | RUN npm install
# | COPY . .
# | CMD ["npm", "run", "start"]
# | 
# | [root@dockerstation01 complex]# more server/Dockerfile.dev 
# | FROM node:alpine
# | WORKDIR "/app"
# | COPY . .
# | COPY ./package.json ./
# | RUN npm install
# | COPY . .
# | CMD ["npm", "run", "dev"]
# | 
# | [root@dockerstation01 complex]# more nginx/default.conf 
# | upstream client {
# |   server client:3000;		-> request from Browser will be routed to 3000
# | }
# | 
# | upstream api {			-> bakend communication will be routed to port 5000
# |   server api:5000;
# | }
# | 
# | server {
# |   listen 80;
# | 
# |   location / {
# |     proxy_pass http://client;
# |   }
# | 
# |   location /socketjs-node {
# |   proxy_pass http://client;
# |   proxy_http_version 1.1;
# |   proxy_set_header Upgrade $http_upgrade;
# |   proxy_set_header Connection "Upgrade";
# |   }
# | 
# | 
# |   location /api {
# |     rewrite /api/(.*) /$1 break;
# |     proxy_pass http://api;
# |   }
# | 
# | }
# | [root@dockerstation01 complex]# more nginx/Dockerfile.dev 
# | FROM nginx
# | COPY ./default.conf /etc/nginx/conf.d/default.conf
# | 
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# more docker-compose.yml 
# | version: '3'
# | services:
# | 
# |   postgres:
# |     image: 'postgres:latest'			-> Creating 'postgres' container with an image from docker hun
# | 
# |   redis:
# |     image: 'redis:latest'				-> Creating 'redis' container with an image from docker hun
# | 
# |   nginx:						-> Custom nginx build on top of the image from Docker hub
# |     restart: always 
# |     build:
# |       dockerfile: Dockerfile.dev
# |       context: ./nginx
# |     ports:
# |       - '3050:80'
# | 
# |   api:						-> backend APi referencing to ./server directory 
# |     build: 
# |       dockerfile: Dockerfile.dev
# |       context: ./server
# |     volumes: 
# |     #- /app/node_modules				-> Volume to exclude in mapping
# |       - ./server:/app				-> Volume to map with local 
# |     environment:			
# |       - REDIS_HOST=redis				-> variable will be active in container's 'env' will be referenced by node application in its codebase
# |       - REDIS_PORT=6379
# |       - PGUSER=postgres
# |       - PGHOST=postgres
# |       - PGDATABASE=postgres
# |       - PGPASSWORD=postgres_password
# |       - PGPORT=5432
# | 
# |   client:						-> Client or react application 
# |     build:
# |       dockerfile: Dockerfile.dev
# |       context: ./client
# |     volumes:
# |       - /app/node_modules				-> VOlumes to exclude 
# |       - ./client:/app				-> VOlume to map
# | 
# |   worker:						-> worker node 
# |     environment:					-> variables used in container runtime in 'env' will be referenced by node
# |       - REDIS_HOST=redis
# |       - REDIS_PORT=6379
# |     build:
# |       dockerfile: Dockerfile.dev
# |       context: ./worker
# |     volumes: 
# |       - /app/node_modules				-> Volumes to  exclude
# |       - ./worker:/app				-> Volumes to map
# | [root@dockerstation01 complex]# 
# | 
#
#
#------------------------------------------------------------------------------------------------------------------
# Chapter 09 - Complex Application's CICD pipeline setup via Travis CI
#
# Udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437326#questions
#------------------------------------------------------------------------------------------------------------------
#
# In this chapter we will be converting the dev application created in chapter 08 to prod. 
# Also we will be introducing a Travis CI to build the containers 
# Then we will push the images to docker hub using docker push via travis CI
#
# * Key Tasks 
#
# | 1. Create Production Dockerfile for client app 
# | 2. Create production Dockerfile for api/server app
# | 3. Created Production Dockerfile for worker 
# | 4. Create Production Dockerfile for nginx 
# | 5. Create custom config for nginx default.conf
# | 6. Create .travis.yml
# |    - update version 
# |    - update docker service
# |    - upate test case for client app and its dev build
# |    - create docker build for client app
# |    - create docker build for api/server app
# |    - create docker build for worker app
# |    - create docker build for nginx 
# |    - Login to docker cli 
# |    - upate doccker cli credentials in travis
# |    - Push client app image
# |    - push ap/server app image 
# |    - push worker app image 
# |    - push nginx app image
# | 7. Create new git repo multi-docker
# | 8. Push the code to the git repo 
# | 9. Conect travis CI with the new Git repo 
# | 10. Configure Docker credentials 
# | 10. Commit the new code base to the git repo 
# | 11. As soon commit arrived Travis should perform build
# | 
#
# * Key Repos 
#
# Docker repo : https://cloud.docker.com/u/ajay291491/repository/list
# Git Repo    : https://github.com/ajay291491/multi-docker
# Travis Project : https://travis-ci.org/ajay291491/multi-docker   

#
# * Key config files 
#
# | [root@dockerstation01 complex]# more client/nginx/default.conf 
# | server {
# |   listen 3000;
# | 
# |   location / {
# |     root /usr/share/nginx/html;
# |     index index.html index.htm;
# |     try_files $uri $uri/ /index.html;
# |   }
# | }
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# more client/Dockerfile
# | FROM node:alpine as builder
# | WORKDIR '/app'
# | COPY ./package.json ./
# | RUN npm install
# | COPY . .
# | RUN npm run build
# | 
# | FROM nginx
# | EXPOSE 3000
# | COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf
# | COPY --from=builder /app/build /usr/share/nginx/html
# | 
# | 
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# more server/Dockerfile
# | FROM node:alpine
# | WORKDIR "/app"
# | COPY . .
# | COPY ./package.json ./
# | RUN npm install
# | COPY . .
# | CMD ["npm", "run", "start"]
# | 
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# more worker/Dockerfile
# | FROM node:alpine
# | WORKDIR /app
# | COPY ./package.json ./
# | COPY . .
# | RUN npm install  
# | CMD ["npm", "run", "start"]
# | 
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# more nginx/Dockerfile
# | FROM nginx
# | COPY ./default.conf /etc/nginx/conf.d/default.conf
# | 
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# more .travis.yml 
# | sudo: required
# | services:
# |   - docker
# | 
# | before_install:
# |   - docker build -t ajay291491/react-test -f ./client/Dockerfile.dev ./client
# | 
# | script:
# |   - docker run -e CI=true ajay291491/react-test npm test -- --coverage
# | 
# | after_success:
# |   - docker build -t ajay291491/multi-client ./client
# |   - docker build -t ajay291491/multi-server ./server
# |   - docker build -t ajay291491/multi-nginx ./nginx
# |   - docker build -t ajay291491/multi-worker ./worker
# |   # upload to docker repo by login to docker cli
# |   - echo "$DOCKER_PASSWORD"|docker login -u "$DOCKER_ID" --password-stdin 
# |   # Push the imags to docker hub
# |   - docker push ajay291491/multi-client
# |   - docker push ajay291491/multi-server
# |   - docker push ajay291491/multi-nginx
# |   - docker push ajay291491/multi-worker
# | [root@dockerstation01 complex]# 
# | [root@dockerstation01 complex]# 
# | 
#
#
#------------------------------------------------------------------------------------------------------------------
# Chapter 10 - Complex Application's AWS Deployment in Production
#
# Udemy : https://www.udemy.com/docker-and-kubernetes-the-complete-guide/learn/lecture/11437352#questions
#------------------------------------------------------------------------------------------------------------------
#
# In this chapter we will be deploying the Complex application into the AWS EB which uses AWS ECS. 
# You will be getting an exposure to understand below technologies too 
#
# . AWS Elasticbeanstalk
# . AWS ECS (Elastic Container Services)
# . AWS RDS (relation database system)
# . AWS MemCache (redis)
#
# In chapter will be using AWS RDS instead of postgres container in the development version 
# Also will be using the memcache instead of Redis which we have used in development version 
#
# * Key Tasks 
# 1. Create the Dockerrun.aws.json
# 2. Define the Json formatted manifest 
# 3. Define the container relationship in nginix defintions
# 4. Validate the and push the code to Git 
# 5. Create EB 
#
#
# * Key configuration files
#
# | [root@dockerstation01 complex]# more Dockerrun.aws.json 
# | {
# |     "WSEBDockerrunVersion": 2,
# |     "ContainerDefinitions": [{
# |                     "name": "client",
# |                     "image": "ajay291491/multi-client",
# |                     "hostname": "client",
# |                     "essential": false
# |             },
# |             {
# |                     "name": "server",				-> name attribue used un AWS context
# |                     "image": "ajay291491/multi-server",
# |                     "hostname": "api",				-> Hostname atrtribute used in container or application level
# |                     "essentail": false
# |             },
# |             {
# |                     "name": "worker",
# |                     "image": "ajay291491/multi-worker",
# |                     "hostname": "worker",
# |                     "essential": false
# |             },
# |             {
# |                     "name": "nginx",
# |                     "image": "ajay291491/multi-nginx",
# |                     "hostname": "nginx",
# |                     "essential": true,
# |                     "portMappings": [{
# |                             "hostPort": 80,
# |                             "containerPort": 80
# |                     }],
# |                     "links": ["client", "server"]			-> This means the container to which nginx container should communicate with 
# |             }
# |     ]
# | 
# | }
# | [root@dockerstation01 complex]# 
# | 
#
#------------------------------------------------------------------------------------------------------------------
# Chapter 11 - Onwards to Kubernetes
#
# Udemy : https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/11482922#overview
#------------------------------------------------------------------------------------------------------------------
#
# * What is Kubernetes
# Kubernetes is an utility which will help to run multiple containers on multiple systems.
#
# * Why you need Kubernetes
# Traditionally when you perform a scaling operation in a micro service architecture it will scale multiple containers instantly, regardless the service which require actual scaling.
# To solve this problem kubernetes gives you the liberty to scale up or down only of the type of containers which required to be scaled up or down.
# This will help you manage your billing much more efficiently only based on what you need.
#
# * What is a MiniKube
# Minikube is an utility to configure the kubernetes in a development environment where you can start practice or test your containers.
# This utility actually spins up a virtual  machine where you will be able to submit your requests via kubectl and your containers will be deployed to teh nodes.
#
# * What are key managed services for kubernetes 
# When you are looking for kubernetes in production then you will need to look for managed services provided by cloud providers. 
# Couple of main providers are 
#
# EKS - Amazon's elastic KUbernetes service 
# GKE - Google Kubernetes Engine
#
# * API Version 
# When you define a config file in Kubernetes it defined how you can define an object in that config file. 
# Based on different API verion you will have different object also available 
#
# Syntax : apiVersion: <api_version>
#
# * What is Kubernetes Objects 
# In Kubernetes objects are used to represent the state of sereveral components in the cluster such as,
# - Deployed containarized application and their workloads 
# - Their networking 
# - Disk resources 
# - Information about what cluster is doing 
#
# When you define a object in the Yaml config file you will represent it as "Kind"
#
# Basic objects are 
# . Pod
# . Service
# . Volume
# . Namespace
#
# Kubernetes rely on higher level abstraction which rely on controllers to build upon the basic objects and provide addition functionality and convenience feature, this include 
# . Deployment
# . DaemonSet
# . StatefulSet
# . ReplicaSet
# . Job
#
# Syntax - kind: <object_name>
#
# * What is pod 
# Pod is the smallest unit in a kubernetes cluster which used to run a single container or a group of continers. 
# It is not posible to run a conatiner alone in k8s, it has to be part of a pods, when container runs from a pods it will have its own unique ID. 
# You can run multiple different type of container in same pod, if there are reason to include such as a logging or backup sidecar container.
#
# - Metadata : Metadata is the identity of that pod when its getting referred in other obejcts 
#              A pod will have a name and label as part of the meta-data. 
#              name : will help to identify the container in kubectl output or logs 
#
# - labels   : Labels are used to refer the pod object in another objects such as networking components etc 
#              Label will always have a key value pair and this will be the pair declared as 'selector' in other objects such as networking serice etc to identify the pods.
#
#
# - spec    : spec is going to where we are going to define the container details and key details are 
#             name  : name of the conatiner 
#             image : This should be a valid path for a conatiner image in docker hub or other image repository 
#             ports : Here we will defining 'containerPort' as a list of ports which we are going to expose from the container 
#                     This will be depend on which port application is starting  
#
# Reference : https://kubernetes.io/docs/concepts/workloads/pods/pod/
#
# Syntax : 
#
# apiVersion: v1
# kind: pod 
# metadata: 
#   name: <name_of_pod>
#   labels:
#     <key> : <value>
# spec: 
#   containers:
#     - name: name of container
#       image: <path_of_the_image>
#       ports: 
#         - containerPort: <port_number>
#     
# Example : Below example show sample config and its various components in config file 
#
# | root@sathsang-Predator-G3620:/apps/kubernetes/client_app# cat client-pod.yaml
# | apiVersion: v1	==> api version
# | kind: Pod		==> Object pod for containarized apps 
# | metadata:
# |     name: client-pod
# |     labels:
# |             components: web
# | spec:
# |   containers:
# |     - name: client
# |       image: stephengrider/multi-client
# |       ports:
# |         - containerPort : 3000
# | root@sathsang-Predator-G3620:/apps/kubernetes/client_app# 
#
#
# * What is Service 
# Service is a way to enable networking components for the pods running containers. 
# Service identifies its asociated pods by using "selector" which refers to a key value pair defined under the "metadata:label" section in the pods.
#
# There are mainly four types of services kubernetes offering 
#
# . NodePort 	 : Exposes the Service on each Node’s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created.
# . ClusterIP	 : Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.
# . LoadBalancer : Exposes the Service externally using a cloud provider’s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.
# . ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record
#
# While defining a service below are the key fields you will be defining 
#
# . metadata : here we can define the name of the service and it will be appearing in the kubectl and logs output 
# . spec     : here is where we define which type of port and other details, few key details are given below 
#              type     : type of port you are enabling 
#              Ports    : list of port mapping information 
#                port       : Port you are opening externally
#                targetPort : port binding with the container
#              selector : here we connect the pod by adding the key value pair defined under "metadata:label" in the pod
#
# Reference : https://kubernetes.io/docs/concepts/services-networking/service/
#
# Syntax : 
# apiVersion: v1
# kind: Serice
# metadata:
#   name: <name_of_the_service>
# spec:
#   type: <Type of port exposing>
#   ports:
#     - port: 
#       target_port: <port_targeted_in_pod>
#       nodeport: <optional, if you want to expose a node port for other apps>
#   selector:
#     <key> : <value> (as defined in the pods label)
#
# Example : Below example shows how to define a port
#
# | root@sathsang-Predator-G3620:/apps/kubernetes/client_app# cat client-node-port.yaml
# | apiVersion : v1	==> api version
# | kind: Service	==> Object Service for Networking components 
# | metadata:
# |   name: client-node-port
# | spec:
# |   type: NodePort	==> Opening nodeport
# |   ports:
# |     - port: 3050		==> Port exposed to external
# |       targetPort: 3000	==> Port opened on container
# |       nodePort: 31515	==> Node port getting opened
# |   selector:
# |     component: web
# |
# | root@sathsang-Predator-G3620:/apps/kubernetes/client_app#
#
#
# * How to apply Pods and Service Objects 
# Once you have created the Pods and Service Object YAML files then you can follow below steps to apply them
#
# | $ kubectl apply -f client-pod.yaml
# | pod/client-pod created
# | $
#  
# | $ kubectl apply -f client-node-port.yaml
# | service/client-node-port created
# | $
#  
# | $ kubectl get pods
# | NAME         READY   STATUS    RESTARTS   AGE
# | client-pod   1/1     Running   0          39s
# | $
# |
#  
# | $ kubectl get services
# | NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
# | client-node-port   NodePort    10.96.55.16   <none>        3050:31515/TCP   11s	==> In this case you will be able to access the application using NodePort
# | kubernetes         ClusterIP   10.96.0.1     <none>        443/TCP          24h
# | $
#  
# | $ curl -k http://192.168.99.100:31515
# | <!doctype html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name="theme-color" content="#000000"><link rel="manifest" href="/manifest.json"><link rel="shortcut icon" href="/favicon.ico"><title>React App</title><link href="/static/css/main.c17080f1.css" rel="stylesheet"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div><script type="text/javascript" src="/static/js/main.a2449523.js"></script></body></html>$
# | $
# 
#
# * Nodes in Kubernetes 
# Kuberntes have mainly two different types of nodes, one is "Master" or its called Kube-api server and other one is "Worker Nodes". 
# When you perform a kubetctl apply then there will be a set of process happens between master and worker nodes, details are explained below. 
#
# Master : Master or kube api server is responsible for running the entire cluster configurations
#          When you run a kubectl command, that time you are directly interacting with the kube api server 
#          WHen you request to create a set of containers in a pod, that time master will go and look for any pods of that running in any worker nodes, if not it will start them.
#          On top of this if any of the pods are not running sufficeint number of containers in pods then it will take action to bring it on to the requested level.
#          If any containers gets terminated, then Master will take necesary action to bring them back by restart 
#
# Worker node : Worker nodes are resposisble run the pods and containers as requested by the master. 
#
# To understand this in detail go through : https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/11482942#overview
#
# * Tips * 
#
# $ kubectl get pods      			=> This will list the pods 
# $ KUbectl get services  			=> This will list the services
#
# $ kubectl describe <pod> <pod_name>  		=> This will decribe the pod with lot of details 
# $ kubectl describe service service_name 	=> This will describe the service details 
#
# $ kubectl get nodes -o wide			=> This will list the nodes
# $ kubectl describe nodes <node_name>		=> This will give the details about the nodes 
#
#
#------------------------------------------------------------------------------------------------------------------
# Chapter 12 - Maintaining sets of containers with Deployments 
#
# Udemy : https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/11482954#overview
#------------------------------------------------------------------------------------------------------------------
#
# * Imperative approach vs Declarative 
# In Kubernetes you can manage your cluster pods and services in two different ways mainly. 
# 
# 1. Imperative  : This is the method where you will be using list of kubectl command to configure your desired state.
# 2. Declarative : In this method you will be defining your desired state in the config file and you will let kubernetes to manage the state based on the same.
#
# * How to update the config for an existing pods 
# To update the config for an existing pod we can update config file and then apply the same. 
# However objects like pods has only very few flexibility such as container image etc and doesn't allow to change property such as ContainerPort etc 
# To accomodate such changes we will need to change from using pods to Deployment object another kubernetes object which provides more flexibility to support more changes.
#
# * Pods vs Deployment
# Pods are normally good at dealing with development when you are dealing with production you will need to look for another kind object "Deployment"
# Reason for choosing Deployment vs Pods is given below 
# 
# Pods       : Runs a single set of containers 
#            : Good for Dev purpose 
#            : Rarely used in Production or not at all 
#
# Deployment : Runs a set of identical pods, one or more 
#            : Monitor the state of each pod and updates as necessary according to changes to failures in pods 
#            : Good for Dev and Production 
#
#
# * Deployment
# Deployment is ued to run set of identical pods in a kubernetes cluster. Below is the syntax to define the Deployment 
#
# Syntax : Below is a sample Syntax 
#
# apiVersion: apps/v1				=> This is the API version which supports Deplyment kind object
# kind: Deployment				=> Here e choose kind as Deplyment instead Pod
# metadata:
#   name: <name of pod deployment>		=> Name of the pod deployment which identied in kubectl comands and logs 
# spec:
#   replicas: <number of replicas needed>	=> Number of replicas need to create 
#   selector:					=> This selctor will look at the label inside the templte defnition and process accordingly, kind of repetative but needed
#     matchLabels:
#       <key>: <value>
#   template:					=> This will be location where all Pod related config will be updated, similar Pod kind object
#     metadata:
#       labels: 
#         <key>: <value>
#     spec:
#       conatiners:
#         - name : <name of container>
#           image: path of conatiner image
#           ports:
#             - containerPort: <port_number>
#
# Example : Below is a sample using Deployment, which use the same service config mentioned in last chapter 
#
# | $ cat client-DeploymentMethod.yaml
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: client-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: web
# |   template:
# |     metadata:
# |       labels:
# |         component: web
# |     spec:
# |       containers:
# |         - name: client
# |           image: stephengrider/multi-client
# |           ports:
# |             - containerPort : 3000
# |
# | $
#
# | $ kubectl apply -f client-DeploymentMethod.yaml                                                        
# | deployment.apps/client-deployment created
# | $                                                        
# | $ kubectl get pods
# | NAME                                 READY   STATUS    RESTARTS   AGE
# | client-deployment-5dfb6bf966-9xnl5   1/1     Running   0          9s
# | $
# | $ kubectl get deployment
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   1/1     1            1           20m
# | $
#
# * How to scale the deployments 
# If you want to increase or decrease the number of replicas in the kubernetes then you can update the key 'replicas' key in the spec 
# Once you have done then perform below command tp apply the configuration.
#
# Example : We are increaseing the replicas to 5 using the above config 
#
# | $ kubectl get deployments
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   1/1     1            1           15h
# | $ 
# | $ kubectl apply -f client-DeploymentMethod.yaml 
# | deployment.apps/client-deployment configured
# | $ 
# | $ kubectl get deployments
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   5/5     5            5           15h
# | $ 
#
#
# * Challenges with updating new container images in kubernetes 
# If you build a new image for your application and you are trying to deploy the same into kubernetes there could be challenges in getting your deployments. 
# Challenges can be based on three scenarios 
#
# Scenario 1 : You have marked container image with "<container_name>:latest" or just <container>.
#              In this situation when you perform kubectl apply, kuberntes won't deploy the new image since there is no change detected on the deployment config file.
#
# Scenario 2 : If you are using Image tagging for the container images you are building and you maintain that in the config file 
#              Lets assume you are using deployment pipeline to build your container and then update the kuberntes cluter, this will also create the problem
#              Since your container tag is getting updted during pipeline, you do not have a chance to update tge deployment config 
#
# There are couple of solutions you can make use to overcome this, but all of them has its downside 
#
# Solution 1 : Kill the existing pods using 'kubectl delete pods <pod_name>' command this will try to launch the new container from new image.
# Solution 2 : manually update the tag number in the eployment config and then apply the same 
# Solution 3 : Update the Image name for the deployment using the imperative method. This is more of a imperative method
#
# Syntax :  kubectl set image deployment/<deployment_name> <container_name>=<image_with_tag>
#
# Example : Below example shows how to update deployment 
#
# | $ kubectl set image deployment/client-deployment client=stephengrider/multi-client:v5
# | deployment.apps/client-deployment image updated
# | $
#
# | $ kubectl get deployments
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   4/5     5            4           16h
# | $
#
# | $ kubectl get pods
# | NAME                                 READY   STATUS        RESTARTS   AGE
# | client-deployment-5dfb6bf966-kgt6p   0/1     Terminating   0          59m
# | client-deployment-6c59d7566f-2zw48   1/1     Running       0          14s
# | client-deployment-6c59d7566f-cb96f   1/1     Running       0          23s
# | client-deployment-6c59d7566f-dscxp   1/1     Running       0          23s
# | client-deployment-6c59d7566f-ghng8   1/1     Running       0          23s
# | client-deployment-6c59d7566f-trxg5   1/1     Running       0          11s
# | $
#  
# | $ kubectl describe pod client-deployment-6c59d7566f-ghng8 |grep Image
# |     Image:          stephengrider/multi-client:v5
# |     Image ID:       docker-pullable://stephengrider/multi-client@sha256:5e2a93a2bcd1ab55756fca4de120ca3ad13ac7965ae1cd9435f6cc9f464cec4e
# | $
#
#
# * Tips * 
#
# $ kubectl get deployments 				=> To list the deployments 
# $ kubectl describe deployment <deployment_name>	=> To get the details about the deployments 
#
# $ kubectl set image deployment/<deployment_name> <container_name>=<image_with_tag>  => This will help you to set the image value for a deployment
#
#
#------------------------------------------------------------------------------------------------------------------
# Chapter 13 - Multi Container App with Kubernetes
#
# Udemy : https://www.udemy.com/course/docker-and-kubernetes-the-complete-guide/learn/lecture/11500808#overview
#------------------------------------------------------------------------------------------------------------------
# 
# * ClusterIP
# As mentioned in the earlier chapter ClusterIP will only expose the internal IP of the deployment which are currently running. 
# Configuring a clusterIP will be useful for components in the kubernetes cluster to access each other. 
# Suppose lets a a frontend pod within the cluster want to reachout to a backend pod then the only method by which it can reachout internally by using the ClusterIP.
# In a nutshell ClusterIP will helps the pods to communicate each other
#
# Syntax : Below syntax tell how to create a CLusterIP service 
#
# apiVersion: v1
# kind: Service
# metadata:
#   name: <name of the ClusterIP Service>
# spec:
#   type: ClusterIP
#   selector:
#     <key> : <value>    ==> This will be the key value pair of the label for referring to Pod Deployment
#   ports:
#     - port: <port number for cluster IP access>
#       targetPort: <port number to map from conatainer>
#
# NOTE : You will be able to see lot of example for ClusterIP at below examples between STEP 1-6
#
#
# * Storage Options in Kubernetes 
# When it comes to Kubernetes you will need to understand about mainly three terminologies such as volumes, persistant volume (PV), Persistant volume claim (PVC)
# Lets take a detail look at each options and what they means.
#
# 1. Volume : Volumes are the ephermeral storage types available in kubernetes 
#           : They are available within the Pod and available as long the Pod available 
#           : Sorage gets destroyed as soon as the pods gets destoyed or recreated & data won't be available 
#           : Good for storing temporary caches and not good for storing data
#
# 2. Persistant Volume (PV) : Persistant volumes are not tied to Pod and they reside outside the Pod 
# 			    : They are available even after the pods getting destroyed or recreated 
# 			    : Data will be stored permenantly and available even after the pod is destroyed
# 			    : This is ideal for storing data and also to host databases
#
# 3. Persistant Volume Claim (PVC) : PVC is way for Pod to claim for its Peristant Volume (PV) dynamically 
# 				   : When you are hosting a pod on service providers such as AWS, GCP or Azure you will need to make a claim for its required volume
# 				   : For example AWS issues EBS as the persistant Volume and for a pod to request that it will need to make PVC claim for the required PV size
# 				   : Then the service provider issues the PV for that pod according to the requested claim
# 				   : There are mainly three access modes for PVC 
# 				     - ReadWriteOnce : the volume can be mounted as read-write by a single node
# 				     - ReadOnlyMany – the volume can be mounted read-only by many nodes
# 				     - ReadWriteMany – the volume can be mounted as read-write by many nodes
#
# NOTE : for PVC syntax and example please refer to STEP 7
# 
# 4. Storage Claim : A StorageClass provides a way for administrators to describe the “classes” of storage they offer. 
#                  : Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. 
#                  : Kubernetes itself is unopinionated about what classes represent. This concept is sometimes called “profiles” in other storage systems.
#                  : Each StorageClass contains the fields provisioner, parameters, and reclaimPolicy, which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned.
#
# NOTE : If need more details on the StorageClass then you can refer https://kubernetes.io/docs/concepts/storage/storage-classes/
#
# Syntax : Below is the syntax for StorageClaim
#
# apiversion: storage.k8s.io/v1
# kind: StorageClass
# metadata:
#   name: <name of the storage claim>
# provisioner: kubernetes.io/<provider-name>   =>   Example is "provisioner: kubernetes.io/aws-ebs"
# parameters:
#   type: <storage type>
#   iopsPerGB: <number>
#   fsType: <fs_type)
#
# Example : Below is an example for StorageClaim
#
# | apiVersion: storage.k8s.io/v1
# | kind: StorageClass
# | metadata:
# |   name: slow
# | provisioner: kubernetes.io/aws-ebs
# | parameters:
# |   type: io1
# |   iopsPerGB: "10"
# |   fsType: ext4
#
#
# * Environment Variables
# As part of running containers there will be requirement to create variables during container runtime so that the underlying container application can access various resources in k8s cluster.
# For example, in our complex app if server deployment container need to have access to other redis and postgres deployments then it will need to know the ClusterIP for those containers which created in runtime
# Similarly when we design an application in kubernetes there will be lot of requirements to configure the environment variables. 
#
# Below is the syntax to define a variable under kubernetes container
#
# Syntax : 
# 
# apiVersion: v1
# kind: Deployment
# metadata:
#   name: {name-of-the-deployment}
# spec:
#   replicas: {number-of-replicas}
#   selector:
#     matchLabels:
#       {key} : {value}
#   template:
#     metadata:
#       labels:
#         {key}: {value}
#     spec:
#       containers:
#         - name: {name-of-the-container}
#           image: {image-name}
#           ports:
#             containerPort: {port-to-open}
#           env:
#             - name: {name-of-the-env-variable-1}
#               value: {value-of-the-env-variable-1}
#             - name: {name-of-the-env-variable-2}
#               value: {value-of-the-env-variable-2}
#     
# On above syntax we have seen how we can define a env variable as name and value under a conatiner definition in the pod deployment. 
# But there are mutiple ways you can access the value of the variable in an env
#
# . Define Directly : In this method you defining both name and value in the conatiner definition itself 
#   env:
#     name: GREETING
#     value: "hello, how are you !!!"	 ==> Make sure these direct values you are enclosed with in the double quotes 
#
# . Access from other Serice configs : Lets say if you want to refer to a ClusterIP of another deployment so that the cotainer can communicate it with.
#                                      In this case you refer to the name of the service of that deployment so that it can get that ClusterIP as variable 
#   env:
#     name: REDIS_HOST
#     value: redis-cluster-ip-service    ==> This is exactly the name mentioned for the redis service which refers to redis deployment, make sure you do not use single or double qoutes here 
#
# Example : To see examples on this, please refer to the STEP 8
#
# * Accessing Secrets using variables 
# Incase you have passwords which you want to pass as an environment variable you can still due to that using environment variables, but using a small workaround 
# This will be a two step proces where you will need intially create secret in the kubernetes cluster and then you will need to refer that into the variable
#
# 1. Creating a secret for the pasword 
#
# Syntax : kubectl create secret {generic|tls|docker-registry} {name-of-the-secret} --from-literal {key}={value}  
#
# Example : kubectl create secret generic pgpassword --from-literal PGPASSWORD=PG*!23
#
# 2. Refer the secret you have just created under the deployment:spec:template:container as env
#
# Syntax : 
#   env:
#     - name: {name-of-the-env-variable}
#       valueFrom: 
#         secretKeyRef: 
#           name: {name-of-the-secret-created}
#           key: {key-provided-while-creating-secret}`
#
# Example: 
#   env:
#     - name: POSTGRES_PASSWORD
#       valueFrom:
#         secretKeyRef:
#           name: pgpassword
#           key: PGPASSWORD 
#
# NOTE : To see detailed examples please refer to STEP 8
#
#
# * Ingress Controller 
# To allow connection to any of the application running in the kubernetes cluster you will need to configure Ingres controller on them.
# When you confgure ingress controller on the it actually managed by an nginx pod running as the ingress controller in the cluster. 
# You can define the Ingress controller as an object in the YAML configuration file. 
#
# To Know more abount Kubernets, please refer below documentation 
# https://www.nginx.com/blog/nginx-plus-ingress-controller-kubernetes-load-balancing/
# https://www.joyfulbikeshedding.com/blog/2018-03-26-studying-the-kubernetes-ingress-system.html
#
# To define an ingress we will need to create an object as below 
#
# Syntax : 
#
# apiVersion: extensions/v1beta1
# kind: Ingress
# metadata:
#   name: {name-of-the-ingress-service}
#   annotations:
#     kubernetes.io/ingress.class: nginx			=> defining Ingress as nginix Ingres 
#     nginx.ingress.kubernetes.io/rewrite-target: /		=> defining default rewrite rule for ingress
# spec:
#   rules:
#     - http:
#         paths:
#           - path: /									=> This will be path used in the main url
#             backend:
#               serviceName: {service-cluster-ip-configured-for-app-deployment_1}	=> this will call the curresponding cluster to map in below port for path requested 
#               ServicePort: {Port-number-exposed-in-service_1}
#	    - path: /api/
#	      backend:
#	        serviceName: {service-cluster-ip-configured-for-app-deployment_2}
#	        ServicePort: {Port-number-exposed-in-service_2}
#       
# Example : To see detailed example, please STEP 9 
#
#
# * Converting the existing app into Kubernetes way
# In this chapter we will try to move the complex app which we put together as part of the Elastic beanstalk into Kubernetes 
# We ill take an approach to delete the existing deployment we have created and then we will create new configs using Service ClusterIP and will deploy that minikube initially 
# Then we will take to the Kubernernetes production environment 
# 
# * How to delete existing Deployment 
# We can delete the existing deployments using below imperative method, details given with an example 
#
# Syntax : kubectl delete deployments <deployment_name>
#
# Example : Deleting a deployment 
#
# | $ kubectl get deployments
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   5/5     5            5           18h
# | $ kubectl delete deployment client-deployment 
# | deployment.apps "client-deployment" deleted
# | $ 
#
#
# * How to delete an existing service 
# use below imperative method to delete a service in cluster 
#
# Syntax : kubectl delete services <service_name>
#
# | $ kubectl get services
# | NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
# | client-node-port   NodePort    10.96.55.16   <none>        3050:31515/TCP   24h
# | kubernetes         ClusterIP   10.96.0.1     <none>        443/TCP          2d1h
# | $ kubectl delete services client-node-port
# | service "client-node-port" deleted
# | $ kubectl get services
# | NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
# | kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   2d1h
# | $ 
#
#
# 							 **** From here we will demonstrate all Examples which exaplined above ***
#
#
# STEP 1: Deploy client application in minikube
#
# | $ tree k8s/					=> New config files created 
# | k8s/
# | ├── client-clusterIP-service.yaml
# | └── client-deployment.yaml
# | 
# | 0 directories, 2 files
#
# | $ cat k8s/client-deployment.yaml 		=> Deployment config 
# | apiVersion: apps/v1
# | kind: Deployment 
# | metadata:
# |   name: client-deployment
# | spec:
# |   replicas: 3
# |   selector:
# |     matchLabels:
# |       component: web
# |   template:
# |     metadata:
# |       labels:
# |         component: web
# |     spec:
# |       containers:
# |         - name: client 
# |           image: stephengrider/multi-client
# |           ports:
# |             - containerPort: 3000
# | $
#
# | $ cat k8s/client-clusterIP-service.yaml		 => Service object file with ClusterIP
# | apiVersion: v1
# | kind: Service
# | metadata:
# |   name: client-cluster-ip-service
# | spec:
# |   type: ClusterIP					=> Defining the type of the service as ClusterIP
# |   selector:
# |     component: web
# |   ports:
# |     - port: 3000					=> ClusterPort which get exposed for other pods to access the Deployment pod 
# |       targetPort: 3000				=> Port getting exposed from the container 
# | 
# | 
# | $ kubectl apply -f k8s				=> applying both Deployment and Service under the K8s directory together 
# | service/client-cluster-ip-service created
# | deployment.apps/client-deployment created
# | $ kubectl get deployments
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   2/3     3            2           10s
# | $ kubectl get pods
# | NAME                                 READY   STATUS    RESTARTS   AGE
# | client-deployment-5dfb6bf966-kchkt   1/1     Running   0          14s
# | client-deployment-5dfb6bf966-pcjvz   1/1     Running   0          14s
# | client-deployment-5dfb6bf966-vwfpj   1/1     Running   0          14s
# | $ kubectl get services
# | NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
# | client-cluster-ip-service   ClusterIP   10.107.236.37   <none>        3000/TCP   19s
# | kubernetes                  ClusterIP   10.96.0.1       <none>        443/TCP    2d1h
# | $ 
# | 
# | 
#
# STEP 2 : Deploy server application and its services 
#
# | $ tree k8s
# | k8s
# | ├── client-clusterIP-service.yaml
# | ├── client-deployment.yaml
# | ├── server-clusterIP-service.yaml
# | ├── server-deployment.yaml
# | └── worker-deployment.yaml
# | 
# | 0 directories, 5 files
# | $
# | $ cat k8s/server-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: server-deployment
# | spec:
# |   replicas: 3
# |   selector:
# |     matchLabels:
# |       component: server
# |   template:
# |     metadata:
# |       labels:$
# |         component: server
# |     spec:
# |       containers:
# |         - name: server
# |           image: stephengrider/multi-server
# |           ports:
# |             - containerPort: 5000
# | $ 
# | $ cat k8s/server-clusterIP-service.yaml 
# | apiVersion: v1
# | kind: Service
# | metadata:
# |   name: server-cluster-ip-service
# | spec:
# |   type: ClusterIP
# |   selector:
# |     component: server
# |   ports:
# |     - port: 5000
# |       targetPort: 5000
# | $ 
#
# STEP 3 : Deploying worker application and it doesn't need a CliusterIP Service needed
#
# | $ cat k8s/worker-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: worker-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: worker
# |   template:
# |     metadata:
# |       labels:
# |         component: worker
# |     spec:
# |       containers:
# |         - name: worker
# |           image: stephengrider/mult-worker
# | $ 
#
# STEP 4 : Applying all deployment and Services configured above 
#
# | $ kubectl apply -f k8s
# | service/client-cluster-ip-service unchanged
# | deployment.apps/client-deployment unchanged
# | service/server-cluster-ip-service created
# | deployment.apps/server-deployment created
# | deployment.apps/worker-deployment created
# | $ 
# | $ kubectl get deployments 
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   3/3     3            3           114m
# | server-deployment   0/3     3            0           10s
# | worker-deployment   0/1     1            0           10s
# | $ 
# | $ kubectl get pods 
# | NAME                                 READY   STATUS    RESTARTS   AGE
# | client-deployment-5dfb6bf966-kchkt   1/1     Running   0          119m
# | client-deployment-5dfb6bf966-pcjvz   1/1     Running   0          119m
# | client-deployment-5dfb6bf966-vwfpj   1/1     Running   0          119m
# | server-deployment-5fdbfcfc99-7zhlr   1/1     Running   0          5m27s
# | server-deployment-5fdbfcfc99-m94dh   1/1     Running   0          5m27s
# | server-deployment-5fdbfcfc99-xqr7x   1/1     Running   0          5m27s
# | worker-deployment-56f6bf5bb8-4vp2z   1/1     Running   0          7s
# | $
# | 
# | 
#
# STEP 5 : Applying redis configuration 
#
# | $ tree k8s/
# | k8s/
# | ├── client-clusterIP-service.yaml
# | ├── client-deployment.yaml
# | ├── redis-clusterIP-service.yaml
# | ├── redis-deployment.yaml
# | ├── server-clusterIP-service.yaml
# | ├── server-deployment.yaml
# | └── worker-deployment.yaml
# | 
# | 0 directories, 7 files
# | $
# | $
# | $ cat  k8s/redis-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: redis-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: redis
# |   template:
# |     metadata:
# |       labels:
# |         component: redis
# |     spec:
# |       containers:
# |         - name: redis
# |           image: redis
# |           ports:
# |             - containerPort: 6379
# | 
# | $
# | $
# | $ cat  k8s/redis-clusterIP-service.yaml 
# | apiVersion: v1
# | kind: Service
# | metadata:
# |   name: cluster-ip-service
# | spec:
# |   selector:
# |     component: redis
# |   type: ClusterIP
# |   ports:
# |     - port: 6379
# |       targetPort: 6379$ 
# | $ 
# | $ 
# | $
# | $ kubectl apply -f k8s
# | service/client-cluster-ip-service unchanged
# | deployment.apps/client-deployment unchanged
# | service/cluster-ip-service created
# | deployment.apps/redis-deployment created
# | service/server-cluster-ip-service unchanged
# | deployment.apps/server-deployment unchanged
# | deployment.apps/worker-deployment unchanged
# | $ 
# | $ 
# | $ kubectl get pods
# | NAME                                 READY   STATUS    RESTARTS   AGE
# | client-deployment-5dfb6bf966-kchkt   1/1     Running   0          4h57m
# | client-deployment-5dfb6bf966-pcjvz   1/1     Running   0          4h57m
# | client-deployment-5dfb6bf966-vwfpj   1/1     Running   0          4h57m
# | redis-deployment-778647c78-z8rg4     1/1     Running   0          8s
# | server-deployment-5fdbfcfc99-7zhlr   1/1     Running   3          3h3m
# | server-deployment-5fdbfcfc99-m94dh   1/1     Running   3          3h3m
# | server-deployment-5fdbfcfc99-xqr7x   1/1     Running   3          3h3m
# | worker-deployment-56f6bf5bb8-4vp2z   1/1     Running   2          177m
# | $
# | $ kubectl get deployments 
# | NAME                READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment   3/3     3            3           4h57m
# | redis-deployment    1/1     1            1           15s
# | server-deployment   3/3     3            3           3h3m
# | worker-deployment   1/1     1            1           3h3m
# | $
# | $ kubectl get services
# | NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
# | client-cluster-ip-service   ClusterIP   10.107.236.37    <none>        3000/TCP   4h57m
# | redis-cluster-ip-service    ClusterIP   10.111.199.17    <none>        6379/TCP   21s
# | kubernetes                  ClusterIP   10.96.0.1        <none>        443/TCP    2d6h
# | server-cluster-ip-service   ClusterIP   10.105.124.124   <none>        5000/TCP   3h3m
# | $ 
# | $
# | 
#
# STEP 6: Lets configure the postgresSQL deployment and service as well 
#
# | $ tree k8s/
# | k8s/
# | ├── client-clusterIP-service.yaml
# | ├── client-deployment.yaml
# | ├── postgres-clusterIP-service.yaml
# | ├── postgres-deployment.yaml
# | ├── redis-clusterIP-service.yaml
# | ├── redis-deployment.yaml
# | ├── server-clusterIP-service.yaml
# | ├── server-deployment.yaml
# | └── worker-deployment.yaml
# | 
# | 0 directories, 9 files
# | $ more k8s/postgres-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: postgres-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: postgres
# |   template:
# |     metadata:
# |       labels:
# |         component: postgres
# |     spec:
# |       containers:
# |         - name: postgres
# |           image: postgres
# |           ports:
# |             - containerPort: 5432
# | 
# |     
# | $ 
# | $ more k8s/postgres-clusterIP-service.yaml 
# | apiVersion: v1
# | kind: Service
# | metadata:
# |   name: postgres-cluster-ip-service
# | spec:
# |   type: ClusterIP
# |   selector:
# |     component: postgres
# |   ports:
# |     - port: 5432
# |       targetPort: 5432
# | $ 
# | $ kubectl apply -f k8s
# | service/client-cluster-ip-service unchanged
# | deployment.apps/client-deployment unchanged
# | service/postgres-cluster-ip-service unchanged
# | deployment.apps/postgres-deployment created
# | service/redis-cluster-ip-service unchanged
# | deployment.apps/redis-deployment unchanged
# | service/server-cluster-ip-service unchanged
# | deployment.apps/server-deployment unchanged
# | deployment.apps/worker-deployment unchanged
# | $ 
# | $ 
# | $ kubectl get deployments 
# | NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment     3/3     3            3           5h21m
# | postgres-deployment   0/1     1            0           11s
# | redis-deployment      1/1     1            1           24m
# | server-deployment     3/3     3            3           3h27m
# | worker-deployment     1/1     1            1           3h27m
# | $ kubectl get pods
# | NAME                                   READY   STATUS             RESTARTS   AGE
# | client-deployment-5dfb6bf966-kchkt     1/1     Running            0          5h21m
# | client-deployment-5dfb6bf966-pcjvz     1/1     Running            0          5h21m
# | client-deployment-5dfb6bf966-vwfpj     1/1     Running            0          5h21m
# | postgres-deployment-7b6ddcfdbc-lrpcm   0/1     CrashLoopBackOff   1          15s		==> Details of this error given below 
# | redis-deployment-778647c78-z8rg4       1/1     Running            0          24m
# | server-deployment-5fdbfcfc99-7zhlr     1/1     Running            3          3h27m
# | server-deployment-5fdbfcfc99-m94dh     1/1     Running            3          3h27m
# | server-deployment-5fdbfcfc99-xqr7x     1/1     Running            3          3h27m
# | worker-deployment-56f6bf5bb8-4vp2z     1/1     Running            3          3h22m
# | $ kubectl get pod^C
# | $ kubectl logs postgres-deployment-7b6ddcfdbc-lrpcm						==> its crashing because of the environment vriables and it will be set later
# | Error: Database is uninitialized and superuser password is not specified.
# |        You must specify POSTGRES_PASSWORD to a non-empty value for the
# |        superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".
# | 
# |        You may also use "POSTGRES_HOST_AUTH_METHOD=trust" to allow all
# |        connections without a password. This is *not* recommended.
# | 
# |        See PostgreSQL documentation about "trust":
# |        https://www.postgresql.org/docs/current/auth-trust.html
# | $ kubectl get services
# | NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
# | client-cluster-ip-service     ClusterIP   10.107.236.37    <none>        3000/TCP   5h21m
# | cluster-ip-service            ClusterIP   10.111.199.17    <none>        6379/TCP   24m
# | kubernetes                    ClusterIP   10.96.0.1        <none>        443/TCP    2d6h
# | postgres-cluster-ip-service   ClusterIP   10.110.58.79     <none>        5432/TCP   3m12s
# | redis-cluster-ip-service      ClusterIP   10.103.246.108   <none>        6379/TCP   19m
# | server-cluster-ip-service     ClusterIP   10.105.124.124   <none>        5000/TCP   3h28m
# | $ 
#
# 
#  STEP 7 : Configure PersistantVolumeClaim and update the postgressql deployment congiguration to access mount the same volume 
#
# | {bash}-$ tree k8s/
# | k8s/
# | ├── client-clusterIP-service.yaml
# | ├── client-deployment.yaml
# | ├── database-persistent-volume-claim.yaml		=> new file created
# | ├── postgres-clusterIP-service.yaml
# | ├── postgres-deployment.yaml
# | ├── redis-clusterIP-service.yaml
# | ├── redis-deployment.yaml
# | ├── server-clusterIP-service.yaml
# | ├── server-deployment.yaml
# | └── worker-deployment.yaml
# | 
# | 0 directories, 10 files
#
#
# | {bash}-$ more k8s/database-persistent-volume-claim.yaml 
# | apiversion: v1
# | kind: PersistentVolumeClaim
# | metadata: 
# |   name : database-persistent-volume-claim
# | spec:
# |   accessModes:
# |     - ReadWriteOnce
# |   resources:
# |     requests:
# |       storage: 2Gi
# | {bash}-$ 
#
#
# | {bash}-$ more k8s/postgres-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: postgres-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: postgres
# |   template:
# |     metadata:
# |       labels:
# |         component: postgres
# |     spec:
# |       volumes:							=> Defining the volume under spec:template 
# |         - name: postgres-storage			
# |           persistentVolumeClaim:
# |             claimName: database-persistent-volume-claim		=> referring to the 'PersistentVolumeClaim' created
# |       containers:
# |         - name: postgres
# |           image: postgres
# |           ports:
# |             - containerPort: 5432
# |           volumeMounts:						=> Defining volumeMounts under the Container referring back to Volumes session for volume name 
# |             - name: postgres-storage				=> Make sure the name you give the name as exatly same as the name mentioned in Volumes, then only it gets picked for mounting
# |               mountPath: /var/lib/postgresql/data			=> Volume will get mounted at this path (make sure this directory exist in container)
# |               subPath: postgres					=> This setting is specific to Postgres, for a generic setup we can ignore this 
# | 
# |     
# | {bash}-$ 
#
#
# | {bash}-$ kubectl apply -f k8s
# | service/client-cluster-ip-service unchanged
# | deployment.apps/client-deployment unchanged
# | persistentvolumeclaim/database-persistent-volume-claim created
# | service/postgres-cluster-ip-service unchanged
# | deployment.apps/postgres-deployment configured
# | service/redis-cluster-ip-service unchanged
# | deployment.apps/redis-deployment unchanged
# | service/server-cluster-ip-service unchanged
# | deployment.apps/server-deployment unchanged
# | deployment.apps/worker-deployment unchanged
# | {bash}-$ 
# | {bash}-$ 
# | {bash}-$ kubectl get pods
# | NAME                                   READY   STATUS             RESTARTS   AGE
# | client-deployment-5dfb6bf966-kchkt     1/1     Running            0          7h57m
# | client-deployment-5dfb6bf966-pcjvz     1/1     Running            0          7h57m
# | client-deployment-5dfb6bf966-vwfpj     1/1     Running            0          7h57m
# | postgres-deployment-5bd6cd4b4-j2pqg    0/1     Error              0          33s
# | postgres-deployment-7b6ddcfdbc-lrpcm   0/1     CrashLoopBackOff   34         156m		==> Ignore this for now
# | redis-deployment-778647c78-z8rg4       1/1     Running            0          3h
# | server-deployment-5fdbfcfc99-7zhlr     1/1     Running            6          6h3m
# | server-deployment-5fdbfcfc99-m94dh     1/1     Running            6          6h3m
# | server-deployment-5fdbfcfc99-xqr7x     1/1     Running            6          6h3m
# | worker-deployment-56f6bf5bb8-4vp2z     1/1     Running            5          5h58m
# | {bash}-$
# | {bash}-$ kubectl get PersistentVolumeClaim
# | NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
# | database-persistent-volume-claim   Bound    pvc-1ec10e26-32c4-4851-8af5-5a96973e68b6   2Gi        RWO            standard       2m11s
# | {bash}-$ 
# | {bash}-$ kubectl describe PersistentVolumeClaim
# | Name:          database-persistent-volume-claim
# | Namespace:     default
# | StorageClass:  standard
# | Status:        Bound
# | Volume:        pvc-1ec10e26-32c4-4851-8af5-5a96973e68b6
# | Labels:        <none>
# | Annotations:   control-plane.alpha.kubernetes.io/leader:
# |                  {"holderIdentity":"e87f68c3-998d-11ea-99d1-080027eff1fa","leaseDurationSeconds":15,"acquireTime":"2020-05-21T14:29:44Z","renewTime":"2020-...
# |                pv.kubernetes.io/bind-completed: yes
# |                pv.kubernetes.io/bound-by-controller: yes
# |                volume.beta.kubernetes.io/storage-provisioner: k8s.io/minikube-hostpath
# | Finalizers:    [kubernetes.io/pvc-protection]
# | Capacity:      2Gi
# | Access Modes:  RWO
# | VolumeMode:    Filesystem
# | Mounted By:    postgres-deployment-5bd6cd4b4-j2pqg
# | Events:
# |   Type    Reason                 Age                    From                                                           Message
# |   ----    ------                 ----                   ----                                                           -------
# |   Normal  ExternalProvisioning   2m24s (x3 over 2m24s)  persistentvolume-controller                                    waiting for a volume to be created, either by external provisioner "k8s.io/minikube-hostpath" or manually created by system administrator
# |   Normal  Provisioning           2m24s                  k8s.io/minikube-hostpath e87f68c3-998d-11ea-99d1-080027eff1fa  External provisioner is provisioning volume for claim "default/database-persistent-volume-claim"
# |   Normal  ProvisioningSucceeded  2m24s                  k8s.io/minikube-hostpath e87f68c3-998d-11ea-99d1-080027eff1fa  Successfully provisioned volume pvc-1ec10e26-32c4-4851-8af5-5a96973e68b6
# | {bash}-$ 
# | {bash}-$ kubectl get PersistentVolume
# | NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                      STORAGECLASS   REASON   AGE
# | pvc-1ec10e26-32c4-4851-8af5-5a96973e68b6   2Gi        RWO            Delete           Bound    default/database-persistent-volume-claim   standard                2m30s
# | {bash}-$ 
# | {bash}-$ kubectl describe PersistentVolume
# | Name:            pvc-1ec10e26-32c4-4851-8af5-5a96973e68b6
# | Labels:          <none>
# | Annotations:     hostPathProvisionerIdentity: e87f688f-998d-11ea-99d1-080027eff1fa
# |                  pv.kubernetes.io/provisioned-by: k8s.io/minikube-hostpath
# | Finalizers:      [kubernetes.io/pv-protection]
# | StorageClass:    standard
# | Status:          Bound
# | Claim:           default/database-persistent-volume-claim
# | Reclaim Policy:  Delete
# | Access Modes:    RWO
# | VolumeMode:      Filesystem
# | Capacity:        2Gi
# | Node Affinity:   <none>
# | Message:         
# | Source:
# |     Type:          HostPath (bare host directory volume)
# |     Path:          /tmp/hostpath-provisioner/pvc-1ec10e26-32c4-4851-8af5-5a96973e68b6
# |     HostPathType:  
# | Events:            <none>
# | {bash}-$ 
# | 
# | 
#
# STEP 8 : Configure Environment for worker-deploymet and environment & postgres secret for server-deployment and postgres-deployment 
#
# | {bash}-$ tree k8s/
# | k8s/
# | ├── client-clusterIP-service.yaml
# | ├── client-deployment.yaml
# | ├── database-persistent-volume-claim.yaml
# | ├── postgres-clusterIP-service.yaml
# | ├── postgres-deployment.yaml
# | ├── redis-clusterIP-service.yaml
# | ├── redis-deployment.yaml
# | ├── server-clusterIP-service.yaml
# | ├── server-deployment.yaml
# | └── worker-deployment.yaml
# | 
# | 0 directories, 10 files
#
#
# | {bash}-$ more k8s/worker-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: worker-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: worker
# |   template:
# |     metadata:
# |       labels:
# |         component: worker
# |     spec:
# |       containers:
# |         - name: worker
# |           image: stephengrider/multi-worker
# |           env:
# |             - name: REDIS_HOST			=> Configuring environments which referred inside the container
# |               value: redis-cluster-ip-service	=> Value fetched from the redis service config 
# |             - name: REDIS_PORT
# |               value: "6379"				=> static value 
# | {bash}-$
#
#
# | {bash}-$ more k8s/server-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: server-deployment
# | spec:
# |   replicas: 3
# |   selector:
# |     matchLabels:
# |       component: server
# |   template:
# |     metadata:
# |       labels:
# |         component: server
# |     spec:
# |       containers:
# |         - name: server
# |           image: stephengrider/multi-server
# |           ports:
# |             - containerPort: 5000
# |           env:
# |             - name: REDIS_HOST
# |               value: redis-cluster-ip-service
# |             - name: REDIS_PORT
# |               value: "6379"
# |             - name: PGUSER
# |               value: "postgres"
# |             - name: PGHOST
# |               value: postgres-cluster-ip-service
# |             - name: PGPOPT
# |               value: "5432"
# |             - name: PGDATABASE
# |               value: "postgres"
# |             - name: PGPASSWORD			==> PGPASSWORD which gets referred inside the container getting value from secret created 
# |               valueFrom:
# |                 secretKeyRef:			
# |                   name: pgpassword			==> secret crteaed 
# |                   key: PGPASSWORD			==> key from the secret
# | 
# | {bash}-$ 
#
#
# | {bash}-$ cat k8s/postgres-deployment.yaml 
# | apiVersion: apps/v1
# | kind: Deployment
# | metadata:
# |   name: postgres-deployment
# | spec:
# |   replicas: 1
# |   selector:
# |     matchLabels:
# |       component: postgres
# |   template:
# |     metadata:
# |       labels:
# |         component: postgres
# |     spec:
# |       volumes:
# |         - name: postgres-storage
# |           persistentVolumeClaim:
# |             claimName: database-persistent-volume-claim
# |       containers:
# |         - name: postgres
# |           image: postgres
# |           ports:
# |             - containerPort: 5432
# |           volumeMounts:
# |             - name: postgres-storage
# |               mountPath: /var/lib/postgresql/data
# |               subPath: postgres
# |           env:
# |             - name: POSTGRES_PASSWORD 	==> PGPASSWORD which gets referred inside the container getting value from secret created
# |               valueFrom:
# |                 secretKeyRef:
# |                   name: pgpassword		==> secret crteaed
# |                   key: PGPASSWORD		==> key from the secret
# | 
# | 
# | {bash}-$ 
#
#
# | {bash}-$ kubectl create secret generic pgpassword --from-literal PGPASSWORD=postgres123		==> Creating the secret
# | secret/pgpassword created
# | {bash}-$ kubectl get secret pgpassword
# | NAME         TYPE     DATA   AGE
# | pgpassword   Opaque   1      16s
# | {bash}-$ kubectl describe secret pgpassword
# | Name:         pgpassword
# | Namespace:    default
# | Labels:       <none>
# | Annotations:  <none>
# | 
# | Type:  Opaque
# | 
# | Data
# | ====
# | PGPASSWORD:  11 bytes		==> This env is referred in earlier config files
# | {bash}-$
#
#
#
# | {bash}-$ kubectl apply -f k8s
# | service/client-cluster-ip-service unchanged
# | deployment.apps/client-deployment unchanged
# | persistentvolumeclaim/database-persistent-volume-claim unchanged
# | service/postgres-cluster-ip-service unchanged
# | deployment.apps/postgres-deployment configured
# | service/redis-cluster-ip-service unchanged
# | deployment.apps/redis-deployment unchanged
# | service/server-cluster-ip-service unchanged
# | deployment.apps/server-deployment configured
# | deployment.apps/worker-deployment configured
# | {bash}-$ 
# | 
#
#
# | {bash}-$ kubectl get deployments 
# | NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
# | client-deployment     3/3     3            3           24h
# | postgres-deployment   1/1     1            1           19h
# | redis-deployment      1/1     1            1           19h
# | server-deployment     3/3     3            3           22h
# | worker-deployment     1/1     1            1           22h
# | {bash}-$ kubectl get pods
# | NAME                                   READY   STATUS    RESTARTS   AGE
# | client-deployment-5dfb6bf966-kchkt     1/1     Running   0          24h
# | client-deployment-5dfb6bf966-pcjvz     1/1     Running   0          24h
# | client-deployment-5dfb6bf966-vwfpj     1/1     Running   0          24h
# | postgres-deployment-6fb8d99dc4-xglmv   1/1     Running   0          2m10s
# | redis-deployment-778647c78-z8rg4       1/1     Running   0          19h
# | server-deployment-8587b49759-4crx9     1/1     Running   0          4m17s
# | server-deployment-8587b49759-b9dvv     1/1     Running   0          4m8s
# | server-deployment-8587b49759-f275d     1/1     Running   0          4m13s
# | worker-deployment-6585996f47-ncc97     1/1     Running   0          42m
# | {bash}-$ kubectl get services
# | NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
# | client-cluster-ip-service     ClusterIP   10.107.236.37    <none>        3000/TCP   24h
# | cluster-ip-service            ClusterIP   10.111.199.17    <none>        6379/TCP   19h
# | kubernetes                    ClusterIP   10.96.0.1        <none>        443/TCP    3d1h
# | postgres-cluster-ip-service   ClusterIP   10.110.58.79     <none>        5432/TCP   19h		==> This env is referred in earlier config files
# | redis-cluster-ip-service      ClusterIP   10.103.246.108   <none>        6379/TCP   19h		==> This env is referred in earlier config files
# | server-cluster-ip-service     ClusterIP   10.105.124.124   <none>        5000/TCP   22h
# | {bash}-$ 
# | 
#
# STEp 9 : Configure an Ingress to access the client and server url from outside the cluter using IP. 
#
# | {bash}-$ tree k8s/
# | k8s/
# | ├── client-clusterIP-service.yaml
# | ├── client-deployment.yaml
# | ├── database-persistent-volume-claim.yaml
# | ├── ingress-service.yaml
# | ├── postgres-clusterIP-service.yaml
# | ├── postgres-deployment.yaml
# | ├── redis-clusterIP-service.yaml
# | ├── redis-deployment.yaml
# | ├── server-clusterIP-service.yaml
# | ├── server-deployment.yaml
# | └── worker-deployment.yaml
# | 
# | 0 directories, 11 files
# | {bash}-$ more k8s/ingress-service.yaml 
# | apiVersion: extensions/v1beta1
# | kind: Ingress
# | metadata:
# |   name: ingress-service				=> name of the Ingress
# |   annotations:
# |     kunernetes.io/ingress.class: nginix		=> Using nginx Ingress
# |     nginx.ingress.kubernetes.io/rewrite-target: /	=> rewrite engien rule for nginx
# | spec:
# |   rules:
# |     - http:
# |         paths:
# |           - path: /					=> path where client-service-ip will be redirected with below Cluster service IP and Port
# |             backend:
# |               serviceName: client-cluster-ip-service
# |               servicePort: 3000
# |           - path: /api/				=> Path wheere server-cliuster-ip will be redirected with below service cluster IP service and Port 
# |             backend:
# |               serviceName: server-cluster-ip-service
# |               servicePort: 5000
# | {bash}-$ 
# | {bash}-$ 
# | {bash}-$ kubectl apply -f k8s
# | service/client-cluster-ip-service unchanged
# | deployment.apps/client-deployment unchanged
# | persistentvolumeclaim/database-persistent-volume-claim unchanged
# | ingress.extensions/ingress-service created
# | service/postgres-cluster-ip-service unchanged
# | deployment.apps/postgres-deployment unchanged
# | service/redis-cluster-ip-service unchanged
# | deployment.apps/redis-deployment unchanged
# | service/server-cluster-ip-service unchanged
# | deployment.apps/server-deployment unchanged
# | deployment.apps/worker-deployment unchanged
# | {bash}-$ 
# | {bash}-$ minikube ip
# | 192.168.99.100
# | {bash}-$ curl -k http://192.168.99.100
# | <!doctype html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name="theme-color" content="#000000"><link rel="manifest" 
href="/manifest.json"><link rel="shortcut icon" href="/favicon.ico"><title>React App</title><link href="/static/css/main.c17080f1.css" rel="stylesheet"></head><body><noscript>You need to enable JavaScript to run
 this app.</noscript><div id="root"></div><script type="text/javascript" src="/static/js/main.a2449523.js"></script></body></html>{bash}-$ 
# | {bash}-$ curl -k http://192.168.99.100/api/
# | Hi
# | {bash}-$ 
# | {bash}-$ 
# | 
#
#
#
#
#
