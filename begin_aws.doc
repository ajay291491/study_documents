#-------------------------------------------------------------------------------------------------------------------
#		Chapter 01 - Introduction and Key terms
#-------------------------------------------------------------------------------------------------------------------
#
# * What is an Availability Zone 
# An availability zone is a one or more than one data centers in one city or place. Together an availbity zone zone make sure the availability of services for that zone
# Examples : us-east-1a
#
# * What is a Region
# Region is a group of availability zones, this might me with respect to one country or a state. 
# Examples : Singapore, Sydney, Ohio 
#
# * Edge Location 
# Its a content delivery network (CDN), its a caching point from where users can download their data locally. 
# For example, if someone from Singapore trying to access a file location USA, then the edge location within Singapore will help to provide the caching content. 
#
# * Scope of AWS certified solution architect 
# - Compute 
# - Storage 
# - Database
# - Network and Content Delivery
# - Security Identity & Compliance 
#
# * Installing AWS CLI
# Install aws cli using your favorite package and then configure the AWS cli using the keys
#
# | root@sathsang-Predator-G3620:/study_docs/git_backup/study_documents# aws configure
# | AWS Access Key ID [****************BOLC]:
# | AWS Secret Access Key [****************NvGD]:
# | Default region name [us-east-1]:
# | Default output format [json]:
# | root@sathsang-Predator-G3620:/study_docs/git_backup/study_documents#
#
# Once you have configured your cli, then you can run commands
#
# |
# | oot@sathsang-Predator# aws s3 ls
# | 2019-08-08 13:45:12 elasticbeanstalk-us-east-2-298337959615
# | root@sathsang-Predator#
# |
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 02 - Identity and Access Management (IAM) 101 
#-------------------------------------------------------------------------------------------------------------------
# * What is IAM 
# IAM will help you to administer you accounts and groups in AWS. This is very important in managing AWS.
# It has mainly few components as below, 
#
# - User	: This is an end user
# - Group	: COllection of users
# - Policy	: These are set of rules which defines what level of access and actions can be performed, these are represented in Json format
# - Roles 	: Roles are created mainly for AWS resources to give permission to access other resources for exampe, role for EC2 to access S3
#
# Note : IAM supports PCI compliance which is a payment card industry framework
#      : AWS IAM roles are held at the global level
#
# * What is a root account 
# Root account is the account you have used to signed up into the AWS console, genrally this will be an email address. 
#
# * What is AWS console 
# This is single place where you can access all the AWS service. 
# My custom console : https://ajay291491.signin.aws.amazon.com/console 
#
# * Creating an user 
# Login to Console  -> Security, Identity and Compliance -> IAM -> Users -> Add User
# . Provide Username 
# . Provide group details 
# . Assign policies to the group 
# . Submit 
# . Download the CSV if needed. 
#
# * What is access key id and access secret
# Access key ID and secret are used to access an user account programatically, they are never used to login to the console.
# You can setup that by going to 
#
# Login to Console  -> Security, Identity and Compliance -> IAM -> Users -> Select user -> Select security Credentials 
#
# * What is Roles
# Roles are used to grant permission for one AWS resource to another.
# For example an EC2 resource want access to an S3 storage then its going to be perimitted via roles
# Roles will always have some attached policies which will in fact provide the actual permission to the resource when it gets allocated.
# Login to Console  -> Security, Identity and Compliance -> IAM -> Users -> Select Roles -> Create Roles 
#
# * Billing Alarm 
# Billing alarm can be useful to get notification during the usage. 
# There are various different options in billing available, to enable billing you will need to do folowing 
#
# Login to Console -> Select 'My Account' -> ' My billing Dashboard' -> Billing Preferences -> Receive Billing Alerts 
#
# Once you have enabled billing preference, then you will need to go to "Billing Dashboard" in could watch to setup your alerts 
#
# Login to Console -> Services -> Cloud Watch -> Bills -> Create alarm
#
#  * What is a policy
# Policy is something which tells whether a user has a permission to a resource or not, Policies are normally defined in the JSON language.
# Once the policies are defined and published then they can be attached to groups or users to grant defined permission to the end users.
# Policies are mainly defined into two different catagories they are
#
# - Managed policies  : These policies are managed and defined by Amazon
# - Inline Policy     : These are on-off policies defined for specific or custom purpose which can attached to users or groups
#
# Example : Below is an example for a policy
#
# |{
# |  "Version": "2012-10-17",
# |  "Statement": [
# |    {
# |      "Action": [                    --> Type of action in the policy
# |        "ec2:Describe*",
# |        "ec2:StartInstances",
# |        "ec2:StopInstances"
# |      ],
# |      "Resource": "*",
# |      "Effect": "Allow"              --> Action can be allow or denied
# |    },
# |    {
# |      "Action": "elasticloadbalancing:Describe*",
# |      "Resource": "*",
# |      "Effect": "Allow"
# |    },
# |    {
# |      "Action": [
# |        "cloudwatch:ListMetrics",
# |        "cloudwatch:GetMetricStatistics",
# |        "cloudwatch:Describe*"
# |      ],
# |      "Resource": "*",
# |      "Effect": "Allow"
# |    },
# |    {
# |      "Action": "autoscaling:Describe*",
# |      "Resource": "*",
# |      "Effect": "Allow"
# |    }
# |  ]
# |}
#
# * What is ARN Name in AWS
# ARN is the amazon resource name which helps to identify the resource uniquely in AWS.
# Every resource in AWS has ARN name and its has a standard format
#
# Syntax : arn:partition:service:region:account-id:resource-id
#          arn:partition:service:region:account-id:resource-type/resource-id
#          arn:partition:service:region:account-id:resource-type:resource-id
#
# Examples : Below are examples for ARN
#
# | arn:aws:iam::123456789012:user/Development/product_1234/*
# | Resource":"arn:aws:iam::123456789012:user/*
# | Resource":"arn:aws:iam::123456789012:group/*
# | arn:aws:s3:::my_corporate_bucket/*
# | arn:aws:s3:::my_corporate_bucket/Development/*
#
# NOTE : https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 03 -  Simple Storege Services (S3)
#-------------------------------------------------------------------------------------------------------------------
# 
# * What is S3
# S3 is one of the oldest AWS service available and it is an object storage which helps you store files and data. 
# S3 is an object storage an it cannot  be used for installing operating systems for that purpose you need look for another block storage. 
# S3 comes in various different flavours, according to the cost and requirements
#
# * What is Object storage means for S3
# S3 or Simple storage service is called as an object storage which allows you to store files
# The reason it is called as an object is storage is because it consist of
#
# Key          : This represents the name of the object 
# Value        : This is simply the data and its made up of sequance of bytes on the disk)
# version ID   : This is helpful for versioning the data and helpful for identifying the version and its copies 
# MetaData     : Its the data about the data you are storing
# Subresources : Access control list of S3 
#
# * Key Fatures about S3 
# - S3 is an object storage which help you to store only the files and data 
# - S3 bucket name should an unique name, because based on that name there will be a web url getting created to access the files. 
# - Bucket is the top level folder name in the S3 storage 
# - When you upload a file it will give a HTTP code 200 to mean its successful
# - S3 enables version controlling 
# - Its provides metadata (data about the data stored) to control the version control
# - S3 provides 99.99% availability and 99.99999999999% durability 
# - S3 provides different tiers in the storage 
# - S3 provides encryption 
# - You can store data upto 5 TB
# - This is an unlimited storage 
# - bucket namespace should be universal 
# - You can use the ACl to control the resource 
# 
# * What is S3 Lifecycle management 
# This allows you to perform certain action based on certain life cycle, couple of examples are 
# - Perform archive to S3 glacier after 30 days 
# - Move data to another S3 after 90 days etc 
#
# * Reading and writing to S3 
# - When you create a object it will be available immediately 
# - When you PUT of UPDATE an obeject in S3, it will take couple of seconds to get it updated 
#
# * S3 - Different Tiers
# There are different tiers of storage available in S3
#
# - S3 : Standard 
#   . It offers 99.99% availability 
#   . It offers 99.99999999999% durability (9 9s after .)
#   . It is stored redudantly accross multiple devices in multiple fascilities. 
#   . It can sustain the loss of 2 fascilities together 
#
# - S3 : Standard IA
#   . This is designed for accessing the highly redudant data which gets accessed in-frequently
#   . But this gets charges for everytime when you access the data 
#   . Cost lower than S3 Standard and charges apply for retrival
#
# - S3 : One zone IA
#   . Compare to Stadard 1A this is limited to only one availablity zones 
#   . Data is less redudant compare to the above 
#   . Cost is less than above two as your redudancy is limited to only one availability zone
#
# - S3 : Intelligent tiering 
#   . This is a new feature which uses Machine learning 
#   . This automatically map your data to the right tier according to the access of the data 
#
# Summary : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=526#bookmarks
#
# * S3 - Glacier (data archival solutions)
# S3 Glacier is a cheaper and reliable storage service which you can use to storage the data which you want to keep it for a long time. 
# You can say your legal documents which need to access only times when needed can be put in here. 
# There are two types of S3 glacier available. 
#
# - S3 : Glacier
#   . Secure, durable low cost solution for data archival 
#   . You can store any amount of data which will be cheaper than on-premisis data 
#   . retrival time can be configurable from minutes to hours 
#
# - S3 : Glacier Deep Archive 
#   . This is the cheapest available 
#   . Data retrival can take upto 12 hours 
#
# S3 Table : https://www.udemy.com/course/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=602#bookmarks
#
# Summary  : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=576#bookmarks
#          : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=600#bookmarks
#
# Exam Tips : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=600#bookmarks
#
# * What is S3 Transfer accelaration 
# AWS S3 transfer accelation service enables customers to transfer files faster, safer and secure. 
# This takes advantages of the amazon's CloudFront Edge locations to accelrate the transfer. 
# This comes into effect mostly when you perform a cross region file replication or file transfer, i.e you are trying to upload file to US-East1 bucket from Sydney region
# When a data arrives at the Edge location then it gets transferred to main S3 bucket via the amazon backbone network. 
# This way customers will be able to get the transfered faster and securely. 
#
# * Pros and Cons of S3 
# 
# Pros
# - S3 is an object storage which allows to upload files 
# - File size can be from 0 to 5 TB
# - There is unlimited storage 
# - S3 uses universal namespacing, i.e when you create an S3 bucket you will need to create a unique bucket name 
# - You can enable MFA while deleting the object from S3 
# - While uploading a file you will get HTTP status code 200
#
# Cons 
# - You cannot install operating system on the S3 bucket 
# - You cannot install database on the S3 bucket 
#
# NOTE : S3 is a major portion of the exam and You need to reas 'S3 Faq' for more details . 
#
# * S3 Security 
# When you create a S3 bucket it gets created as private and if you want to manage the permission on S3, it offers offers two levels of security 
# - Bucket policy       : This manages the security at the bucket level 
# - Access control list : This manages the security at the object level 
#
# * S3 Encryption
# S3 offers two encryption methods 
# - Encryption in transit : This is the process of encrypting the data using an https while the data is travelling between 
# - Encryption at REST    : This is the process of encrypting the data it self, this can be managed in thre ways 
#   Encryption done by S3 : S3 managed keys SSE-S3 (Server side encryption by S3) 
#   Encryption done using cuctsomer provided keys  : SSE-C (Server side encryption using client provided keys)
#   Encryption done using the AWS keys : SSE-KMS (Serverside encryption using Amazon Key Management service)
#
# * Versioning in S3
# Versioning is a powerful tool in S3 which helps to perform version controlling as well as it act as a backup tool. 
# When you have a file which can change historically and at any point of time there could be a chance that you might need to restore to older version, then in that case enabling versioning will be good. 
# When you Enable versioning then there will be more storage gets used since all versions of the files are getting stored as versions. 
# When you delte a object completely from S3, then make sure you are deleting all versionins of that object removed. 
#
# * S3 Public and Private 
# When you create a bucket it will be by default in private state and if you want to enable Public access to a object then you will need to go to console and set the permission level to Public. 
# Also when you have versioning enabled and when you upload the new version of the object permission will be revoked and you will need to set the Public permission again for that object.
#
# * S3 Lifecycle management 
# Lifecycle management is used to transition an S3 bucket to another storage tier after certain time. 
# If you want to move you object from S3 standard to some other storage tier after certain time then lifecycle management helps. 
#
# Example : Below is an example for S3 Lifecycle management 
#
# | # aws s3api get-bucket-lifecycle-configuration --bucket ajay291testbucket
# | {
# |     "Rules": [
# |         {
# |             "Expiration": {
# |                 "Days": 365
# |             },
# |             "ID": "ajay291lifecycle",
# |             "Filter": {
# |                 "Prefix": ""
# |             },
# |             "Status": "Enabled",
# |             "Transitions": [
# |                 {
# |                     "Days": 20,
# |                     "StorageClass": "GLACIER"	=> This is for the current versions 
# |                 }
# |             ],
# |             "NoncurrentVersionTransitions": [
# |                 {
# |                     "NoncurrentDays": 7,
# |                     "StorageClass": "DEEP_ARCHIVE"  => This is for the previous versions 
# |                 }
# |             ],
# |             "NoncurrentVersionExpiration": {
# |                 "NoncurrentDays": 365
# |             },
# |             "AbortIncompleteMultipartUpload": {
# |                 "DaysAfterInitiation": 7
# |             }
# |         }
# |     ]
# | }
# | #
# |
#
# * S3 - Cross region replication
# Cross region replication is used to replicate the data from one region to another region in aws.
# There are few keys things we need to note when we are looking at cross region replication.
#
# - Versioning must be enabled for the bucket before we enable replication 
# - Region must be unique for source and desination buckets 
# - You can create replication for the same account or into another account 
# - You will need to create a role which is required for the replication and it will be associated to buckets 
# - Only the new objects getting created only after the point from where the replication enabled will be replicated
# - If you mark a delete marker on any of the object then that will not get replicated to destination bucket 
#
# Example : Below example shows us about the cross region replication in S3
#
# aws s3api get-bucket-replication --bucket ajay291s3bucket 
# | {
# |     "ReplicationConfiguration": {
# |         "Role": "arn:aws:iam::304832273789:role/service-role/s3crr_role_for_ajay291s3bucket_to_ajay291s3bucketreplication",
# |         "Rules": [
# |             {
# |                 "ID": "ajay291replicationrole",
# |                 "Priority": 1,
# |                 "Filter": {},
# |                 "Status": "Enabled",
# |                 "Destination": {
# |                     "Bucket": "arn:aws:s3:::ajay291s3bucketreplication"	=> Destination bucket
# |                 },
# |                 "DeleteMarkerReplication": {
# |                     "Status": "Disabled"
# |                 }
# |             }
# |         ]
# |     }
# |  }
# | #
#
# * S3 - Transfer accelaration 
# S3 transfer accelaration is a feature provided to upload the files more faster from multiple regions across the world. 
# To perform tranfer acceleration amazon make use of the AWS cloud Front Edge network, which is the backbone for amazon.
# To upload via cloud front edge network you will get a distinct url which will help to upload to edge network and from there cloud front will transfer your data to the bucket. 
# We can use below URL to test the S3 transfer accelaration for various locations. 
#
# URL : https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html
#
# * S3 - Cloud Front 
# Cloud front is a content delivery network which used to cache media contents like videos, files, photos etc. 
# Below are the key terminilogies we need to be aware of cloud front 
#
# - Edge Location : This is the location where content will be cached and this seperate from AWS region or avalability zone 
# - origin        : This is the origin of all the files that will be distributed by the CDN, this can be a S3 bucket  or EC2 instance or ELB or Route 53
# - Distribution  : This is the name given to the CDN which consist of collection of edge location 
#
# Note : key usage of using Cloud front is enable faster access for web and media contents through the edge location by providing caching when multiple users accesing it from same region 
#  Web distribution : This is used for web contents 
#  RTMP : used for media contents 
#  TTL  : Objects are configured live until the TTL, it  is basically the caching time
#
# NOTE : You can clear the cached contents, but it is chargable  
#
# * AWS Snowball
# Snowwall is a petabyte capable data tranferring devices which comes in two different flavors
# - 80 TB
# - 50 TB
# Using snowball adresses few of the most common challenges such as
# - Ease the large scale data transfer including the network costs 
# - Long transfer times and security concerns 
# - Transferring data with snowball is fast, simple and secure 
# - It provides a tamper resistant enclosure 
# - Provides a industry standard Trusted Platform Module (TPM) designed to encure the security and chain of custody
#
# * AWS Snowball Edge 
# Snowwall Edge is 50TB storage transfer as well as compute capable device which helps you to have a device onboard to transfer data as well as compute capabilities. 
# This can run Lamda functions to process the data which it has. One of the major airline uses snowball Edge onboard to provide its capablities for testing. 
#
# * AWS Snowmobile 
# Snowmobile is a technology which used by AWS to move Exabytes of data which is a large shipping container docked on a Truck
# It can transfer 100 PB per snowmobile. 
#
# * Exam Tips for Snowmobile 
# What is snow mobile : 
# It can export and import data from and to S3
#
#-------------------------------------------------------------------------------------------------------------------
# Chapter 05 -  Database
#-------------------------------------------------------------------------------------------------------------------
# Amazon has various different promises when it comes to database, based on the customer needs they can request for right solution 
# It also provides a fascility of read replicas which can allow you to read the database from a seperate node from write, which can be used for high read intense applications
# Major type of database available are 
#
# * RDS (Relation Database system)
# These are database running on virtual machines, however customer won't be having access to RDS. 
# Any patching and maintanance of RDS is taken care by Amazon itself. 
# Below are ket RDS promises available 
# - MySql 
# - SQL 
# - Oracle 
# - PostgresQL
# - MariaDB
#
# * Aurora 
# This is a relational database system which provided by Amazon and it runs on a serverless architecture. 
#
# * Dynamo DB (NoSQL)
# This ia a no SQL Db which provided by AWS. 
# This will help you to store multi dimensional data outside the traditional SQL Db which relies only on rown and columns model 
#
# * Elastic Cache 
# These are the caching technologies available in AWS, there are mainly two of them 
# - Memcahe  : This can be used for small application require less amount of caching 
# - Redis    : This can be used for caching on larger data for long hours 
#
# * Redshift
# This is a data warehousing technology provided by Amazon. 
# This is used for business intelligence and available only on one availability zone. 
# This can have a copy of the backup on the S3 storage 
#
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 06 - Route 53 (DNS)
#-------------------------------------------------------------------------------------------------------------------
#
# In AWS DNS and loadbalancing technologies are termed as Route 53. 
# There are mainly 6 load balancing technologies available in AWS 
#
# 1. Sample Routing        : This is a round robin based routing and we cannot associate health check with this 
# 2. Weighed Routing       : Here you can specify how much percentage traffic you want to keep this to one IP over another
# 3. Latency Based routing : Here routing will be based on the latency you experience between multiple IPs
# 4. Region based routing  : This will help you to route the traffic based on the region 
# 5. Failover Routing      : This can be used in case for auto failover incase the service is not available for any IPs
# 6. Multi Value Routing   : This is similar to the simple routing with health check 
#
# NOTE : Most of the above routing you can associate with the a health check.
#      : In the health check you can associate a service or port and you can request to keep monitor it for any failure to redirect traffic based on that 
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 07 - VPC
#-------------------------------------------------------------------------------------------------------------------
# VPC is a virtual isolation layer of resources in AWS. If you create an account there will be a default VPC attached with it.
# A VPC mainly has many components with in it which control the resource access. Few of the key components you should be aware are 
#
# - Internet Gateway : Using this component any resource in the VPC can reach from inside or outside to internet
# - Subnet           : This is the subnet which decides network isolation with in a VPC, you can have more than one subnet in VPC
# - NACL (Network Access control list) : This decides the access controls like which IP and port can access resource on what subnets etc.
# - Routing Table    : Similar to Unix routing this decides from where to where the network should be routed
# 
# Below are the few key specialities of VPC 
# - They are highly available 
# - They are highly redudant 
# - Bydefault one Internet Gateway will be attached to VPC, you can have multiple IGW but only one can be attached
# - You cannot delete a VPC if there are resource in that VPC.
#
# * Routing Table
# This does the routing between network IPs which help in determining the source and destination
# Below are few key things you need to note about routing table 
# - When you create a VPC there will be two route table configured by defaul, one "local" which determines local traffic with in VPC
# - Second route will be to the IGW this will dtermine the traffic to the internet gateway
# - By default the route table to internet gateway will be 0.0.0.0/*, which means any resource can access IGW.
#
# * Internetgateway
# IGW is the way for any resource to connect to internet.
# - There can be only one IGW which can be attached to a VPC
# - Routing to the IGW can be done through the route table 
# - IGW has two states which is "attached" and "detached". On a detached state this will communicate with inetrnet through there are routes
#
# * Network ACL
# This is an optional layer which control the access to VPC like a firewall. 
# - There will be 6 subnets created when there will be a VPC gets created.
# - One NACL will act as a firewall for all the 6 subnets associated with the VPC
# - You can have the Inbound rules which tell what external resource can access VPC with what port number
# - You can have outbound rules which tells what source can send traffic via what port
# - NACL always have a number, rules are processed in lowest order, which means lowest ACL rule will override if there are duplicate
# 
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 08 - CloudFormation
#-------------------------------------------------------------------------------------------------------------------
# CloudFormation is an automated way to provision the aws resources in an Infrastructure as a code.
# It has mainly three compnents which helps to drive the whole automation around, they are 
#
# - Templates 
# - Stacks 
# - Changesets  
#
# * Templates 
# Templates are manifest files which contain infromation about how a resource or group of resource need to be provisioned. 
# This can be written either in JSON or YAML syntax. This will have definition about various resources how that needs to be configured. 
# Templates has mainly few sessions and each are described below.
#
# Syntax : Below is a high level syntax of the Templates 
#
# | { 
# |	"AWSTemplateFormatVersion" : "2020-01-07",
# |	"Description" : "Sample Template to Launch an EC2 Instance",
# |	"Parameters" : { # input parameter details goes here and one sample given below 
# |		Parameter_01{
# |			"Description" : "note about the parameter getting passed",
# |			"Type"	: "Valid type"}
# |	}
# |	"mapping"  : { # mapping details goes here }
# | "Resource" : { # resource definitions goes here }
# |	"output"   : { # output you want to show from stack you can mention here }
# | }
# 
# - Parameters 
# Inputs which is getting passed into the template while creating the stack, we can mainly define the type and description about a paramter here.
# There few valid types allowed for parameters they are catagorized into two types they are general and aws parameters.
# 
# . General Parameter types : Below are the few general parameters allowed
#   String : "Resource123"
#   Number : 100
#   List (Number) : [1, 2, 3]
#   CommaDelimitedList : ["t2.nano", "t2.micro", "t2.small"] 
#
# . AWS parameter types : Below are few sample aws parameter types 
#   AWS::EC2::Image::ID				: "ami-i566757"
#   AWS::EC2::Instance::ID			: "i-12hjh876"
# 	AWS::EC2:: SecurityGroup::Id 	: "sg-ashak7c"
# 
# NOTE : Please see url for complete Types : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html
#
# - Mapping 
# The optional Mappings section matches a key to a corresponding set of named values. 
# For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. 
# You use the Fn::FindInMap intrinsic function to retrieve values in a map.
# 
# Example : Below is sample for mapping 
# 
# | "Mappings" : {
# | "RegionMap" : {
# |   "us-east-1"      : { "HVM64" : "ami-0ff8a91507f77f867"},
# |   "us-west-1"      : { "HVM64" : "ami-0bdb828fd58c52235"},
# |   "eu-west-1"      : { "HVM64" : "ami-047bb4163c506cd98"},
# |   "ap-southeast-1" : { "HVM64" : "ami-08569b978cc4dfa10"},
# |   "ap-northeast-1" : { "HVM64" : "ami-06cd52961ce9f0d85"}
# |  }
# | } 
#
# - Resource 
# This is the session on the template where you define the targetted resource and their properties. 
# AWS has almost all aws resource types available for you to define via the cloud formation template and it is documented below.
# URL : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html
# 
# | "Resources" : {
# |   "EC2Instance": {
# |		"Type": "AWS::EC2::Instance"
# |		"Properties" : {
# |			"ImageId" : {"FindInMap": ["RegionMap", {"Ref", "AWS::Region"}, "HVM64"]},
# |			"InstanceType" : "t2.nano", 
# |			"NetworkInterface": [{
# |				"AssociatePublicIpAddress" : "true",
# |				"DeviceIndex" : "0",
# |	 			"GroupSet": [{"Ref": "SecurityGroup"}],
# |				"SubnetId": {"Ref": "Subnet"}
# |				}],
# |		}
# |	}
# 
# - output 
# When you want to provide an output from the stack then you will need to define the corresponding outputs in template. 
# This is an optional parameter in the template and you can define multiple outputs in the template.
# To get the a value assigned to a output we can either use the 'href' function if its a simple and direct method and 'Getatt' for complex lookups.
#
# | "Outputs" : {
# | "BackupLoadBalancerDNSName" : {
# |   "Description": "The DNSName of the backup load balancer",  
# |   "Value" : { "Fn::GetAtt" : [ "BackupLoadBalancer", "DNSName" ]},
# |   "Condition" : "CreateProdResources"
# | },
# | "InstanceID" : {
# |   "Description": "The Instance ID",  
# |   "Value" : { "Ref" : "EC2Instance" }
# |  }
# | }
#
#
# * Stacks 
# When you use AWS CloudFormation, you manage related resources as a single unit called a stack. 
# You create, update, and delete a collection of resources by creating, updating, and deleting stacks. 
# All the resources in a stack are defined by the stack's AWS CloudFormation template. 
# Suppose you created a template that includes an Auto Scaling group, Elastic Load Balancing load balancer, and an Amazon Relational Database Service (Amazon RDS) database instance. 
# To create those resources, you create a stack by submitting the template that you created, and AWS CloudFormation provisions all those resources for you. 
# You can work with stacks by using the AWS CloudFormation console, API, or AWS CLI.
#
# Example : Below are the sample commands you can check to understand about stacks 
#
# | # aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE
# | [
# |     {
# |         "StackId": "arn:aws:cloudformation:us-east-2:123456789012:stack/myteststack/
# | 644df8e0-0dff-11e3-8e2f-5088487c4896",
# |         "TemplateDescription": "AWS CloudFormation Sample Template S3_Bucket: Sample template showing how to create a publicly accessible S3 bucket. **WARNING** This template creates an
# | S3 bucket. You will be billed for the AWS resources used if you create a stack from this template.",
# |         "StackStatusReason": null,
# |         "CreationTime": "2013-08-26T03:27:10.190Z",
# |        "StackName": "myteststack",
# |         "StackStatus": "CREATE_COMPLETE"
# |     }
# | ]
# | #
#
# | # aws cloudformation describe-stacks --stack-name myteststack
# | {
# |     "Stacks":  [
# |         {
# |             "StackId": "arn:aws:cloudformation:us-east-2:123456789012:stack/myteststack/a69442d0-0b8f-11e3-8b8a-500150b352e0",
# |             "Description": "AWS CloudFormation Sample Template S3_Bucket: Sample template showing how to create a publicly accessible S3 bucket. **WARNING** This template creates an S3 bucket.
# | You will be billed for the AWS resources used if you create a stack from this template.",
# |             "Tags": [],
# |             "Outputs": [
# |                 {
# |                     "Description": "Name of S3 bucket to hold website content",
# |                     "OutputKey": "BucketName",
# |                     "OutputValue": "myteststack-s3bucket-jssofi1zie2w"
# |                 }
# |             ],
# |             "StackStatusReason": null,
# |             "CreationTime": "2013-08-23T01:02:15.422Z",
# |             "Capabilities": [],
# |             "StackName": "myteststack",
# |             "StackStatus": "CREATE_COMPLETE",
# |             "DisableRollback": false
# |         }
# |     ]
# | }
# | #
#
#
# * Changesets 
# If you need to make changes to the running resources in a stack, you update the stack. 
# Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. 
# Change sets allow you to see how your changes might impact your running resources, especially for critical resources, before implementing them.
# For example, if you change the name of an Amazon RDS database instance, AWS CloudFormation will create a new database and delete the old one. 
# You will lose the data in the old database unless you've already backed it up. 
# If you generate a change set, you will see that your change will cause your database to be replaced, and you will be able to plan accordingly before you update your stack. 
#
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 08 - Applications 
#-------------------------------------------------------------------------------------------------------------------
#
# * SQS (MQ - Pull based Messaging Queue Service)
# This is a very old service which offered by AWS, this offers a pull based messaging queue system  which helps to decouple your resource from queue. 
# This is a distributed queue system which helps to store messages in the queue until a computer resource is available to process them.
# Messages stored on the queue can be accesses by application using the SQS API service. 
#
# Below are few key features of SQS 
# . SQS offers minimum 1 min to maximum 14 days retention where default retention is for 4 days 
# . SQS offers a pull method to get message, which means the application has to push the message to queue and also another application need to pull the message from queue
# . SQS offers default 256KB of text in any format
# . For larger files it offers a solution via S3 which can store upto 2GB
#
# . What is Visibility Timeout 
# When a message picked up by a server for processing then that message will disappear in the SQS queue for some time since it is getting processed, this is called visibility timeout.
# But with in certain time if that message is not processed then it is expected to comeback and another server will attempt to process the message again. 
# In this situation there are chances that you message might get delivered twice. 
#
# NOTE : The maximum visibilty timeout which can configured is 12 Hours 
#
# There are mainly two types of queue 
# - Standard Queue
#   Offers best effort ordering, generally order in the same order it goes. But the ordering not guaranteed.  
#   This Gurantees the messages will be delivered atleast once. 
#   But the messages can get delivered more than once 
#
# - FIFO Queue
#   This is FIFO, it support first in first out
#
# . What is long polling 
# This is way to save the cost, in a normal short polling an EC2 instance will be constantly polling the service. 
# But when it comes to long polling it will wait for a message to arrive in the queue and then it return the message or long poll times out.
#
# * SWF (Task workflow)
# Amazon SWF is a simple workflow service which provides the capability of creating a series of workflow tasks. 
# This is a web service which makes it easy to coordinate work across various distributed application components. 
# This enables for various different use cases 
# - Including media processing 
# - Web application backends 
# - Business processing workflows 
# - analtical pipeline 
# - also for coordination tasks 
#
# There mainly three core conepts which we need to understand about SWF
# 
# 1. Workflow Starters : This is the component which intiate the workflow, an example could be an Ansible API
# 2. Deciders          : This component keeps track of the status from previous action and it decides what to do next 
# 3. Activity workers  : This carry out the activity tasks
#
# Example : Buying a book from amazon will trigger lot of backend workflows such as checking stock, making payment, human interaction to  physicaly shipping the book, ackowldging the receipt etc 
#
# . What is a task 
# SWF triggers a series of tasks and tasks represent various invocation step in an application step which can be performed by executable code, api calls or by human interaction 
#
# Exam Tips 
#  . SQS workflow execution last upto 1 year 
#  . This offers a task oreinted API 
#  . SWF make sure the task is assigned only once and it will never get duplicated 
#  . SWF keeps track of all tasks and events in an application 
#
#
# * SNS (Simple Notification Service - Push based Messaging)
# This service is used for pushing notification into various different services. 
# This allows developers to push notification to users immedietly for subscribed useres. 
# Any messages delovered to the SNS platform will be stored redudantly in multiple availability zones 
# This allows you to push notification to below platforms. 
# - Apple 
# - Google 
# - Fire OS
# - Windows devices 
# - All android devices
# - Baidu Cloud push 
#
# SNS has mainly three components to work together 
#
# - Publisher  : This is the person or organization or function which publish the topics to the subscribers 
# - Topics     : Its a topic regarding some subject where someone will be scbscribed for the message, like stock market, youtube channel
# - Subscriber : End user email or device or component which subscibe to a topic 
#
# It supports various end points as subscribers such as HTTP, HTTPS , Email, Email-JSON, SQS, AWS LAMBDA, SMS, platform application end point
#
# NOTE : on top of the push notification services SNS can be used to send SMS messages 
#
# * Elastic Transcoder
# Amazon Elastic transcoder are used to transcode low resolution videos to high resolution videos. 
# To make a elastic transcoder work you will require three essential components 
#
# - Input S3 Bucket  : This is where you will keep your source format videos 
# - Pipeline         : This pipeline picks up the video from input S3 bucket and does the processing and Save that to the Output S3 bucket
# - Output S3 Bucket : This is the destination bucket for all the processed video from the Transcoder 
#
#
# * API gateway 
# API Gateway is used for connecting to AWS resource using an API. This can be configured to autoscale and as well as throttle API gateway to prevent attack.
# - API gateway has caching capabilities, which means if multiple users asking for the same GET requests it can serve the content from cache until the TTL expires 
# - TTL values can be configured 
# - API gateway is low coast and scale automatially 
# - You can throttle API gateway to prevent attack 
# - You can log results of API gateway to CloudWatch 
# - In case you are using Javascript or AJAX if you are calling the API gateway, make sure you enable CORS to accept content from multi domains such as s3, ec3 etc 
# - CORS is enforced by client 
#
#
#


