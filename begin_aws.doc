#-------------------------------------------------------------------------------------------------------------------
#		Chapter 01 - Introduction and Key terms
#-------------------------------------------------------------------------------------------------------------------
#
# * What is an Availability Zone 
# An availability zone is a one or more than one data centers in one city or place. Together an availbity zone zone make sure the availability of services for that zone
# Examples : us-east-1a
#
# * What is a Region
# Region is a group of availability zones, this might me with respect to one country or a state. 
# Examples : Singapore, Sydney, Ohio 
#
# * Edge Location 
# Its a content delivery network (CDN), its a caching point from where users can download their data locally. 
# For example, if someone from Singapore trying to access a file location USA, then the edge location within Singapore will help to provide the caching content. 
#
# * Scope of AWS certified solution architect 
# - Compute 
# - Storage 
# - Database
# - Network and Content Delivery
# - Security Identity & Compliance 
#
# * Installing AWS CLI
# Install aws cli using your favorite package and then configure the AWS cli using the keys
#
# | root@sathsang-Predator-G3620:/study_docs/git_backup/study_documents# aws configure
# | AWS Access Key ID [****************BOLC]:
# | AWS Secret Access Key [****************NvGD]:
# | Default region name [us-east-1]:
# | Default output format [json]:
# | root@sathsang-Predator-G3620:/study_docs/git_backup/study_documents#
#
# Once you have configured your cli, then you can run commands
#
# |
# | oot@sathsang-Predator# aws s3 ls
# | 2019-08-08 13:45:12 elasticbeanstalk-us-east-2-298337959615
# | root@sathsang-Predator#
# |
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 02 - Identity and Access Management (IAM) 101 
#-------------------------------------------------------------------------------------------------------------------
# * What is IAM 
# IAM will help you to administer you accounts and groups in AWS. This is very important in managing AWS.
# It has mainly few components as below, 
#
# - User	: This is an end user
# - Group	: COllection of users
# - Policy	: These are set of rules which defines what level of access and actions can be performed, these are represented in Json format
# - Roles 	: Roles are created mainly for AWS resources to give permission to access other resources for exampe, role for EC2 to access S3
#
# Note : IAM supports PCI compliance which is a payment card industry framework
#      : AWS IAM roles are held at the global level
#
# * What is a root account 
# Root account is the account you have used to signed up into the AWS console, genrally this will be an email address. 
#
# * What is AWS console 
# This is single place where you can access all the AWS service. 
# My custom console : https://ajay291491.signin.aws.amazon.com/console 
#
# * Creating an user 
# Login to Console  -> Security, Identity and Compliance -> IAM -> Users -> Add User
# . Provide Username 
# . Provide group details 
# . Assign policies to the group 
# . Submit 
# . Download the CSV if needed. 
#
# * What is access key id and access secret
# Access key ID and secret are used to access an user account programatically, they are never used to login to the console.
# You can setup that by going to 
#
# Login to Console  -> Security, Identity and Compliance -> IAM -> Users -> Select user -> Select security Credentials 
#
# * What is Roles
# Roles are used to grant permission for one AWS resource to another.
# For example an EC2 resource want access to an S3 storage then its going to be perimitted via roles
# Roles will always have some attached policies which will in fact provide the actual permission to the resource when it gets allocated.
# Login to Console  -> Security, Identity and Compliance -> IAM -> Users -> Select Roles -> Create Roles 
#
# * Billing Alarm 
# Billing alarm can be useful to get notification during the usage. 
# There are various different options in billing available, to enable billing you will need to do folowing 
#
# Login to Console -> Select 'My Account' -> ' My billing Dashboard' -> Billing Preferences -> Receive Billing Alerts 
#
# Once you have enabled billing preference, then you will need to go to "Billing Dashboard" in could watch to setup your alerts 
#
# Login to Console -> Services -> Cloud Watch -> Bills -> Create alarm
#
#  * What is a policy
# Policy is something which tells whether a user has a permission to a resource or not, Policies are normally defined in the JSON language.
# Once the policies are defined and published then they can be attached to groups or users to grant defined permission to the end users.
# Policies are mainly defined into two different catagories they are
#
# - Managed policies  : These policies are managed and defined by Amazon
# - Inline Policy     : These are on-off policies defined for specific or custom purpose which can attached to users or groups
#
# Example : Below is an example for a policy
#
# |{
# |  "Version": "2012-10-17",
# |  "Statement": [
# |    {
# |      "Action": [                    --> Type of action in the policy
# |        "ec2:Describe*",
# |        "ec2:StartInstances",
# |        "ec2:StopInstances"
# |      ],
# |      "Resource": "*",
# |      "Effect": "Allow"              --> Action can be allow or denied
# |    },
# |    {
# |      "Action": "elasticloadbalancing:Describe*",
# |      "Resource": "*",
# |      "Effect": "Allow"
# |    },
# |    {
# |      "Action": [
# |        "cloudwatch:ListMetrics",
# |        "cloudwatch:GetMetricStatistics",
# |        "cloudwatch:Describe*"
# |      ],
# |      "Resource": "*",
# |      "Effect": "Allow"
# |    },
# |    {
# |      "Action": "autoscaling:Describe*",
# |      "Resource": "*",
# |      "Effect": "Allow"
# |    }
# |  ]
# |}
#
# * What is ARN Name in AWS
# ARN is the amazon resource name which helps to identify the resource uniquely in AWS.
# Every resource in AWS has ARN name and its has a standard format
#
# Syntax : arn:partition:service:region:account-id:resource-id
#          arn:partition:service:region:account-id:resource-type/resource-id
#          arn:partition:service:region:account-id:resource-type:resource-id
#
# Examples : Below are examples for ARN
#
# | arn:aws:iam::123456789012:user/Development/product_1234/*
# | Resource":"arn:aws:iam::123456789012:user/*
# | Resource":"arn:aws:iam::123456789012:group/*
# | arn:aws:s3:::my_corporate_bucket/*
# | arn:aws:s3:::my_corporate_bucket/Development/*
#
# NOTE : https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 03 -  Simple Storege Services (S3)
#-------------------------------------------------------------------------------------------------------------------
# 
# * What is S3
# S3 is one of the oldest AWS service available and it is an object storage which helps you store files and data. 
# S3 is an object storage an it cannot  be used for installing operating systems for that purpose you need look for another block storage. 
# S3 comes in various different flavours, according to the cost and requirements
#
# * What is Object storage means for S3
# S3 or Simple storage service is called as an object storage which allows you to store files
# The reason it is called as an object is storage is because it consist of
#
# Key          : This represents the name of the object 
# Value        : This is simply the data and its made up of sequance of bytes on the disk)
# version ID   : This is helpful for versioning the data and helpful for identifying the version and its copies 
# MetaData     : Its the data about the data you are storing
# Subresources : Access control list of S3 
#
# * Key Fatures about S3 
# - S3 is an object storage which help you to store only the files and data 
# - S3 bucket name should an unique name, because based on that name there will be a web url getting created to access the files. 
# - Bucket is the top level folder name in the S3 storage 
# - When you upload a file it will give a HTTP code 200 to mean its successful
# - S3 enables version controlling 
# - Its provides metadata (data about the data stored) to control the version control
# - S3 provides 99.99% availability and 99.99999999999% durability 
# - S3 provides different tiers in the storage 
# - S3 provides encryption 
# - You can store data upto 5 TB
# - This is an unlimited storage 
# - bucket namespace should be universal 
# - You can use the ACl to control the resource 
# 
# * What is S3 Lifecycle management 
# This allows you to perform certain action based on certain life cycle, couple of examples are 
# - Perform archive to S3 glacier after 30 days 
# - Move data to another S3 after 90 days etc 
#
# * Reading and writing to S3 
# - When you create a object it will be available immediately 
# - When you PUT of UPDATE an obeject in S3, it will take couple of seconds to get it updated 
#
# * S3 - Different Tiers
# There are different tiers of storage available in S3
#
# - S3 : Standard 
#   . It offers 99.99% availability 
#   . It offers 99.99999999999% durability (9 9s after .)
#   . It is stored redudantly accross multiple devices in multiple fascilities. 
#   . It can sustain the loss of 2 fascilities together 
#
# - S3 : Standard IA
#   . This is designed for accessing the highly redudant data which gets accessed in-frequently
#   . But this gets charges for everytime when you access the data 
#   . Cost lower than S3 Standard and charges apply for retrival
#
# - S3 : One zone IA
#   . Compare to Stadard 1A this is limited to only one availablity zones 
#   . Data is less redudant compare to the above 
#   . Cost is less than above two as your redudancy is limited to only one availability zone
#
# - S3 : Intelligent tiering 
#   . This is a new feature which uses Machine learning 
#   . This automatically map your data to the right tier according to the access of the data 
#
# Summary : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=526#bookmarks
#
# * S3 - Glacier (data archival solutions)
# S3 Glacier is a cheaper and reliable storage service which you can use to storage the data which you want to keep it for a long time. 
# You can say your legal documents which need to access only times when needed can be put in here. 
# There are two types of S3 glacier available. 
#
# - S3 : Glacier
#   . Secure, durable low cost solution for data archival 
#   . You can store any amount of data which will be cheaper than on-premisis data 
#   . retrival time can be configurable from minutes to hours 
#
# - S3 : Glacier Deep Archive 
#   . This is the cheapest available 
#   . Data retrival can take upto 12 hours 
#
# S3 Table : https://www.udemy.com/course/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=602#bookmarks
#
# Summary  : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=576#bookmarks
#          : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=600#bookmarks
#
# Exam Tips : https://www.udemy.com/aws-certified-solutions-architect-associate/learn/lecture/13886254?start=600#bookmarks
#
# * What is S3 Transfer accelaration 
# AWS S3 transfer accelation service enables customers to transfer files faster, safer and secure. 
# This takes advantages of the amazon's CloudFront Edge locations to accelrate the transfer. 
# This comes into effect mostly when you perform a cross region file replication or file transfer, i.e you are trying to upload file to US-East1 bucket from Sydney region
# When a data arrives at the Edge location then it gets transferred to main S3 bucket via the amazon backbone network. 
# This way customers will be able to get the transfered faster and securely. 
#
# * Pros and Cons of S3 
# 
# Pros
# - S3 is an object storage which allows to upload files 
# - File size can be from 0 to 5 TB
# - There is unlimited storage 
# - S3 uses universal namespacing, i.e when you create an S3 bucket you will need to create a unique bucket name 
# - You can enable MFA while deleting the object from S3 
# - While uploading a file you will get HTTP status code 200
#
# Cons 
# - You cannot install operating system on the S3 bucket 
# - You cannot install database on the S3 bucket 
#
# NOTE : S3 is a major portion of the exam and You need to reas 'S3 Faq' for more details . 
#
# * S3 Security 
# When you create a S3 bucket it gets created as private and if you want to manage the permission on S3, it offers offers two levels of security 
# - Bucket policy       : This manages the security at the bucket level 
# - Access control list : This manages the security at the object level 
# - IAM Roles           : You can also use IAM roles to manage (allow/ deny) api calls someone can make to a bucket
#
# * S3 Encryption
# S3 offers two encryption methods 
# - Encryption in transit : This is the process of encrypting the data using an https while the data is travelling between 
# - Encryption at REST    : This is the process of encrypting the data it self, this can be managed in thre ways 
#   Encryption done by S3 : S3 managed keys SSE-S3 (Server side encryption by S3) 
#   Encryption done using cuctsomer provided keys  : SSE-C (Server side encryption using client provided keys)
#   Encryption done using the AWS keys : SSE-KMS (Serverside encryption using Amazon Key Management service)
#
# * Versioning in S3
# Versioning is a powerful tool in S3 which helps to perform version controlling as well as it act as a backup tool. 
# When you have a file which can change historically and at any point of time there could be a chance that you might need to restore to older version, then in that case enabling versioning will be good. 
# When you Enable versioning then there will be more storage gets used since all versions of the files are getting stored as versions. 
# When you delte a object completely from S3, then make sure you are deleting all versionins of that object removed. 
#
# * S3 Public and Private 
# When you create a bucket it will be by default in private state and if you want to enable Public access to a object then you will need to go to console and set the permission level to Public. 
# Also when you have versioning enabled and when you upload the new version of the object permission will be revoked and you will need to set the Public permission again for that object.
#
# * S3 Lifecycle management 
# Lifecycle management is used to transition an S3 bucket to another storage tier after certain time. 
# If you want to move you object from S3 standard to some other storage tier after certain time then lifecycle management helps. 
#
# Example : Below is an example for S3 Lifecycle management 
#
# | # aws s3api get-bucket-lifecycle-configuration --bucket ajay291testbucket
# | {
# |     "Rules": [
# |         {
# |             "Expiration": {
# |                 "Days": 365
# |             },
# |             "ID": "ajay291lifecycle",
# |             "Filter": {
# |                 "Prefix": ""
# |             },
# |             "Status": "Enabled",
# |             "Transitions": [
# |                 {
# |                     "Days": 20,
# |                     "StorageClass": "GLACIER"	=> This is for the current versions 
# |                 }
# |             ],
# |             "NoncurrentVersionTransitions": [
# |                 {
# |                     "NoncurrentDays": 7,
# |                     "StorageClass": "DEEP_ARCHIVE"  => This is for the previous versions 
# |                 }
# |             ],
# |             "NoncurrentVersionExpiration": {
# |                 "NoncurrentDays": 365
# |             },
# |             "AbortIncompleteMultipartUpload": {
# |                 "DaysAfterInitiation": 7
# |             }
# |         }
# |     ]
# | }
# | #
# |
#
# * S3 - Cross region replication
# Cross region replication is used to replicate the data from one region to another region in aws.
# There are few keys things we need to note when we are looking at cross region replication.
#
# - Versioning must be enabled for the bucket before we enable replication 
# - Region must be unique for source and desination buckets 
# - You can create replication for the same account or into another account 
# - You will need to create a role which is required for the replication and it will be associated to buckets 
# - Only the new objects getting created only after the point from where the replication enabled will be replicated
# - If you mark a delete marker on any of the object then that will not get replicated to destination bucket 
#
# Example : Below example shows us about the cross region replication in S3
#
# aws s3api get-bucket-replication --bucket ajay291s3bucket 
# | {
# |     "ReplicationConfiguration": {
# |         "Role": "arn:aws:iam::account_no:role/service-role/s3crr_role_for_ajay291s3bucket_to_ajay291s3bucketreplication",
# |         "Rules": [
# |             {
# |                 "ID": "ajay291replicationrole",
# |                 "Priority": 1,
# |                 "Filter": {},
# |                 "Status": "Enabled",
# |                 "Destination": {
# |                     "Bucket": "arn:aws:s3:::ajay291s3bucketreplication"	=> Destination bucket
# |                 },
# |                 "DeleteMarkerReplication": {
# |                     "Status": "Disabled"
# |                 }
# |             }
# |         ]
# |     }
# |  }
# | #
#
# * S3 - Transfer accelaration 
# S3 transfer accelaration is a feature provided to upload the files more faster from multiple regions across the world. 
# To perform tranfer acceleration amazon make use of the AWS cloud Front Edge network, which is the backbone for amazon.
# To upload via cloud front edge network you will get a distinct url which will help to upload to edge network and from there cloud front will transfer your data to the bucket. 
# We can use below URL to test the S3 transfer accelaration for various locations. 
#
# URL : https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html
#
# * S3 - Cloud Front 
# Cloud front is a content delivery network which used to cache media contents like videos, files, photos etc. 
# Below are the key terminilogies we need to be aware of cloud front 
#
# - Edge Location : This is the location where content will be cached and this seperate from AWS region or avalability zone 
# - origin        : This is the origin of all the files that will be distributed by the CDN, this can be a S3 bucket  or EC2 instance or ELB or Route 53
# - Distribution  : This is the name given to the CDN which consist of collection of edge location 
#
# Note : key usage of using Cloud front is enable faster access for web and media contents through the edge location by providing caching when multiple users accesing it from same region 
#  Web distribution : This is used for web contents 
#  RTMP : used for media contents 
#  TTL  : Objects are configured live until the TTL, it  is basically the caching time
#
# NOTE : You can clear the cached contents, but it is chargable  
#
# * AWS Snowball
# Snowwall is a petabyte capable data tranferring devices which comes in two different flavors
# - 80 TB
# - 50 TB
# Using snowball adresses few of the most common challenges such as
# - Ease the large scale data transfer including the network costs 
# - Long transfer times and security concerns 
# - Transferring data with snowball is fast, simple and secure 
# - It provides a tamper resistant enclosure 
# - Provides a industry standard Trusted Platform Module (TPM) designed to encure the security and chain of custody
#
# * AWS Snowball Edge 
# Snowwall Edge is 50TB storage transfer as well as compute capable device which helps you to have a device onboard to transfer data as well as compute capabilities. 
# This can run Lamda functions to process the data which it has. One of the major airline uses snowball Edge onboard to provide its capablities for testing. 
#
# * AWS Snowmobile 
# Snowmobile is a technology which used by AWS to move Exabytes of data which is a large shipping container docked on a Truck
# It can transfer 100 PB per snowmobile. 
#
# * Exam Tips for Snowmobile 
# What is snow mobile : 
# It can export and import data from and to S3
#
#-------------------------------------------------------------------------------------------------------------------
# Chapter 04 - Compute 
#-------------------------------------------------------------------------------------------------------------------
# 
# * What is EC2
# EC2 is Amazon's compute power which help you to boot server a few minutes with the capacity and flexibility you needed for the pricing. 
# Based on the Billing there are mainly few types of instances available in AWS 
#
# . On Demand EC2 Instance 
#   These instances are spin up on demand as requested and can be terminated when needed, Billing will be Hourly or per second basis.
#   But the pricing will be fixed and there won't be any changes to the standard pricing and you will pay as you go without any commitment.
#   This will be a idle solution for developer to test their code and shutdown when they do not need it. 
#
# . Reserved Instance
#   These instances will be on reservation for the capacity you request for. 
#   They offer a significant amount of discount on hourly pricing for the instances.
#   Reserved instances offered based on on 1 year or 3 year contract terms 
#
# . Spot Instances 
#   These kind of instances allow you to bid for instances on the price  you want. 
#   This is a way amazon sells their reserved unused capacity instances. 
#   This will even provide greater savings for you, but the instances can be terminates if the demand and prices goes up than you bid for. 
#   IMPORTANT : If an instance gets terminated in the middle of an hour by amazon then you will not get chanrged, but if thats gets terminated by you then you will get charged. 
#
# . Dedicated Instances 
#   These are dedictaed instances which can be used similar to a physical server 
#   These instances can be used if you have licence or regulatory requirement to build a decidated servers. 
#
# Based on the capacity and performance EC2 isntances are catagorized into several different catagories which can be given below. 
#
# https://aws.amazon.com/ec2/instance-types/
#
# * Key things to note about EC2
# Here are few things you will need to remember when you are dealing with EC2
#
# . Public and Private keys
#   When you create the EC2 instance there will be a provision to create a Key pair once you generate the keys then you will be able to download the private key from the amazon. 
#   You will need to set the private key permission to '400' in order the keys to be working
#   Public key for that will remain the EC2 instance and that will get validated during the authentication. 
#   If you are using a Putty to connect to EC2 instance, then you will need to convert your '.pem' file into '.ppk' file using the puttygen utility, else it will not be able to connect  
#
# . Termination Protection 
#   If you want to protect your instance against any accidental termination then you can enable termination protection. 
#   Termination protection will not allow to terminate the instance until you go ahead and disable that option. 
#
# . HDD & Encryption 
#   You can enable encryption to all the Hard disk devices in EC2 including the Root device 
#   By default the root disk device will be named as '/dev/xvda' 
#   
# . Delete on Termination 
#   If you have enabled delete on termination then the EBS volume won't be getting deleted when the instance is getting terminated 
#   By default root voalume '/dev/xvda' will get terminated when the instance is getting terminated 
#
# * Security groups 
# Security groups are virtual firewalls associated with the instances created in the VPC. 
# This can help you to allow inbout traffic and outbound traffic on specific ports from specific or all IPs. 
# Below are few features to remember about the ecurity group
#
# - By default Security will deny traffic from all IPs and Ports 
# - You can add IPs and allowed ports or range of ports in Security group 
# - When you add an inbound rule it will add similar outbound rule also for the instance
# - Security groups are called stateful
# - You can have multiple security groups on the same instance 
# - You can have multiple instances attached to a security group
# - You cannot sepcifically block any IP or Ports on Security group, but by default all ports and IPs and blocked except the one you have allowed. 
#
# * EBS - Elastic Block Storage 
# EBS are the virtual hard disks in AWS. EBS is mainly devided into two catagories 
# 
# 1. Solid State Drive (SSD)
#    
#    - General Purpose SSD  (GP2)         : Capacity from 1TB - 16TB, IOPS = 16000
#    - Provisioned IOPS SSD (io1)         : Capacity from 4TB - 16TB, IOPS = 64000
#
# 2. Hard Disk Drives (HDD)
#
#   - Throughput Optimized HDD (st1)      : Capacity from 500G - 16T, IOPS = 500
#   - Cold HDD                 (sc1)      : Capacity from 500G - 16T, IOPS = 250
#   - EBS Magnatic             (standard) : Capacity from 1G   - 1T,  IOPS = 40 - 200
#
# . Key details about EBS
#   - Each EBS volume can be attached to an instance and not more than that
#   - An instanced can be attached with multiple EBS instance based on the allowed capacity for the account. 
#   - In case need more capacity than the allocated EBS storage you will need to submit a support case for that 
#   - EBS volume created will be specific to that availability zones and it will be only available to use in the instance from the same availbility zone 
#
# * EBS Snapshots
#   Snapshots are the point in time copy for the volume which will be stored on the S3 storage. 
#   In order to make a snaphot available in another region you will need to copy the snapshot to another region 
#   Snapshot can be used to restore an existing volume or to create a new volume of out of it. 
#   Snaphost can be taken from both root volume and additional volumes.
#
# . Deleting EBS and EC2 Instances
#   When you delete an EC2 instance then the associated root volume will be deleted automatically 
#   Additional volumes attached to the Instance will be still available after the delete, that you can manually delete if you want 
#
# . Resizing the Volume 
#   Resizing of the volume is supported but filesystem level changes for the volume need to be done from the Operating system level for the current running instances. 
#
# . Creating AMI for another region 
#   To create an AMI you can take the snapshot from the root volume and then it be promoted to an AMI.
#   Then copy the AMI to the other region 
#   Then create instance in that region from the same AMI which is just provisioned  
#
# . Creating Volume for another region 
#   To create volume on another region you can follow the steps to create a snapshot first 
#   Then Copy the snapshot to another region and then create a volume out of it. 
#
# * Instant Store Volumes 
# Instant store volumes are temporary storage volumes which can be enabled for caching, buffer, scratching data or any other temporary content data
# Instant store volumes must not be used for any of the long term storage requirement since it can be lost at anytime 
# Instant store volumes are backed by an underlying host but, in case of an unavailbilty of the underlying host you will loose all the data in the volume. 
# Instant store volumes backed instanced cannot be stop or start, but you can restart them but data will remain even after the restart. 
# But if you terminate the instance then the data will be lost and that cannot be retrieved, but in case of EBS you have an option to retain the volume 
#
# * EBS Encryption 
# EBS root and non root volumes can be encrypted and AWS does the does the encryption and stores the keys in AWS KMS
# Its uses KMS to store the Customer master keys while creating the volume. EBS ueses AES-256 algorithm to encrypt the volume 
# Encryption will be working on below areas.
# - All data at rest in the volume 
# - All data moving between volume and instance
# - All snapshot taken from the volume 
# - All volumes created from the snapshot 
#
# . How to Encrypt a volume which is not encrypted 
#   You can create encrypted snapshot from the volume which is currently not encrypted and then create a volume from that snapshot.
#   Once you create a encrypted snapshot you won't be able to create a Unencrypted volume from that snapshot 
#   Also you cannot share a encrypted snapshot between AWS accounts 
#
# * Amazon CloudWatch 
# Amazon CloudWatch helps to monitor the performance and metrics from almost all the amazon resources. 
# Cloud Watch will normally monitor the metrics such as cpu usage, network throughput, disk IO etc 
# Cloudwatch monitor the events every 5 minutes and it be customized to 1 minute
# You can create cloudwatch alarm to trigger notifications 
#
# NOTE : memory monitoring is not default enabled 
# Standard Monitoring : 5 Mins
# Detailed Monitoring : 1 Mins 
# Can Create - Dashboards, alerts, notfication 
# Logging    - You also also send logs to Cloudwatch logs 
#
# * Amazon CloudTrial 
# Cloud Trial stands for audit related monitoring which helps us to understand who did what action. 
# This is veryhelpful in understanding on why, who and what did exactly on what time. 
#
# * Userdata and metadata 
# user data shows the details about the boot strap details 
# Metadata will show the details about the Ec2 instance metadata 
# To get metadatta or userdata from an instanve you can run below commands 
#
# user-data : http://169.254.169.254/latest/user-data 
# meta-data : http://169.254.169.254/latest/meta-data 
#
# * EFS - Elastic File System 
# Its a kind of NFS file system within AWS which supports NFSv4 protocol
# Supports upto 1000 of concurrent NFS connections 
# Can scale upto petabytes 
# Data stored in Multiple AZ within the region
# Read after write consistency 
#
# * EC2 Placement Groups 
# There are mainly three types of placement groups available for EC2 based on how they are placed. 
#
# 1. Clustered Placement group 
#    This type of instances are placed together closly within an AZ. 
#    This can be used for low latency application such as high freequency trading 
#
# 2. Spread Placement group 
#    Striclty places small group of instances placed in an distint underlying hardware to reduce correlated failures 
#    Can provision in multiple AZ, Spread placement groups have a specific limitation that you can only have a maximum of 7 running instances per Availability Zone
#
# 3. Partitioned Placement Groups
#    Spreads your logical group of instances in multiple logical parittions of hardware without sharing hardware for each instance groups 
#    This is ideal for high workload application such as kafka, hadoop, Cassandra 
#    Can provision in multiple AZ
#
#
#-------------------------------------------------------------------------------------------------------------------
# Chapter 05 -  Database
#-------------------------------------------------------------------------------------------------------------------
# Amazon has various different promises when it comes to database, based on the customer needs they can request for right solution 
# It also provides a fascility of read replicas which can allow you to read the database from a seperate node from write, which can be used for high read intense applications
# Major type of database available are 
#
# * RDS (Relation Database system)
# These are database running on virtual machines, however customer won't be having access to RDS. 
# Any patching and maintanance of RDS is taken care by Amazon itself. 
# Below are the RDS promises available 
# - MySql 
# - SQL 
# - Oracle 
# - PostgresQL
# - MariaDB
# - Aurora : This is a relational database system which provided by Amazon and it runs on a serverless architecture. Aurora currently support MySQL and PostgreSql databases. 
#
# * RDS Key things to Know 
#
# . How RDS databases are managed. 
#   RDS databases are running on an EC2 instance but those instances are managed by AWS, which means any maintenance on the underlying RDS infra is managed by Amazon. 
#   But there is an exception to Aurora which is server less technology. 
#
# . RDS backups : There are mainly two types of backups are available in RDS
#
#   1. Automated backups 
#      Automated backups are done automatically and which has a retention period from 1 - 35 days. 
#      Automated backups are stored in an S3 storage and it will be free. 
#      AWS will take full backups and transaction logs as part of the automated backups 
#      When DB gets restored it will be restored to the latest available daily backup and apply transaction logs relevant to that day
#      Automated backups are triggered on the given window and IOPS will be suspended during the DB backups 
#
#   2. Manual backups 
#      Manual backups are manual snapshot copies of the database and they are stored even after you have delete the database. 
#      In case there isn't any manual backup available for the database which is getting deleted then automated backups will also get deleted
#
# . RDS database restore : When you restore a RDS database then it is going to be a new RDS instance with new DNS FQDN. You cannot restore to the original database back.
#
# . RDS Encryption 
#   Encryption is supported in AWS RDS and it does encryption at rest using AWS KMS service. 
#   It support encryption in Oracle, MS SQL, mySQL, PostGreSQL, MariaDB and Aurora 
#   Once the RDS service is encrypted it will encrypt the DB, underlying storage, automated backups, manual snapshots and read replicas, 
#
# . RDS Storage autoscaling : This feature will help to increase the storage size automatically if the specified storage space is reached. 
#
# * RDS Multi-AZ
# RDS multi AZ provides replication for your primary database. 
# When you Enable multi AZ there will be replica which gets created in another availability zone in the same region
# replication will be taken care by AWS and it will be done on a synchrnous basis 
# In the event of a disaster or failure of the primary DB, amazon will switch the DNS to the replica and DB will be still available
#
# * RDS read replica 
# Read replicas allow you to have the read only copies of the RDS database and this primarily stands for performance not for DR.
# This will be useful when it comes to cases of high read intense application. Read replicas syncs in an Asynchrouns replication mode. 
# Read Replicas are supported by below DBs 
# - Oracle 
# - MySql
# - PostgreSQL
# - Aurora 
# - MariaDB
#
# Note : MS SQL is not supported for read replicas 
#
# Few keys things to note about RDS are below 
# - For read replicas to work automatic backup needs to be turned on 
# - Maximim 5 copies of Read replica are possible 
# - We can create read replica of read replica, but need to watch out for latency 
# - Read replicas are supported in multi region as well, so that you can have high read only application can be accesses for multi regions 
# - Read replicas has its own seperate DNS endpoint other than the Primari DB
# - Read replica can  be promoted to primary DB, but that will break the replication 
#
# - Read replicas can be failed over to primary by rebooting it 
# - Encryption can be enabled on read replias by AWS KMS
# 
#
# * Dynamo DB (NoSQL)
# This is a no-SQL database service for application need consistant and millisecond latency at any scale. 
# It is fully managed and support both documentation and key-value data models. 
# Its flexible data model and reliable performamce makes it reliable database for mobile, gaming and IOT applications 
# This will help you to store multi dimensional data outside the traditional SQL DB which relies only on rown and columns model 
#
# - Data is stored in underlying SSD storage which improves the performance 
# - Spread across 3 geographically distinct data centers 
#
# . Read Modes in DynamoDB
#   1. Eventual Consistant Reads : If you want to read the data with in 1 sec after writing to DynamoDB and its replicas, you can eventually consistent mode 
#   2. Strongly consistant Reads : If you app doesn't even wait for 1 sec after writing and upating to all copies, then you will need to use Strongly consistant mode
#
# * Redshift 
# Redshift is amazon's data warehousing or business intellegence solution. 
# This works in a node concept where it can have a single node or multi node configuration 
# - Single node : has a capacity of 160 GB
# - MUlti Node  : This consist of one Leader node and multiple compute node 
#               : Leader node will receive the data from multiple nodes and queries 
#               : Compute node will process the requsts, maximum of 128 compute nodes can be provisioned
#
# . Few features 
#  - It is currently available in only one availability zone. 
#  - Redhsift always maintain 3 copies of data
#  	- Original Copy of the data 
#  	- Replica copy of the data 
#  	- Backup stoared in S3
#  - Redshift provides a backup retention of 35 days 
#  - Redshift also support asynchrouns data replication of S3 in another region 
#  - Redshift support encryption at rest using AES-256 encryption and also support encryption in rest using SSL
#
# * Aurora 
# Aurora is a fully managed database provided by amazon which provides the capabilities of both traditional mysql and postgresql database along with capabilities of new open source DBs. 
# Aurora is 5 x times faster than traditional MySQL database
# Aurora is 3 X times faster than the traditional PostGreSql DBs 
# It maintains 2 copies of data one availability zone and data will be always replicated in 3 availability zones, so total 6 copes of data will be available 
# You can start from 10GB and it can scale upto maximum of 64 TB. 
# It also supports replication over multiple regions in case your application require geographically available read replicas. 
# Incase you have unpredicted workloads then you can use the AWS Aurora serverless which can start, stop and scale according to application demands. 
#
#
# * Elastic Cache 
# These are the caching technologies available in AWS, there are mainly two of them 
# - Memcahe  : This can be used for small application require less amount of caching 
# - Redis    : This can be used for caching on larger data for long hours 
#
# Adding Elasticache will help you to improve the performance of your web application and database 
# Redis support Multi A-Z 
# You can backup and restore redis 
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 06 - Route 53 (DNS)
#-------------------------------------------------------------------------------------------------------------------
#
# In AWS DNS technology are termed as Route 53. 
# Using AWS console's Route 53 you can register for a DNS domain and it can take from couple of hours to maximum 3 days for it to complete the registration. 
# There is a limit of 50 domain names you can own in route 53, but it can increased by contacting AWS support 
#
# * What is Route 53 Health checks
# Health checks are a way to test your underlying IP associated with a DNS record is currently available. 
# health check can be configured under route 53 session with following parameters 
# - IP Address & Hostname 
# - Port number 
# - Protocol 
# - Path (optional, i.e if running web server use the index.html as the path to verify the health)
#
# NOTE : If a IP address fails on health check that will be removed from Route 53 until it passes the health checks 
#      : You can also set SNS notification alert in case any health check is failed. 
#
# * Routing Policies 
# There are mainly 6 routing policies are available in AWS 
#
# 1. Simple Routing        
#    In Simple routing you will be associate one or multiple IPs to a DNS record. 
#    This will do a round robin based routing and we cannot associate health check with this 
#
# 2. Weighed Routing       
#    Here you can specify how much percentage traffic you want to keep this to one IP over another
#    For Example you can say 30% to one IP, 35% to another IP and another 35% to another IP etc 
#    You can associate health check with weighed routing so that in case any of the IP is unavailable DNS will make sure it will route traffic to another one available 
#
# 3. Latency Based routing 
#    Here routing will be based on the latency experienced between the enduser and destination server 
#    Normally this should route you to the closest regional server until and unless if there isn't any internet based latency 
#    You can associate health check with Latency based routing as well.
#
# 4. Geolocation based routing
#    This will help you to route the traffic based on the region. 
#    This means using this you can route your users from APAC to one server where APAC spcific contents are delivered and users from other region to other network.
#
# 5. Failover Routing      
#    This can be used in case for auto failover incase the service is not available for any IPs
#    In case of failover routing you will require a primary where all your primary traffic should flow to, also a secondary IP where trafic should failover to 
#    You will need to associate health check with Primary and secondary to make sure route 53 is monitoring the health of th eprimary IP
#
# 6. Multi Value Routing 
#    This is similar to the simple routing with health check 
#
# NOTE : Most of the above routing you can associate with the a health check.
#      : In the health check you can associate a service or port and you can request to keep monitor it for any failure to redirect traffic based on that 
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 07 - VPC
#-------------------------------------------------------------------------------------------------------------------
#
#* What is VPC 
# VPC is a virtual isolation layer of resources in AWS. If you create an account there will be a default VPC attached with it.
# You can also create custom VPC apart from the default VPC which is getting created. 
# A VPC mainly has many components with in it which control the resource access. Few of the key components you should be aware are 
#
# * VPC Peering 
# - You can connect one VPC to another VPC by enabling proper routing 
# - You can connect between VPC in same account as well as in different account 
# - Once Enable VPC peering instances in two different VPC will behave like they are in same account
# - Peering is always start like config, which means if you enable peering for one VPC to another multiple only that VPC which connect across all can communicate 
# - Transitive peering is not allowed which means if there are VPC want to communicate via a proxy VPC its not allwed, VPC need to peer each other to communicate 
#
# * CIDR Blocks and reserved IPS 
# When you create a VPC you will be able to create CIDR block for that VPC, highest range allowed is x.x.x.x/16 (65,536 IPs) and lowest allowed is x.x.x.x/28 (16 IPs)
# From these CIDR blocks you can create smaller subnet as required for your use. 
# When you create a VPC there will be five IPs will be reserved by amazon for its own backend and reserved purpose, they are 
# . x.x.x.0   -> Subnet ID 
# . x.x.x.1   -> VPC Router Address
# . x.x.x.2   -> DNS address 
# . x.x.x.3   -> reserved for future 
# . x.x.x.255 -> Boradcast address 
#
# * Key Components of VPC 
# - Internet Gateway        : Using this component any resource in the VPC can reach from inside or outside to internet
# - Virtual Private gateway : VGW will help you to connect to a VPN gateway 
# - Subnet                  : This is the subnet which decides network isolation with in a VPC, you can have more than one subnet in VPC
# - NACL (Network Access control list) : This decides the access controls like which IP and port can access resource on what subnets etc.
# - Routing Table           : Similar to Unix routing this decides from where to where the network should be routed
# 
# Below are the few key specialities of VPC 
# - They are highly available 
# - They are highly redudant 
# - Bydefault one Internet Gateway will be attached to VPC, you can have multiple IGW but only one can be attached
# - You cannot delete a VPC if there are resource in that VPC.
#
# * Routing Table
# This does the routing between network IPs which help in determining the source and destination
# Below are few key things you need to note about routing table 
# - When you create a VPC there will be two route table configured by defaul, one "local" which determines local traffic with in VPC
# - Second route will be to the IGW this will dtermine the traffic to the internet gateway
# - By default the route table to internet gateway will be 0.0.0.0/*, which means any resource can access IGW.
#
# * Internetgateway
# IGW is the way for any resource to connect to internet.
# - There can be only one IGW which can be attached to a VPC
# - Routing to the IGW can be done through the route table 
# - IGW has two states which is "attached" and "detached". On a detached state this will communicate with inetrnet through there are routes
#
# * Network ACL
# This is an optional layer which control the access to VPC like a firewall. 
# - There will be 6 subnets created when there will be a VPC gets created.
# - One NACL will act as a firewall for all the 6 subnets associated with the VPC
# - You can have the Inbound rules which tell what external resource can access VPC with what port number
# - You can have outbound rules which tells what source can send traffic via what port
# - NACL always have a number, rules are processed in lowest order, which means lowest ACL rule will override if there are duplicate in higher order 
# - Also if any specific DENY or ALLOW rule you want to include, make sure you are including them in the lower number so that blanket level rules won't override them 
#
# . Few Key things to remember about NACL
#   - You VPC automatically comes with a NACL and by default all traffic will be allowed
#   - YOu can create custom NACL, but by default that will DENY all traffics 
#   - You can associate subnets with NACL, as soon as you associate the subnet all NACL rules will be applied to the subnets 
#   - You can block IP address only using NACL not Security groups 
#   - You can only add port number to NACL, not service name. Once you add port number then assciated service name automatically will appear in NACL list 
#   - You can associate only one NACL to one subnet, but a NACL can have multiple subnets
#   - NACL are stateless, which means you will need to add inbound and outbound rules seperately 
#
# * VPC Flow Logs 
# VPC Flow logs are similar to tcpdump in Linux which helps to capture the VPC traffic. 
# TO create VPC Flologs you will need to dirst create the CloudWatch Log group, you can also log Flow Logs to S3 bucket 
# Once you have the flow logs created then you can create the IAM roles for Flow logs 
# Once that is done you can create the flow logs. 
#
# . Below are few key things to remember about VPC Flowlogs 
#  - All Internal DNS related traffic is not logged 
#  - YOu cannot tag Flow logs at the moment 
#  - Traffic related to 169.25.169.254 are not logged 
#  - DHCP trafic is not logged 
#  - IP reservation traffic are not logged 
#
# * Direct Connect 
# Direct Connect is a way of connecting between Amazon and Customer datacenter or office or colocation 
# This enables a dedicated line between amazon and customer DCs which will help in achieving high throughput as well as low cost
# To Achive this there will direct connect cage pair will be established, one pair from AWS end and another one from Customer end 
# Once both devices are connected and started communicating then the direct connect will be established 
# 
# * VPC Lab - Working with Public Ec2 and Private EC2 instances 
#
# STEP 1 : You will need to create a custom VPC which has a CIDR block range of 10.0.0.0/16
#
# | root@sathsang-Predator-G3620:/data/amazon# aws ec2 describe-vpcs --vpc-id vpc-0c5e5696e3d771ab5 --profile default
# | {
# |     "Vpcs": [
# |         {
# |             "CidrBlock": "10.0.0.0/16",
# |             "DhcpOptionsId": "dopt-1a827c71",
# |             "State": "available",
# |             "VpcId": "vpc-0c5e5696e3d771ab5",
# |             "OwnerId": "298337959615",
# |             "InstanceTenancy": "default",
# |             "Ipv6CidrBlockAssociationSet": [
# |                 {
# |                     "AssociationId": "vpc-cidr-assoc-05e9b0e12c951cf52",
# |                     "Ipv6CidrBlock": "2600:1f16:86c:8c00::/56",
# |                     "Ipv6CidrBlockState": {
# |                         "State": "associated"
# |                     }
# |                 }
# |             ],
# |             "CidrBlockAssociationSet": [
# |                 {
# |                     "AssociationId": "vpc-cidr-assoc-032386be3c892e79c",
# |                     "CidrBlock": "10.0.0.0/16",
# |                     "CidrBlockState": {
# |                         "State": "associated"
# |                     }
# |                 }
# |             ],
# |             "IsDefault": false,
# |             "Tags": [
# |                 {
# |                     "Key": "Name",
# |                     "Value": "ajay291491VPC"
# |                 }
# |             ]
# |         }
# |     ]
# | }
# | root@sathsang-Predator-G3620:/data/amazon# 
# | 
#
# STEP 2 : You need to create two subnets one with 10.0.1.0/24 and 10.0.2.0/24 
#
# | root@sathsang-Predator-G3620:/data/amazon# aws ec2 describe-subnets --subnet-id subnet-03040158cc669902f
# | {
# |     "Subnets": [
# |         {
# |             "AvailabilityZone": "us-east-2a",
# |             "AvailabilityZoneId": "use2-az1",
# |             "AvailableIpAddressCount": 250,
# |             "CidrBlock": "10.0.1.0/24",
# |             "DefaultForAz": false,
# |             "MapPublicIpOnLaunch": true,
# |             "State": "available",
# |             "SubnetId": "subnet-03040158cc669902f",
# |             "VpcId": "vpc-0c5e5696e3d771ab5",
# |             "OwnerId": "298337959615",
# |             "AssignIpv6AddressOnCreation": false,
# |             "Ipv6CidrBlockAssociationSet": [],
# |             "Tags": [
# |                 {
# |                     "Key": "Name",
# |                     "Value": "10.0.1.0_us-east-2a_ajay291491"
# |                 }
# |             ],
# |             "SubnetArn": "arn:aws:ec2:us-east-2:298337959615:subnet/subnet-03040158cc669902f"
# |         }
# |     ]
# | }
# | root@sathsang-Predator-G3620:/data/amazon# 
# | root@sathsang-Predator-G3620:/data/amazon# aws ec2 describe-subnets --subnet-id subnet-0deb3da61bc229aec
# | {
# |     "Subnets": [
# |         {
# |             "AvailabilityZone": "us-east-2b",
# |             "AvailabilityZoneId": "use2-az2",
# |             "AvailableIpAddressCount": 250,
# |             "CidrBlock": "10.0.2.0/24",
# |             "DefaultForAz": false,
# |             "MapPublicIpOnLaunch": false,
# |             "State": "available",
# |             "SubnetId": "subnet-0deb3da61bc229aec",
# |             "VpcId": "vpc-0c5e5696e3d771ab5",
# |             "OwnerId": "298337959615",
# |             "AssignIpv6AddressOnCreation": false,
# |             "Ipv6CidrBlockAssociationSet": [],
# |             "Tags": [
# |                 {
# |                     "Key": "Name",
# |                     "Value": "10.0.2.0_us-east-2b_ajay291491"
# |                 }
# |             ],
# |             "SubnetArn": "arn:aws:ec2:us-east-2:298337959615:subnet/subnet-0deb3da61bc229aec"
# |         }
# |     ]
# | }
# | root@sathsang-Predator-G3620:/data/amazon# 
# | 
#
# STEP 3 : Attach an internet gateway with the VPC, so that VPC will be able to communicate with the internet
#
# | root@sathsang-Predator-G3620:/data/amazon# aws ec2  describe-internet-gateways --query "InternetGateways[?contains(InternetGatewayId, 'igw-06301d49b08f3b4a9')]"
# | [
# |     {
# |         "Attachments": [
# |             {
# |                 "State": "available",
# |                 "VpcId": "vpc-0c5e5696e3d771ab5"
# |             }
# |         ],
# |         "InternetGatewayId": "igw-06301d49b08f3b4a9",
# |         "OwnerId": "298337959615",
# |         "Tags": [
# |             {
# |                 "Key": "Name",
# |                 "Value": "ajay291491_IGW"
# |             }
# |         ]
# |     }
# | ]
# | root@sathsang-Predator-G3620:/data/amazon# 
# | 
#
# STEP 4 : Attch one of the Subnet to VPC so that it can comunicate with the Internet 
#
# | root@sathsang-Predator-G3620:/data/amazon# aws ec2 describe-subnets --query "Subnets[?contains (SubnetId, 'subnet-0deb3da61bc229aec')]"
# | [
# |     {
# |         "AvailabilityZone": "us-east-2b",
# |         "AvailabilityZoneId": "use2-az2",
# |         "AvailableIpAddressCount": 250,
# |         "CidrBlock": "10.0.2.0/24",
# |         "DefaultForAz": false,
# |         "MapPublicIpOnLaunch": false,
# |         "State": "available",
# |         "SubnetId": "subnet-0deb3da61bc229aec",
# |         "VpcId": "vpc-0c5e5696e3d771ab5",
# |         "OwnerId": "298337959615",
# |         "AssignIpv6AddressOnCreation": false,
# |         "Ipv6CidrBlockAssociationSet": [],
# |         "Tags": [
# |             {
# |                 "Key": "Name",
# |                 "Value": "10.0.2.0_us-east-2b_ajay291491"
# |             }
# |         ],
# |         "SubnetArn": "arn:aws:ec2:us-east-2:298337959615:subnet/subnet-0deb3da61bc229aec"
# |     }
# | ]
# | root@sathsang-Predator-G3620:/data/amazon# 
#
#
# STEP 5 : Launch two EC2 instance and make sure one connect to a private subnet and other to public subnet, create ssh rule for security group
#
# STEP 6 : Once the Instance is launched then access the Public subnet instance one from Public subnet
#
# STEP 7 : Add the private subnet to allow SSH, ICMP, HTTP protocols from the Public Subnet to access it 
#
#
# * NAT gateways 
# NAT gateways uses the  network translation protocol which helps to instance in private subnet to communicate with the public subnet. 
# This also do not allow the instances to be accessed from public network. 
# NAT gaetways supports redudancy on one availability zone and its limited to only one availability zone at the moment. 
# NAT Gateways scales from 5 Gb to 45 GB at the moment 
# NAT gateways are completely managed by Amazon and no need to patch them. 
# NAT gateways will be automatically allocated with an Elastic IP address while creating them, when you delete the NAT instance the elastic IP won't get deleted instead it will only get disassociated 
# 
# Below is the stape to Create a NAT gateway 
# 1. Create a NAT gateway from AWS Console 
# 2. Choose a public subnet you want to associate the NAT gateway 
# 3. You will need to generate an elastic IP as part of the creation 
# 4. Once you created the NAT gateway then you will need to associate that NAT gateway with one or more priviate subnets routing table
# 5. Once the association is done, instances from the private subnet will be able to communicate with the Internet using the NAt gateways 
#
# . Things to remeber about the NAT gateways 
#   - NAT gateways do not have security groups so you cannot set any restrictions there 
#   - To setup restriction to what to be accessed from the Ineternet you can setup that in Security group of instance or NACL at the subnet level 
#
# * NAT Instances 
# NAT Instaces are used for enabling internet or public network connectivity for the Private subnet instances 
# To Create NAT instances you will need to locate amzn-nat-ami and then launch the instance in the public subnet 
# Once you have launched the instance make sure you have to disable the "Source and Destination check"
# Once that is done then You will need to configure the route for private subnet with the NAY instance, so that it will commununicate via the elastic IP configured for NAT 
# Once the Elastic IP configured then you can go to the instance in provate subnet to make sure its communicating 
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 07 - Elatic Load balancer 
#-------------------------------------------------------------------------------------------------------------------
#
# * What is a Load balancer 
# Load balancer will help to distribute the traffic to various different underlying instance to load the traffic. 
# Based on the methods, load balancer are different types 
#
# 1. Appliation load balancer 
# 2. Network Load Balancer
# 3. Classic Load Balancer 
#
# * Application Load balancer 
# Application load balancer normally operate in the layer 7 (application) and they mostly handle the http/ https based traffic. 
# Application load balancers are more of an intelligent devices which is cpable of doing decission making. 
# This is best suite for application with modern architecture such as micro services and containers. 
# It is capable of routing the traffc based on the content on the input traffico
# Application load balancer works on the model of "Target groups" to route traffic
# . Target Groups
#   Target groups allows to route traffic to group of instance where the listener rules gets matched. 
#   Listner will be configured with specific rules which helps to deltermine few conditions and based on the condition the traffic will be routed 
#
# * Network Load Balancer 
# Network Load balancers are operate at the Layer 4 (Transport) of the OSI model. 
# This load balancer can be used with application which is dealing with TCP and UDP layer
# This Load balancer provides the highest performance among any other load balancers 
#
# * Classic Load balancer 
# Class Load balancer are the oldest among the load balancers and they are capable of catering to http /https and TCP/UDP traffic as well. 
# Classic load balancer do not have much logical capabilities of application load balancer, but it can send traffic in round robin method 
# Classic load balancer works on layer 4 to route the traffic between TCP and UDP traffic 
# Classic load balancer works in layer 7 to route traffic between https/ https 
# 
# * Exam tips for Load balancers 
# - Load balancer will have a DNS name, but you won't be getting the IPS address for the same 
# - Health checks will check the health status of the instance underlying to the load balancer 
# - Instances monitored in ELB are mentioned as In Service or Out of Service
#
# * Stickey Session 
# When you want a traffic from a user to be sent to a same instance under the load balancer because of cache the app might store then you can enable sticky session 
# Sticky session will allow you to send traffic from a specific user to a specific instance  for the whole session. 
# You can enable the sticky session for appliaction load balancer as well, but at the target group level
#
# * Cross zone load balancing 
# When you enable cross zone load balancing it will help to send equal traffic to the instances in different availability zones 
# Suppose lets you have enabled weighed routing on your Route 53 to send 50% traffic to east-1 and east-2 and east-1 you have 5 instance and east-2 you have 1 instances 
# In this case server in east-2 is expected to get lot of load since it is weighed routing. 
# In case you enable cross zone routing load balancing has the capability to understad he amount of instances in both regions and based on that it will re-route traffic low loaded zone instances 
# This will infact help to leverage the total load across various instances 
#
# * Pattern based load balancing 
# When there are scenarios such as in case of an application where you want to send audio related traffic to one zone image related traffic to another zone 
# Then you can send pattern based routing such as image based routing to one url and audio related traffic to one load balancer etc 
#
# * Auto Scaling groups 
# Auto scaling groups will help you to create redudant instances when you are planning to host an application 
# ASG has mainly two parts when you are planning to launch as ASG 
#
# 1. Launch Configuration 
#    Launch configurations are kind of boiler plate which tells what kind of configuration it should be using to launch an ASG
#    This will request to create below parameter as part of the ASG creation 
#    - I am Role 
#    - AMI
#    - Monitoring to be set to Y/N
#    - Storage 
#    - Security Group 
#    - Key Pair 
#
# 2. Creating a ASG
#    Once You have a launch config available then you will be able to create a ASG from that 
#    Once you Create an ASG you will need to provide below information 
#    - ASG Name 
#    - ASG size 
#    - Scaling Policy - Min and max 
#    - Scaling Policy - Monitoring based on CPU usage 
#    - Warm up and Colling period - Amount of time before launching or terminating an instance after the last scale up or scale down event 
#    - Notification via SNS ( If you have a cloud watch monitoring enabled you enable also a SNS push to create an alert incase of a scale up or scale down event)
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 08 - CloudFormation
#-------------------------------------------------------------------------------------------------------------------
# CloudFormation is an automated way to provision the aws resources in an Infrastructure as a code.
# It has mainly three compnents which helps to drive the whole automation around, they are 
#
# - Templates 
# - Stacks 
# - Changesets  
#
# * Templates 
# Templates are manifest files which contain infromation about how a resource or group of resource need to be provisioned. 
# This can be written either in JSON or YAML syntax. This will have definition about various resources how that needs to be configured. 
# Templates has mainly few sessions and each are described below.
#
# Syntax : Below is a high level syntax of the Templates 
#
# | { 
# |	"AWSTemplateFormatVersion" : "2020-01-07",
# |	"Description" : "Sample Template to Launch an EC2 Instance",
# |	"Parameters" : { # input parameter details goes here and one sample given below 
# |		Parameter_01{
# |			"Description" : "note about the parameter getting passed",
# |			"Type"	: "Valid type"}
# |	}
# |	"mapping"  : { # mapping details goes here }
# | "Resource" : { # resource definitions goes here }
# |	"output"   : { # output you want to show from stack you can mention here }
# | }
# 
# - Parameters 
# Inputs which is getting passed into the template while creating the stack, we can mainly define the type and description about a paramter here.
# There few valid types allowed for parameters they are catagorized into two types they are general and aws parameters.
# 
# . General Parameter types : Below are the few general parameters allowed
#   String : "Resource123"
#   Number : 100
#   List (Number) : [1, 2, 3]
#   CommaDelimitedList : ["t2.nano", "t2.micro", "t2.small"] 
#
# . AWS parameter types : Below are few sample aws parameter types 
#   AWS::EC2::Image::ID				: "ami-i566757"
#   AWS::EC2::Instance::ID			: "i-12hjh876"
# 	AWS::EC2:: SecurityGroup::Id 	: "sg-ashak7c"
# 
# NOTE : Please see url for complete Types : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html
#
# - Mapping 
# The optional Mappings section matches a key to a corresponding set of named values. 
# For example, if you want to set values based on a region, you can create a mapping that uses the region name as a key and contains the values you want to specify for each specific region. 
# You use the Fn::FindInMap intrinsic function to retrieve values in a map.
# 
# Example : Below is sample for mapping 
# 
# | "Mappings" : {
# | "RegionMap" : {
# |   "us-east-1"      : { "HVM64" : "ami-0ff8a91507f77f867"},
# |   "us-west-1"      : { "HVM64" : "ami-0bdb828fd58c52235"},
# |   "eu-west-1"      : { "HVM64" : "ami-047bb4163c506cd98"},
# |   "ap-southeast-1" : { "HVM64" : "ami-08569b978cc4dfa10"},
# |   "ap-northeast-1" : { "HVM64" : "ami-06cd52961ce9f0d85"}
# |  }
# | } 
#
# - Resource 
# This is the session on the template where you define the targetted resource and their properties. 
# AWS has almost all aws resource types available for you to define via the cloud formation template and it is documented below.
# URL : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html
# 
# | "Resources" : {
# |   "EC2Instance": {
# |		"Type": "AWS::EC2::Instance"
# |		"Properties" : {
# |			"ImageId" : {"FindInMap": ["RegionMap", {"Ref", "AWS::Region"}, "HVM64"]},
# |			"InstanceType" : "t2.nano", 
# |			"NetworkInterface": [{
# |				"AssociatePublicIpAddress" : "true",
# |				"DeviceIndex" : "0",
# |	 			"GroupSet": [{"Ref": "SecurityGroup"}],
# |				"SubnetId": {"Ref": "Subnet"}
# |				}],
# |		}
# |	}
# 
# - output 
# When you want to provide an output from the stack then you will need to define the corresponding outputs in template. 
# This is an optional parameter in the template and you can define multiple outputs in the template.
# To get the a value assigned to a output we can either use the 'href' function if its a simple and direct method and 'Getatt' for complex lookups.
#
# | "Outputs" : {
# | "BackupLoadBalancerDNSName" : {
# |   "Description": "The DNSName of the backup load balancer",  
# |   "Value" : { "Fn::GetAtt" : [ "BackupLoadBalancer", "DNSName" ]},
# |   "Condition" : "CreateProdResources"
# | },
# | "InstanceID" : {
# |   "Description": "The Instance ID",  
# |   "Value" : { "Ref" : "EC2Instance" }
# |  }
# | }
#
#
# * Stacks 
# When you use AWS CloudFormation, you manage related resources as a single unit called a stack. 
# You create, update, and delete a collection of resources by creating, updating, and deleting stacks. 
# All the resources in a stack are defined by the stack's AWS CloudFormation template. 
# Suppose you created a template that includes an Auto Scaling group, Elastic Load Balancing load balancer, and an Amazon Relational Database Service (Amazon RDS) database instance. 
# To create those resources, you create a stack by submitting the template that you created, and AWS CloudFormation provisions all those resources for you. 
# You can work with stacks by using the AWS CloudFormation console, API, or AWS CLI.
#
# Example : Below are the sample commands you can check to understand about stacks 
#
# | # aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE
# | [
# |     {
# |         "StackId": "arn:aws:cloudformation:us-east-2:123456789012:stack/myteststack/
# | 644df8e0-0dff-11e3-8e2f-5088487c4896",
# |         "TemplateDescription": "AWS CloudFormation Sample Template S3_Bucket: Sample template showing how to create a publicly accessible S3 bucket. **WARNING** This template creates an
# | S3 bucket. You will be billed for the AWS resources used if you create a stack from this template.",
# |         "StackStatusReason": null,
# |         "CreationTime": "2013-08-26T03:27:10.190Z",
# |        "StackName": "myteststack",
# |         "StackStatus": "CREATE_COMPLETE"
# |     }
# | ]
# | #
#
# | # aws cloudformation describe-stacks --stack-name myteststack
# | {
# |     "Stacks":  [
# |         {
# |             "StackId": "arn:aws:cloudformation:us-east-2:123456789012:stack/myteststack/a69442d0-0b8f-11e3-8b8a-500150b352e0",
# |             "Description": "AWS CloudFormation Sample Template S3_Bucket: Sample template showing how to create a publicly accessible S3 bucket. **WARNING** This template creates an S3 bucket.
# | You will be billed for the AWS resources used if you create a stack from this template.",
# |             "Tags": [],
# |             "Outputs": [
# |                 {
# |                     "Description": "Name of S3 bucket to hold website content",
# |                     "OutputKey": "BucketName",
# |                     "OutputValue": "myteststack-s3bucket-jssofi1zie2w"
# |                 }
# |             ],
# |             "StackStatusReason": null,
# |             "CreationTime": "2013-08-23T01:02:15.422Z",
# |             "Capabilities": [],
# |             "StackName": "myteststack",
# |             "StackStatus": "CREATE_COMPLETE",
# |             "DisableRollback": false
# |         }
# |     ]
# | }
# | #
#
#
# * Changesets 
# If you need to make changes to the running resources in a stack, you update the stack. 
# Before making changes to your resources, you can generate a change set, which is a summary of your proposed changes. 
# Change sets allow you to see how your changes might impact your running resources, especially for critical resources, before implementing them.
# For example, if you change the name of an Amazon RDS database instance, AWS CloudFormation will create a new database and delete the old one. 
# You will lose the data in the old database unless you've already backed it up. 
# If you generate a change set, you will see that your change will cause your database to be replaced, and you will be able to plan accordingly before you update your stack. 
#
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 08 - Applications 
#-------------------------------------------------------------------------------------------------------------------
#
# * SQS (MQ - Pull based Messaging Queue Service)
# This is a very old service which offered by AWS, this offers a pull based messaging queue system  which helps to decouple your resource from queue. 
# This is a distributed queue system which helps to store messages in the queue until a computer resource is available to process them.
# Messages stored on the queue can be accesses by application using the SQS API service. 
#
# Below are few key features of SQS 
# . SQS offers minimum 1 min to maximum 14 days retention where default retention is for 4 days 
# . SQS offers a pull method to get message, which means the application has to push the message to queue and also another application need to pull the message from queue
# . SQS offers default 256KB of text in any format
# . For larger files it offers a solution via S3 which can store upto 2GB
#
# . What is Visibility Timeout 
# When a message picked up by a server for processing then that message will disappear in the SQS queue for some time since it is getting processed, this is called visibility timeout.
# But with in certain time if that message is not processed then it is expected to comeback and another server will attempt to process the message again. 
# In this situation there are chances that you message might get delivered twice. 
#
# NOTE : The maximum visibilty timeout which can configured is 12 Hours 
#
# There are mainly two types of queue 
# - Standard Queue
#   Offers best effort ordering, generally order in the same order it goes. But the ordering not guaranteed.  
#   This Gurantees the messages will be delivered atleast once. 
#   But the messages can get delivered more than once 
#
# - FIFO Queue
#   This is FIFO, it support first in first out
#
# . What is long polling 
# This is way to save the cost, in a normal short polling an EC2 instance will be constantly polling the service. 
# But when it comes to long polling it will wait for a message to arrive in the queue and then it return the message or long poll times out.
#
# * SWF (Task workflow)
# Amazon SWF is a simple workflow service which provides the capability of creating a series of workflow tasks. 
# This is a web service which makes it easy to coordinate work across various distributed application components. 
# This enables for various different use cases 
# - Including media processing 
# - Web application backends 
# - Business processing workflows 
# - analtical pipeline 
# - also for coordination tasks 
#
# There mainly three core conepts which we need to understand about SWF
# 
# 1. Workflow Starters : This is the component which intiate the workflow, an example could be an Ansible API
# 2. Deciders          : This component keeps track of the status from previous action and it decides what to do next 
# 3. Activity workers  : This carry out the activity tasks
#
# Example : Buying a book from amazon will trigger lot of backend workflows such as checking stock, making payment, human interaction to  physicaly shipping the book, ackowldging the receipt etc 
#
# . What is a task 
# SWF triggers a series of tasks and tasks represent various invocation step in an application step which can be performed by executable code, api calls or by human interaction 
#
# Exam Tips 
#  . SQS workflow execution last upto 1 year 
#  . This offers a task oreinted API 
#  . SWF make sure the task is assigned only once and it will never get duplicated 
#  . SWF keeps track of all tasks and events in an application 
#
#
# * SNS (Simple Notification Service - Push based Messaging)
# This service is used for pushing notification into various different services. 
# This allows developers to push notification to users immedietly for subscribed useres. 
# Any messages delovered to the SNS platform will be stored redudantly in multiple availability zones 
# This allows you to push notification to below platforms. 
# - Apple 
# - Google 
# - Fire OS
# - Windows devices 
# - All android devices
# - Baidu Cloud push 
#
# SNS has mainly three components to work together 
#
# - Publisher  : This is the person or organization or function which publish the topics to the subscribers 
# - Topics     : Its a topic regarding some subject where someone will be scbscribed for the message, like stock market, youtube channel
# - Subscriber : End user email or device or component which subscibe to a topic 
#
# It supports various end points as subscribers such as HTTP, HTTPS , Email, Email-JSON, SQS, AWS LAMBDA, SMS, platform application end point
#
# NOTE : on top of the push notification services SNS can be used to send SMS messages 
#
# * Elastic Transcoder
# Amazon Elastic transcoder are used to transcode low resolution videos to high resolution videos. 
# To make a elastic transcoder work you will require three essential components 
#
# - Input S3 Bucket  : This is where you will keep your source format videos 
# - Pipeline         : This pipeline picks up the video from input S3 bucket and does the processing and Save that to the Output S3 bucket
# - Output S3 Bucket : This is the destination bucket for all the processed video from the Transcoder 
#
#
# * API gateway 
# API Gateway is used for connecting to AWS resource using an API. This can be configured to autoscale and as well as throttle API gateway to prevent attack.
# - API gateway has caching capabilities, which means if multiple users asking for the same GET requests it can serve the content from cache until the TTL expires 
# - TTL values can be configured 
# - API gateway is low coast and scale automatially 
# - You can throttle API gateway to prevent attack 
# - You can log results of API gateway to CloudWatch 
# - In case you are using Javascript or AJAX if you are calling the API gateway, make sure you enable CORS to accept content from multi domains such as s3, ec3 etc 
# - CORS is enforced by client 
#
# * Kenisis 
# Amazon Kenisis is a platform on AWS which is used to handle streaming data from various different sources. 
# Kenisis is mainly devided into three catagories
#
# 1. Kenisis Streams 
# - Kenisis Streams consist of shards 
# - Data producers will send the data to streams in kenisis 
# - Retention is from 24 hours to 7 days 
# - Shrads are kind of partition within the Stream which collective group similar data for streaming
# - 1 MB/s or 1000 messages at write per shard 
# - 2 MB/s or 5 API calls per second per shards across all consumers 	
#
# 2. Kenisis Firehose 
# - Firehose doesn't allow persistant storage of data within 
# - As soon as the data arrives you will need to do something with the data 
# - Normally data from Kenisis firehose will be passed to S3 buckets and from there it will get transferred to destination such as splunk, ELK or Redshift
# - Near real time with a 60 sec latency or by having a buffer interval of 60 seconds minimum.
# - auto scaling
# - Supports Data conversion from CSV/JSON to Parquet / ORC
#
# 3. Kenisis Analytics 
# - Kenisis Data analytics will allow you to quickly author SQL or Apache Flink code that contiously reads, process and stores data in near real time.
# - This can be mainly used for 
#   . Time Series Analytics 
#   . Feed to real time dashboards 
#   . Create real time metrics 
# - You can also have a reference table, which can be used for joining your lookup input data with certain reference data keys
# - This can work with combination with stream or firehose as an input data source to perform data analytics 
# - This will be working inside the Streams and Firehose 
# - Destination of the Kenisis analytics can be kenisis streams or firehose or a lambda function
# - Using the firehose we can send the data to Amazon S3 or redshift as well
# - Serverless and auto scaling
# - This also supports machine learning for some complex data analytics and couple of algorithms supported are 
#   . RANDOM_CUT_FOREST : This is to detect the anomalies in the given data set
#   . HOTSPOT   : This is to detect dense regions in your data 
# More details : https://docs.aws.amazon.com/kinesisanalytics/latest/dev/how-it-works.html
#
# 4. Kenisis Video streaming
# - This is used for streaming videos from various devices such as security camera, smart phones, body camera, radar data, RTSP camera etc 
# - This can store data from 1 hour upto 10 hours, this satisfy various security and regulatory requirement to store data over a period of time
# - This also has video playback capability 
# - One producer per video stream, which mean if we have 100 devices producing video then we will require 100 video streams 
# - We can send this data to various consumers such as 
#   . Sagemaker 
#   . Your own custom mxnet or Tensorflow algorithms 
#   . Amazon recognition Video service etc 
#
# * Kenisis Summary - For machine learning 
# - Kinesis Data Stream   : This can be used for creating real time machine learning Applications
# - Kenisis Data Firehose : Ingest massive data near-real time for machine learning applications
# - Kenisis Data Analytics: This can be used for real time ETL, SQL or Flunk based apps to transform the data or can be used for ML algorithsm in real time such as random cut forest or hostspot
# - Kenisis Video Stream  : Real time video stream to create ML applications 
#
#
# * Congnito 
# Congnito is a web identity federation service which provided by amazon which can be integrated with application 
# This also has an integration with many major social media account federation services such as google, facebook and amazon 
# This helps to manage authentication services for your application without much code written for that into your codebase 
# This helps to synchronise the data and action between multiple different devices for your application such as password change from phone reflect in desktop etc 
#
# There are mainly two pools in Cognito which works together to provide the authenication and autherization service 
#
# . User Pools 
# User pools will be the component in Cognito which helps to vefiry the user credentials of an user. 
# There can be two ways users can be available in Cognito, one is maintaing user information in Cognito and other is using identity providers such as facebook or google.
# When your application makes a request to authetnicate an user it will act as a broker between the identity provider and make sure user exist and its password is correct 
# Once the Username and password are validated then the identity provider such as google or facebook will issue a JWT token as a confirmation of authetication is done 
# Using this JWT your application an again contact the COgnito Identity pool to get the aws resources. 
#
# NOTE : User Pools are stritly for User registration, authentication and account recovery 
#
# . Identity Pools
# Identity Pools are pools which gives the access to the AWS resources based on the JWT token issued by the identity providers 
# Up on successful authentication they grant access to AWS resources such as S3, DynamoDB, RDS etc 
# What identity provider does is it will Grant the IAM role to access the AWS resource based on the authenticated JWT provided by the authetication provider
# 
# . SNS in action 
# Cognito uses a SNS push based method to synchronise and update all the devices if there are changes like password or other user preference in application. 
#
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 09 - Lambda 
#-------------------------------------------------------------------------------------------------------------------
#
# * What is Lambda
# It is called as as server less computing or Function as a service which helps you to run the code on demand without hanving a need of an infrastructure. 
# Lambda will be triggered on demand and will perform an action as requested in the codebase. 
# There are few reason someone should choose Lambda over an infrastructure component to run your small on demand jobs 
#
# - Event Driven : Lambda is event driven which can be triggered based on various triggers 
# - Scheduler    : Lambda can be configured with cron based scheduling or on perticular schedules 
# - APi Driven   : Lambda can be used from another application to trigger code  
# - Cost         : Lambda is very cost effective because it charges only for the time it runs and the resource consumed during that time.
# - Scaling      : Lambda is capable of scaling which means if multiple requests comes in it will scale up the concurrent instances
#
# Commnaly used lambda fucntions 
# - Data stream processing 
# - Image processing 
# - Running batch jobs
#
# Limitations of Lambda 
# - Uncompressed File size : 250 MB
# - Compressed File Size   :  50 MB
# - Stoarge Max size       : 512 MB
# - Execution Duration Max : 300 seconds (5 Mins)
# - Concurrent Functions   : 100 
# - Minimum Memory         : 128 MB
# - Maximum Memory         : 1.5 GB
# - CPU                    : Automated scaling according to requests 
#
# * Structure of Lambda function 
# Below format is the normal syntax of the Lambda function 
#
# Syntax : 
#
# | def handler_name(event, context):
# |    => Start your code 
# |    => End your code 
# |    return result
#
# Lambda functions has mainly two components first one is event and another one is context. 
#
# Event   : Event contains the information about what triggers the lambda function, this will be normally defined as a dict in th event parameter 
# Context : Run time information about the lambda such as memory info, size, log integrations etc 
#
# * Creating a Lambda function
# When creating a lambda function there should be a few components we need to be aware of 
#
# Name     : This should be the name of the lambda function 
# Runtime  : Which runtime environment you want to use for the lambda function 
# Role     : This should be the role with right level of policy permissions 
# Policy   : Make sure you have enabled the policy with all resource level access so that Lambda can access the resource 
# Test     : This should be the test case which need to be validated for the lambda 
#
# Example : Below is a sample Lambda function created to take Backup on DynamoDb which created with Table Name "Inventory"
#
# | root@sathsang-Predator-G3620:/study_docs/aws/JPMC_AWS# aws lambda get-function --function-name DynamoBakups
# | {
# |     "Configuration": {
# |         "FunctionName": "DynamoBakups",
# |         "FunctionArn": "arn:aws:lambda:us-east-1:account_no:function:DynamoBakups",
# |         "Runtime": "python3.6",
# |         "Role": "arn:aws:iam::account_no:role/service-role/DynamoBakups-role-kjlcbl9s",
# |         "Handler": "lambda_function.lambda_handler",
# |         "CodeSize": 272,
# |         "Description": "",
# |         "Timeout": 3,
# |         "MemorySize": 128,
# |         "LastModified": "2020-01-27T07:48:35.788+0000",
# |         "CodeSha256": "/Eu1PKa/ji0QEGvWl/Gns2sDoD4YQxtm6g50PcmmvoM=",
# |         "Version": "$LATEST",
# |         "TracingConfig": {
# |             "Mode": "PassThrough"
# |         },
# |         "RevisionId": "26fffccb-02ca-4eb5-987c-8fdf8bdc712e"
# |     },
# |     "Code": {
# |         "RepositoryType": "S3",
# |         "Location": "https://prod-04-2014-tasks.s3.us-east-1.amazonaws.com/snapshots/account_no/DynamoBakups-9e4f1d40-c42d-4e7e-a659-de932731e7a3?versionId=WcE2vGRYzM0VXWvbfc7fH7Y2DnJqblxa&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAgaCXVzLWVhc3QtMSJGMEQCIB%2BoBHNKltX6q0NSzMr1EB8hP72eEhvzDhRpcyGOtIWZAiAjdbxbZjGYIi8FwGjuRGO1IDhdEM%2FGd1zkloJHUqdYayq9Awig%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDc0OTY3ODkwMjgzOSIMLZdyHEJFiM3GRjQQKpEDz1D9S4YmVE3Hk4r9RWcKuRjCGJ0IcABQ8AEmToiR1%2Fa%2BSwcdOEx%2FTmVe31GlIzXvRU0oIGfgNlqjVrk5wRc2y%2F9z%2Beit9o2f3lRWyaX%2FQf0FqpVDyDrRgFdbVbQ0CcWd2a4BjsLopBpiY816oUc0ujrPNh%2B7IDHUpE15SWcdM6Akj%2BzaTmubYwtHAhMHflTZ3BhiV763X7ONjgxkUsZO6Aqp5zeAOgUm1%2FA5ynOdkaFxw6c8p7NwAA9EoFK4pjN5oLYxyeDMrjF5qdJ0f6zYVF5d8VQuWyEyEvy27s6BMmBZTsEX97jTehZutr0SniKzLMiPhSsaPLu7CeIyvROaVdAh0u55PwnEs4nk2YPl3ah6WSrgR7mg6gfu2Q2G1Kmq4zoUqLJrQdItZZ61CovE8ccFxS39hleBkPT3h6eZbyep0YS9TfERh8JLm3mc7uWusAktvNpPqBQ8j4nTWX8qiMhgFbP6RN%2BouwWlICPTCJ%2F3k5nL6Su5PiXazh9xpEsDkaorl89BCgD62fA5Q6vs4AQwyKi68QU67AHk2Sj03FUfY3L0b2rYueaMnTqTAEud%2FYiAUxdg2Fb9yhWMrUGjG1SriGkKU49CLpQixBuQa76m9rdxCywgBcOkOe7IzkyKuJt6S0uQdWlEGlzEHdKODfPcxWFUQvR1QYWnnlNTxCqiJR1Rr7xXzB781aWymE%2FTH8HudWPHN%2BSgWeBlQUU5xVt6QpXYCtnwvIAthuOJVEENulekkLUrrAu2UF%2BI6iskN2qSIaKozoqDpzu%2BkcrLmmuLRebYOp4C53SFRIwGrTlAHP0BwxmmWkeEFSlwTALvjNcKVndq0D4bZfJf4SAF0witfHRw6A%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200127T075626Z&X-Amz-SignedHeaders=host&X-Amz-Expires=600&X-Amz-Credential=ASIA25DCYHY37BW5ZMOL%2F20200127%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=75a75d9a52948891d3533b67939230fa81b4c669ac4af2dbce6cd72f1d3e070d"
# |     }
# | }
# | root@sathsang-Predator-G3620:/study_docs/aws/JPMC_AWS#
# |
# |
#
# Example Code : 
#
# | import boto3
# |
# | client = boto3.client('dynamodb')             => Connecting to DyanmoDB using Boto3 
# |
# | def lambda_handler(event, context):
# |     client.create_backup(
# |         TableName = "Inventory",              => DyanmoDB Table name 
# |         BackupName = "MyAutomaticBackup"      => DynamoDB backup name 
# |         )
# |
#
# * How to Create lambda using external code 
# This will be often possible that you will be developing the lamda code outside the lamda editor and want to push that lambda 
# There are mainly two ways you can achieve this for both you will need to zip the lambda function with all the code and dependacy required for it. 
#
# 1. Create a zip file local in your desktop and upload them using the AWS CLI 
# 2. Upload the code zip file to an s3 bucket and then publish that using the AWS CLI 
#
# Syntax : 
#
# | $ aws lambda create-function \
# |   --region us-east-1 \
# |   --function-name MyNewLambda \
# |   --zip-file c://my_code.zip  or  --code S3Bucket=bucket-name, S3Key=zip-file-obj-key
# |   --role arn:aws:iam:account-id:role/my_lambda_role \
# |   --handler hello_python.my_handler \
# |   --runtime python3.6
# |   --timeout 15
# |   --memory-size 512
#
# * Lambda and CloudWatch 
# When you invoke any action that by default gets updated to CloudWatch, which means you can find more details about the Lambda function during the runtime in the cloudWatch logs 
# If you want to give more detailed information in cloud watch you can add more logging by using logging framework 
#
# Example : We are refactoring the existing lambda function to add more logs 
#
# | import boto3
# | import logging
# |
# | client = boto3.client('dynamodb')
# | logger = logging.getLogger()
# | logger.setLevel(logging.INFO)
# |
# |
# | def lambda_handler(event, context):
# |     logger.info("Function {} has started".format(context.function_name))
# |     client.create_backup(
# |         TableName = "Inventory",
# |         BackupName = "MyAutomaticBackup"
# |         )
#
# Now lets see how that is working in CloudWatch 
#
# | $ aws logs start-query \
# | --log-group-name '/aws/lambda/hello-world-insight' \
# | --start-time 1546009200 \
# | --end-time 1546020000 \
# | --query-string 'fields @timestamp, @message
# | | sort @timestamp desc
# | | limit 4'
# | 
# | 
# Output
# | 
# | {
# |     "queryId": "01251c78-7e7b-4c8b-872d-0864390183fs"
# | }
# | 　　
# | 
# | $  aws logs get-query-results --query-id '01251c78-7e7b-4c8b-872d-0864390183fs'
# | {
# |     "results": [
# |         [
# |             {
# |                 "field": "@timestamp",
# |                 "value": "2018-12-28 17:59:16.155"
# |             },
# |             {
# |                 "field": "@message",
# |                 "value": "REPORT RequestId: 3a67f8af-0aca-11e9-be16-c16d11edadca\tDuration: 0.41 ms\tBilled Duration: 100 ms \tMemory Size: 128 MB\tMax Memory Used: 20 MB\t\n"
# |             }
# |         ],
# |         [
# |             {
# |                 "field": "@timestamp",
# |                 "value": "2018-12-28 17:59:16.155"
# |             },
# |             {
# |                 "field": "@message",
# |                 "value": "END RequestId: 3a67f8af-0aca-11e9-be16-c16d11edadca\n"
# |             }
# |         ],
# |         [
# |             {
# |                 "field": "@timestamp",
# |                 "value": "2018-12-28 17:59:16.154"
# |             },
# |             {
# |                 "field": "@message",
# |                 "value": "value1 = Hello,World!!\n"
# |             }
# |         ],
# |         [
# |             {
# |                 "field": "@timestamp",
# |                 "value": "2018-12-28 17:59:16.154"
# |             },
# |             {
# |                 "field": "@message",
# |                 "value": "START RequestId: 3a67f8af-0aca-11e9-be16-c16d11edadca Version: $LATEST\n"
# |             }
# |         ]
# |     ],
# |     "statistics": {
# |         "recordsMatched": 18.0,
# |         "recordsScanned": 18.0,
# |         "bytesScanned": 1504.0
# |     },
# |     "status": "Complete"
# | }
#
# * Exam Tips 
# - Lambda Scales out automatically 
# - Lambda functions are indipendant, which means 1 event = 1 function 
# - One Lambda function and can invoke several other lambda functions 
# - Lambda is serverless 
# - Know what services are serverless in AWS 
# - Lambda architecture can be extremely complicated, AWS X-RAY allow you to debug what is happening 
# - Lambda can do things globally, You can use it to backup S3 buckets to other S3 buckets etc 
# - Various triggers available for lambda 
#
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 10 - Data Engineering (Machine Learning Certification)
#-------------------------------------------------------------------------------------------------------------------
#
# * Glue 
# AWS Glue is a managed ETL service that makes it simple and cost effective to catagorize your data, clean it, enrich it and move it reliably between various data stores and data streams. 
# AWS Glue consist of a central meta data repository called as AWS Glue catalog an ETL engine which automatically generate python and scala code for transformation and flexible scheduler.  
# AWS Glue is designed to work with Semi structured data 
# It introduces a component called dynamic frame which is similar to apache spark data frame which can be used for organizing data into rows and columns (like pandas data frame)
# AWS Glue is serverless
#
# More Details : https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html
# 
# * Glue Data catalog 
#  - Glue Data catalog is the meta data store for all your tables
#  - This provides automated schema inference and versioning 
#  - This can integrate with Athena and Redshift specturum 
#  - Glue Crawlers will help you to create data catalog
#
# * Glue Data catalog Crawlets
#  - Crawler will help to go through your data for schemas and partitions
#  - Works on Json, Parquet, CSV and relation data store 
#  - Run the Crawler on a schedule or on demand 
#  - You will require access to data store as well s3 buckets to access the data and tables
#  - Glue Crawlers will support data partitioning and you will need to plan partition primary key primarily by time range or device
#
# * Glue ETL
#  - This helps to transform, clean and enrich data before doing any analysis or ML training on them
#  - Generate ETL code in python or scala and which also gives a provision to modify them 
#  - You can also provide your own spark or pyspark scripts 
#  - This is fully managed and cost effective and pay only for the resource consumed 
#  - This is a server less spark platform 
#  - Glue also provides a scheduler which can be used to schedule jobs
#  - Glue 'Triggers' automated jobs based on 'events'
# 
# * Redshift
# - Redshift is a Data warehousing or SQL Analytics service 
# - OLAP : This used for online analytical processing 
# - We can load data from S3 to redshift 
# - Redshift spectrum can be used to query data directly in S3
# - Redshift uses column node based indexing for data 
#
# * RDS, Aurora 
# - These are relational database store, SQL 
# - OLTP : This used for online transation processing 
# - Must provision servers in adavance 
# 
# For Exam take look at Dynamo DB, Elastic cache also in high level 
#
# * AWS Data Pipeline
# AWS Data pipeline is a service which can be used to automate the movement and transformation of data.
# Using Data pipeline you can define task and workflow which are depend on previous tasks, Here you define a paramter for your data transformation and data pipeline takes care of the execution. 
# AWS Data pipeline contains mainly three components 
#
# - Pipeline definition : This specifies the business logic of your data management 
# - Pipeline    	: Pipeline schedules and runs tasks by creating EC2 instances according to the definition provided. You can upload the pipeline definition to the pipeline and activate it. 
# 			: If you are editing a pipeline, then you will need to activate the pipeline again to take the effect. 
# 			: You can deactivate pipeline, modify a data source and then activate the pipeline again 
# 			: Once the pipeline finished you can delete them. 
# - Task Runner 	: This will poll the tasks and then perform the tasks 
#
# Using pipeline you can perform tasks such as 
#
# - Moving data from RDS to S3 
# - Performing ETL on the given data etc 
#
# Unlike Glue which is serverless, here user will have more control on the resources since resources like EC2 and launched in the account as customer demands
#
# More Details : https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html
#
# * AWS Batch 
# AWS batch is used for running batch jobs as Docker Images and it is not necessarily need to be ETL or any data tramsformation jobs, any compute job can be processed as batch 
# - This is serverless and you do not need to manage any resource 
# - You will need to pay only for the compute resource spin up in your account by Batch according to your bacth volume 
# - You can schedule batch jobs based on cloud watch events 
# - You can orchetrate batch jobs using AWS step functions 
# - Batch is mainly good for any non ETL related work, for ETL you can look for Glue 
#
# * DMS - Database Migration Service 
# - This is a database migration service which help you to migrate database from your on-prem DBs to AWS 
# - This can either hogenious migration such as on-prem Oracle DB to AWS RDS oracle 
# - This can also be used for on prem SQL DBs to AWS RDS aurora 
# - This helps you to quickly and securely migrate database to AWS, it is resielient and self healing 
# - Source DB will remain available during the migration process 
#
# * Step Function 
# Step Functions are used for workflow orchestration with an easy visualization, error handling and retry mechanism outside the code 
# - This provides the audit of the history of workflows 
# - This has an ability to wait for arbitary amount of time to execute a task in workflow execution 
# - This workflow can have a maximum execution time of a state is 1 year 
# - For exam orchestration step function is the candidate for that 
#
#
#-------------------------------------------------------------------------------------------------------------------
#		Chapter 11 - Exploitary Data Analysis
#-------------------------------------------------------------------------------------------------------------------
# With Data Engineering we learned how to get the data and transferred to a data store such as s3 or redshift etc to analyze the data. 
# Exploitary data analysis will deal with analyzing and preparing the data which is needed for machine learning models to get trained. 
# As part of the this chapter we will go through various techniques needed for this purpose 
#
# * Python 
# Python is the default choice for machine learning currently, but the exam won't be testing us in pythion skills 
# But at the same time we will need to learn more about few of the machines learning modules such as pandas, numpy, matplotlibs and also scikit-learn
#
# To learn more about Python, look at : https://github.com/ajay291491/study_documents/blob/master/begin_python.py
#
# * Pandas 
# Pandas is used for handing csv files which are in rows and column format 
# - DataFrame : Pandas load the datas into dataframe, which is nothing but gives us a way to access the data in rows and column format. 
# - Series    : Series is a techniq used in pandas to access the data in rows which will convert the data from two dimension to single dimension 
# - Pandas is very good to interoperate with numpy 
#
# To Learn more about Pandas, look at : https://github.com/ajay291491/study_documents/blob/master/pandas_tutorial.ipynb
#
# * Matplotlibs
# Matplotlibs are used for visualizing the data which you are analyzing 
# You can represent your data in various charts and graphs using matplotlibs 
#
# More Info : Please link for more details on available charts from matplotlibs 
#           : https://matplotlib.org/stable/tutorials/introductory/sample_plots.html
#
# * Seaborn
# Seaborn is an advanced version of the matplotlibs which provides graphs with much prettier and meaningful insights 
# It also provides some additional options to work with the graphs.
#
# More Info : To know more about various graphs available with seaborn please below link 
# 	    : https://seaborn.pydata.org/examples/index.html
#
# * numpy 
# NumPy is the fundamental package for scientific computing in Python. 
# It is a Python library that provides a multidimensional array object
#
# More Info : https://numpy.org/doc/stable/user/whatisnumpy.html
#
# * Preparing Data for Machine Learning in Jupyter Notebook
#
# * Scikit_learn
# scikit_leran is a machine learning library used in python which helps to solves few key machine learning problems such as 
# - Classification		: Classification is the problem of identifying which sets of categories and putting them in right buckets 
# 				: Example : Classifying an email a spam or non-spam
#
# - Regression 			: Regression analysis is a set of statistical process of estimating the relationship between a dependant variable and one or more indipendant variable. 
#				: Example : Finding the housing prices based on various market condition using age of the property, sqft and location
#
# - Clustering problems. 	: Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). 
# 				: It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
#				: Example : Classifying network traffic 
#
# This is one of the most popular library in the machine learning world today which used for various different use cases. 
# scikit_learn algorithm features such as 
# - Support vector Machines 	: https://en.wikipedia.org/wiki/Support-vector_machine
# - Random Forest		: https://en.wikipedia.org/wiki/Random_forest
# - Gradient Boosting 		: https://en.wikipedia.org/wiki/Gradient_boosting
# - k-means 			: https://en.wikipedia.org/wiki/K-means_clustering
# - DBSCAN 			: https://en.wikipedia.org/wiki/DBSCAN
#
#
# * Data Types
# When it comes to machine learning you will often deal with multiple types of data and details are below. 
#
# - Numerical Data 	: This represents some sort of the quantitaive meassurement such as heights of people, page load time, stock price etc 
# 		   	: Numercial data is devided into two different catagories such as Discrete data and Continous data
#
#   . Discrete Data  	: Intiger based with out any flaoting point numbers, few example such as how car you have, how many house you own, how many plants in the garden 
#   . Continous Data 	: This data has infinitive possible values which can even contacin floating point values, few example like hight of a person, how much rain fall on a day 
#
# - Categorical Data 	: This is the type of data which do not have any specific mathematical meaning 
# 			: Few examples, Gender of people in a school, Place where people live in a state, Designation of employees in a company
# 			: It can also have numbers, but they do not have any mathematical values, such as street number, pin code, country code etc 
#
# - Ordinal Data	: This is a mix of numerical and categorical data 
# 			: Examples, star rating for a movie.
#
#
# * Data Distribution
# Distribution is a function which show the possible value for a variable and how often they occur.
#
# More Details : https://www.youtube.com/watch?v=UdAD2pOalmY
#
# - Normal Distribution : In this distribution values will be more centered around the mean and you will probability of a value to occur near mean will be higher 
# 			  This can be either lower from or higher from mean, where as a as the value deviate from mean either higher or lower you will see the probability of values are less
# 			  A standard bell curve or Guassian distribution is the way to represent the normal distrubution 
# 			  Example : Average marks scored by the students in a school, lets say the mean is around 80% where few will be higher than and few will be lesser than 80%
#
#  More Details : https://www.investopedia.com/terms/n/normaldistribution.asp
#
#  NOTE : Only normal distribution deal with normal data, all below distributions are deal with the Disceret data 
#
# - Probability Mass Distribution	: Probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value
#
# - Poisson Distribution 		: Poisson distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if 
# 					  these events occur with a known constant mean rate and independently of the time since the last event.
# 					  The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.
#
# - Binoli Distribution 		: True or False condition type distribution 
#
# - Bernoulli Distribution 		: Bernoulli distribution, is the discrete probability distribution of a random variable which takes the value 1 with probability and the value 0 with probability.
#					: Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. 
#
#
# * Amazon Athena
# Serverless way of querying the data within the S3 bucket, This is an interactive query service and no need to load data and it stays in S3.
# This uses presto under the hood and supports many data formats CSV, JSON, ORC, Parquet, Avro
# This supports unstructured, semi-structured and structured data, this can be mainly used for 
# - Querying web logs 
# - Querying and staging data before loading to redshift
# - Analyze cloud trial, cloudfront, VPC, ELB etc logs in s3 
# 
# It offers various integration such as 
# - Jupyter notebooks, Zeeplin, rStudio Notebooks 
# - Integration with QuickSight 
# - Integration via ODBC / JDBC with other visualization tools 
#
# NOTE : You can use the Glue Data catalog created using Crawlers as a meta data to query the actual data sits in the S3, this is a very polular integration
#
# Pricing : $5 Per TB scanned 
# 	  : Successful or cancelled queries count, failed one don't
# 	  : No Charge for DDL
# 	  : By using column format ORC or Parquest format you can save upto 30-90% of the cost and gain better performance 
# 	  : if you use Glue or S3 you will have additional cost
#
# Access Control : IAM, S3 Bucket Policy, ACLs, Cross account S3 access is also possible
#
# Encryption 	 : SSE-S3, SSE-KMS, CSE-KMS, TLS encrption in transit
#
# Where You should not consider Athena : ETL or visualization 
#
# * Amazon Quick Sight
# Amazon Quick Sight is a fast and easy cloud powered 'buisness analytics' service which allows to build visualization, adhoc analysis and also quickly gets insights from data. 
# This is a serverless service which offer integration with redshift, Athena, Aurora/RDS, EC2 hosted Databases, files on S3 or on prem data in CSV, TSV, XLS and common log format
# This tool is mainly used for business analytics and visualization and and allows to perform minimal ETL. 
#
# - SPICE : This is a query accelaration techniqueue which used within Amazon quick sight which offers super fast, parallel and in-memory calculation engine 
# 	  : It used columnar storage, in-memory machine code generation 
# 	  : It accelerated interactive queries on large datasets  
# 	  : Each user gets 10 GB of SPICE, after that it is chargable 
# 	  : It is highly durable and scalable and scales upto hundreds of users 
#
# - ML Capabilities : Amazon quick sights provides three different ML capabilities, most of it is powred by random forest 
# 		    : Anomaly Detection 
# 		    : Forecasting
# 		    : Auto-narratives 
#
# - Where you should not use Quick Sight : Not for ETL and large canned reports, but can be used for adhoce reporting 
#
# Access Control : IAM, email signup and Active Directory Integration wtih Quick Sight Enterprise Edition 
#
# Pricing  : See https://www.udemy.com/course/aws-machine-learning/learn/lecture/16561576#notes
#
# Visyalization : This offers various standard visualization techniqueu such as line, bar, pie, histogram, time series etc 
#
#
# * EMR (Elastic Map reduce)
# EMR is a managed hadoop framework on EC2 instances which includes Spark, HBase, Preseto, Flink, Hive & More, EMR offers several integration points within AWS
# EMR also offers couple of notebooks such as EMR notebooks and zeeplin notebooks. 
#
# EMR mainly consist of three different component within the cluster 
# - Master node : This is a single EC2 instance manage the cluster 
# - Core node 	: Hosts HDFS data and run tasks, can be scaled up and down, but with some risk 
# - Task node 	: Run tasks, does not host data, no loss of data while removing and good use of spot instance 
#
# There are mainly two types of EMR cluster 
# - Persistant (Long running) cluster 	: In this case you can spin up the cluster and then submit the jobs to the cluster whenever you want 
# 					: You will need to understand the underlying EC2 won't get terminated until you manually terminate the cluster 
# 					: Reserved Instances will be good use for long running clusters 
# 					: You can submit multiple steps from the console. 
# - Transient (short lived) cluster 	: This cluster will spinup along with the job and then it runs the jobs, once the job is completed cluster will automatically gets terminated 
#
# Few things to note about 
# . You can launch a cluster within public subnet or private subnet with a NAT in the VPC 
# . You can use S3 instead of HDFS to input and output the data
# . cloudwatch can be used to monitor the cluster performance 
# . Cloudtrial will give us the details of all the call made from and to the cluster 
# . IAM roles should be configured for the cluster as well the underlying EC2 instances to access resources such as S3 etc 
# . AWS data pipeline can be configured to trigger EMR clusters as they transfer the data 
#
# Key things about Storage in EMR 
# . HDFS is hosted on the EBS volumes with in the EC2 instances and it can go away when an instance is getting terminated, so you might need to load the data somewhere other target before you terminate the cluster 
# . EMRFS is backed by S3, so you can EMRFS if you want your data to be persistant 
#
# Pricing : EMR charges per hour + the underlying EC2 costs 
# 	  : You won't get charged if any nodes gets failed during the launch and you can start a new one instead 
# 	  : Can add or remove nodes on the fly 
# 	  : Can resize the running cluster core nodes 
#
# Access Control : IAM, Kerberos, SSH 
#
# * Hadoop 
# Hadoop is a framework which runs with mainly three components 
#
# MapReduce or Spark : Both are used for transformation of the data
# YARN 		     : This is a resource negotioator within hadoop framework which is called Yet Another Resource Negotioator 
# HDFS 		     : File system used by the hadoop framework 
#
#
# * Spark - How it works 
# Spark mainly has three components as part of its execution 
#
# - Spark Context   : This is driver program which sets the memory etc for the spark 
# 		    : When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker node.
# - Cluster manager : Which manages resources using YARN 
# - Executor        : Which runs the task which sent by Spark conect 
# 
# Flow :  "Spark Context" --> "Cluster manager" --> "Executor"
#
# Note : To know more about pyspark refer - https://www.tutorialspoint.com/pyspark/pyspark_quick_guide.htm
# 
# Components of Spark are below 
#
# - RDDS 	    : RDD (Resilient Distributed Dataset) is the fundamental data structure of Apache Spark which are an immutable collection of objects which computes on the different node of the cluster. 
# 		    : Each and every dataset in Spark RDD is logically partitioned across many servers so that they can be computed on different nodes of the cluster.
# - Spark Streaming : This is used for analyzing data in streams, normally data will be send in mini batches for analyzing
# 		    : With streaming data frame which initialized will be keep gowring as the new mini batch of streams gets added 
# 		    : You can Integrate Kenisis streaming with spark streaming
# - Spark SQL       : Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine
# - MLLib           : This is machine learning library with the spark
# - GraphX          : This produces graphs 
#
# * Spark MLLib 
# MLlib is a machine learning library used by pyspark and its intended to provide the practical machine learning scalable and possible. 
# At a high level MLLib privides tools such as. 
# . ML Algotithms : Common learning algorithms such as classification, regression, clustering and collaborative filtering 
# . featurization : feature extraction, transformation, dimensionality reduction and selection 
# . Pipeline      : Tools for constructing, evaluating and tuning ML pipelines 
# . Persistance   : Saving and load algorithms, models and pipelines 
# . Utilities     : Linear algebra, statistics, data handling etc 
#
#
# * Feature Engineering
# Feature Engineering is the process of using domain knowledge on both data and data analysis technique to extract featres (chrectristics, properties and attributes) from raw data.
# Feature is a property shared by independant units on which analysis or prediction is to be done. 
# Features are used by predictive modles and influence results.
# 
# NOTE  : Too many feature can also be a problem since every feature is a new dimesion and you can'yt fit all in one model 
# 	: This problem is called the "The curse of Dimensionality"
#
# * Outliers 
# Outliers are the kind of values within a dataset which often tend to be outside the range of normal value for a variable. 
# The value might be way beyond an expected range or sometime way lower. 
# While you often deal with Outliers in data set you will need to either drop those values or find median range to replace those values, else the existance of these values can affect the model performance. 
#
# * Imputing Mising Data 
# While dealing with the data often you will see multiple challenges and out of that one key challenge is missing. 
# If there are missing data within your model then there are mutliple techniques to deal with that data 
#
# 1. Mean replacement : In this technique you can take the mean value for a specific column and replace the no value cells with that value, which will help to fit the data 
# 	  Example     : If you are trying to work with a data set where Specific car model and its re-selling price over the year, for those missing values you can take the mean value from price column and replace it
#
# 2. Median replacement : If there are outlier values which more often in your data set which is kind of making your mean value wrong then you can median replacement where you will pick the value in the middle and replace them. 
#
# NOTE : Above methods are applicable only with the numerical data and cann't be used with Catagorical data 
#
# 3. Dropping Data  : If you have a wealth of lot of data in your dataset and you can drop some rows of the dataset which is incomplete, then you can drop and its a easier solution to have. 
# 		    : For a smaller dataset if you are dropping data then it will have impact on the modle preceission and end up in some bias.
#
# NOTE : All above methods are not the best methods, but they still works. 
#
# * Imputing Missind Data - Using Machine Learning 
# Similar to the methods we mentioned above you can also use few additional techniques 
# 
#
#
#
#
#
#
#
#
~                                                                                                                                                                                                            
~                                                                                                                                                                                                            
~                           

